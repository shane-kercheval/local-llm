{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/code\n"
     ]
    }
   ],
   "source": [
    "%cd /code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.service.datasets import DATA\n",
    "from source.library.scraping import scrape_all_urls#, scrape_urls\n",
    "\n",
    "# This is needed because openai.text_completion calls asynchronous functions but\n",
    "# Jupyter is already running its own event loop.\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1317\n",
      "('https://python.langchain.com/en/deployments.html', '404 Not Found\\n | Read the Docs Read the Docs 404 Not Found You are browsing the documentation of langchain . The documentation version you are looking for at /en/deployments.html was not found. Documentation changes over time and pages are moved around. You can try to navigate to the index page of the project and use its navigation, or search for a similar page. Try searching? Search all the docs')\n"
     ]
    }
   ],
   "source": [
    "langchain_doc_chunks = DATA.langchain_doc_chunks.load()\n",
    "print(len(langchain_doc_chunks))\n",
    "print(langchain_doc_chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = '/code/.vectordb/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:chromadb:Using embedded DuckDB with persistence: data will be stored in: /code/.vectordb/\n",
      "WARNING:chromadb.api.models.Collection:No embedding_function provided, using default embedding function: SentenceTransformerEmbeddingFunction\n",
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "vectordb = Chroma(persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Document Loaders # Note Conceptual Guide Combining language models with your own text data is a powerful way to differentiate them.\\nThe first step in doing this is to load the data into “documents” - a fancy way of say some pieces of text.\\nThis module is aimed at making this easy. A primary driver of a lot of this is the Unstructured python package.\\nThis package is a great way to transform all', metadata={'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders.html'}),\n",
       " Document(page_content='Document Loaders\\n==========================\\n\\n.. note::\\n   `Conceptual Guide `_\\n\\n\\nCombining language models with your own text data is a powerful way to differentiate them.\\nThe first step in doing this is to load the data into \"documents\" - a fancy way of say some pieces of text.\\nThis module is aimed at making this easy.\\n\\nA primary driver of a lot of this is the `Unstructured `_ python', metadata={'url': 'https://python.langchain.com/en/latest/_sources/modules/indexes/document_loaders.rst'}),\n",
       " Document(page_content='the functionality we dive deep on those topics. For an overview of everything related to this, please see the below notebook for getting started: Getting Started We then provide a deep dive on the four main components. Document Loaders How to load documents from a variety of sources. Text Splitters An overview of the abstractions and implementions around splitting text. VectorStores An overview of', metadata={'url': 'https://python.langchain.com/en/latest/modules/indexes.html'}),\n",
       " Document(page_content='Models # Note Conceptual Guide This section of the documentation deals with different types of models that are used in LangChain.\\nOn this page we will go over the model types at a high level,\\nbut we have individual pages for each model type.\\nThe pages contain more detailed “how-to” guides for working with that model,\\nas well as a list of different model providers. LLMs Large Language Models (LLMs)', metadata={'url': 'https://python.langchain.com/en/latest/modules/models.html'}),\n",
       " Document(page_content=\"for data and tools related to '\\n             'chain-of-thought reasoning in large language models. Developed @ '\\n             'Samwald research group: https://samwald.info/',\\n  'title': 'ThoughtSource',\\n  'link': 'https://github.com/OpenBioLink/ThoughtSource',\\n  'engines': ['github'],\\n  'category': 'it'},\\n {'snippet': 'A comprehensive list of papers using large language/multi-modal '\", metadata={'url': 'https://python.langchain.com/en/latest/modules/agents/tools/examples/searx_search.html'})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb.similarity_search(query=\"Document Loaders # Note Conceptual Guide Combining language models\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ./models/ggml-alpaca-7b-q4.bin\n",
      "llama.cpp: can't use mmap because tensors are not aligned; convert to new format to avoid this\n",
      "llama_model_load_internal: format     = 'ggml' (old version with low tokenizer quality and no mmap support)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 4113739.11 KB\n",
      "llama_model_load_internal: mem required  = 5809.32 MB (+ 1026.00 MB per state)\n",
      "...................................................................................................\n",
      ".\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | VSX = 0 | \n",
      "llama_init_from_file: kv self size  = 1024.00 MB\n"
     ]
    }
   ],
   "source": [
    "# from langchain.chains import VectorDBQA\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "\n",
    "model_path = './models/ggml-alpaca-7b-q4.bin'\n",
    "llm = LlamaCpp(model_path=model_path, temperature=0.0, n_ctx=1024 * 2)\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectordb.as_retriever()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is Langchain?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Langchain is a distributed ledger technology that enables secure, private and reliable data sharing between organizations. It is based on blockchain technology but designed to meet the specific needs of enterprise customers.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(prompt=f\"Question: {question}? Answer: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Langchain is a programming language designed to make it easier for developers to create intelligent applications. It uses a declarative approach, allowing developers to specify what the application should do and let Langchain figure out how to do it. It has built-in machine learning capabilities, making it easy to develop AI-powered applications.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(question)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How do I load hugging face models with Langchain?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Download the HuGG model from the GPT Model Hub. 2. Extract the files inside the zip folder and move them to a suitable location. 3. Open the Langchain CLI and type \"load model <model_name>.<format>\" where <model_name> is the name of the HuGG model you downloaded, and <format> is one of the following: \"txt\" (text), \"bin\" (binary) or \"pbt\" (pre-trained). 4. Follow the instructions in the Langchain CLI to load your model.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(prompt=f\"Question: {question}? Answer: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You can use the Huggiing Face Hub wrapper to easily load models into Langchain. The wrapper is available as a library and can be installed with pip install huggiingface_hub. After installing the library, you will need to create an account on Huggiing Face Hub if you don't already have one. Once you have your model, you can use the Hueggiing Face Hub wrapper to load it into Langchain.\n"
     ]
    }
   ],
   "source": [
    "answer = qa.run(question)\n",
    "print(answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base-Model:\n",
      "1. Open the chatbot creator in Mitsuku. 2. Select the \"Create a New Prompt\" option from the main menu. 3. Choose a template from the list of available prompts. 4. Edit the template to customize it for your needs. 5. Preview and test the chatbot.\n",
      "Context-Model\n",
      " You can use the Huggiing Face Hub wrapper to easily load models into Langchain. The wrapper is available as a library and can be installed with pip install huggiingface_hub. After installing the library, you will need to create an account on Huggiing Face Hub if you don't already have one. Once you have your model, you can use the Hueggiing Face Hub wrapper to load it into Langchain.\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I use prompt templates for creating a chat bot?\"\n",
    "print(\"Base-Model:\")\n",
    "print(llm(prompt=f\"Question: {question}? Answer: \"))\n",
    "print(\"Context-Model\")\n",
    "context_answer = qa.run(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base-Model:\n",
      "1. Open the chatbot creator in Mitsuku. 2. Select the \"Create a New Prompt\" option from the main menu. 3. Choose a template from the list of available prompts. 4. Edit the template to customize it for your needs. 5. Preview and test the chatbot.\n",
      "Context-Model:\n",
      " To create a chatbot using prompt templates, you will need to set up a chat API and then use the provided prompt template to generate structured responses. You can find an example of a chat prompt template here and further information on how to use it in the linked documentation.\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I use prompt templates for creating a chat bot?\"\n",
    "print(\"Base-Model:\")\n",
    "base_answer = llm(prompt=f\"Question: {question}? Answer: \")\n",
    "print(base_answer)\n",
    "print(\"Context-Model:\")\n",
    "context_answer = qa.run(question)\n",
    "print(context_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_similarity_search(results):\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://python.langchain.com/en/latest/modules/prompts/prompt_templates/examples/custom_prompt_template.html\n",
      "of default prompt templates here . Creating a Custom Prompt Template # There are essentially two distinct prompt templates available - string prompt templates and chat prompt templates. String prompt templates provides a simple prompt in string format, while chat prompt templates produces a more structured prompt to be used with a chat API. In this guide, we will create a custom prompt using a\n",
      "https://python.langchain.com/en/latest/modules/prompts.html\n",
      "retrying if necessary). Go Deeper # Prompt Templates Chat Prompt Template Example Selectors Output Parsers\n",
      "https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/figma.html\n",
      "have no idea if the Jon Carmack thing makes for better code. YMMV. # See https://python.langchain.com/en/latest/modules/models/chat/getting_started.html for chat info system_prompt_template = \"\"\"You are expert coder Jon Carmack. Use the provided design context to create idomatic HTML/CSS code as possible based on the user request. Everything must be inline in one file and your response must be\n",
      "https://python.langchain.com/en/latest/_sources/modules/prompts.rst\n",
      "to make constructing and working with prompts easy.\n",
      "\n",
      "This section of documentation is split into four sections:\n",
      "\n",
      "**LLM Prompt Templates**\n",
      "\n",
      "How to use PromptTemplates to prompt Language Models.\n",
      "\n",
      "**Chat Prompt Templates**\n",
      "\n",
      "How to use PromptTemplates to prompt Chat Models.\n",
      "\n",
      "**Example Selectors**\n",
      "\n",
      "Often times it is useful to include examples in prompts.\n",
      "These examples can be hardcoded, but it is often\n",
      "https://python.langchain.com/en/latest/getting_started/getting_started.html\n",
      "ChatPromptTemplate . from_messages ([ system_message_prompt , human_message_prompt ]) chain = LLMChain ( llm = chat , prompt = chat_prompt ) chain . run ( input_language = \"English\" , output_language = \"French\" , text = \"I love programming.\" ) # -> \"J'aime programmer.\" Agents with Chat Models # Agents can also be used with chat models, you can initialize one using\n"
     ]
    }
   ],
   "source": [
    "results = vectordb.similarity_search(query=question, k=5)\n",
    "for r in results:\n",
    "    print(r.metadata['url'])\n",
    "    print(r.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='of default prompt templates here . Creating a Custom Prompt Template # There are essentially two distinct prompt templates available - string prompt templates and chat prompt templates. String prompt templates provides a simple prompt in string format, while chat prompt templates produces a more structured prompt to be used with a chat API. In this guide, we will create a custom prompt using a', metadata={'url': 'https://python.langchain.com/en/latest/modules/prompts/prompt_templates/examples/custom_prompt_template.html'}),\n",
       " Document(page_content='PromptLayer ChatOpenAI # This example showcases how to connect to PromptLayer to start recording your ChatOpenAI requests. Install PromptLayer # The promptlayer package is required to use PromptLayer with OpenAI. Install promptlayer using pip. pip install promptlayer Imports # import os from langchain.chat_models import PromptLayerChatOpenAI from langchain.schema import HumanMessage Set the', metadata={'url': 'https://python.langchain.com/en/latest/modules/models/chat/integrations/promptlayer_chatopenai.html'}),\n",
       " Document(page_content='= SystemMessagePromptTemplate ( prompt = prompt ) LLMChain # You can use the existing LLMChain in a very similar way to before - provide a prompt and a model. chain = LLMChain ( llm = chat , prompt = chat_prompt ) chain . run ( input_language = \"English\" , output_language = \"French\" , text = \"I love programming.\" ) \"J\\'adore la programmation.\" Streaming # Streaming is supported for ChatOpenAI', metadata={'url': 'https://python.langchain.com/en/latest/modules/models/chat/getting_started.html'}),\n",
       " Document(page_content='collection of useful prompts you can use in your project. You can read more about LangChainHub and the prompts available with it here . from langchain.prompts import load_prompt prompt = load_prompt ( \"lc://prompts/conversation/prompt.json\" ) prompt . format ( history = \"\" , input = \"What is 1 + 1?\" ) You can learn more about serializing prompt template in How to serialize prompts . Pass few shot', metadata={'url': 'https://python.langchain.com/en/latest/modules/prompts/prompt_templates/getting_started.html'}),\n",
       " Document(page_content='prefix = \"\"\"Have a conversation with a human, answering the following questions as best you can. You have access to the following tools:\"\"\" suffix = \"\"\"Begin!\" {chat_history} Question: {input} {agent_scratchpad} \"\"\" prompt = ZeroShotAgent . create_prompt ( tools , prefix = prefix , suffix = suffix , input_variables = [ \"input\" , \"chat_history\" , \"agent_scratchpad\" ] ) Now we can create the', metadata={'url': 'https://python.langchain.com/en/latest/modules/memory/examples/agent_with_memory_in_db.html'})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb.similarity_search(query=base_answer, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='of default prompt templates here . Creating a Custom Prompt Template # There are essentially two distinct prompt templates available - string prompt templates and chat prompt templates. String prompt templates provides a simple prompt in string format, while chat prompt templates produces a more structured prompt to be used with a chat API. In this guide, we will create a custom prompt using a', metadata={'url': 'https://python.langchain.com/en/latest/modules/prompts/prompt_templates/examples/custom_prompt_template.html'}),\n",
       " Document(page_content='to make constructing and working with prompts easy.\\n\\nThis section of documentation is split into four sections:\\n\\n**LLM Prompt Templates**\\n\\nHow to use PromptTemplates to prompt Language Models.\\n\\n**Chat Prompt Templates**\\n\\nHow to use PromptTemplates to prompt Chat Models.\\n\\n**Example Selectors**\\n\\nOften times it is useful to include examples in prompts.\\nThese examples can be hardcoded, but it is often', metadata={'url': 'https://python.langchain.com/en/latest/_sources/modules/prompts.rst'}),\n",
       " Document(page_content='prompts easy. This section of documentation is split into four sections: LLM Prompt Templates How to use PromptTemplates to prompt Language Models. Chat Prompt Templates How to use PromptTemplates to prompt Chat Models. Example Selectors Often times it is useful to include examples in prompts.\\nThese examples can be hardcoded, but it is often more powerful if they are dynamically selected.\\nThis', metadata={'url': 'https://python.langchain.com/en/latest/modules/prompts.html'}),\n",
       " Document(page_content='have no idea if the Jon Carmack thing makes for better code. YMMV. # See https://python.langchain.com/en/latest/modules/models/chat/getting_started.html for chat info system_prompt_template = \"\"\"You are expert coder Jon Carmack. Use the provided design context to create idomatic HTML/CSS code as possible based on the user request. Everything must be inline in one file and your response must be', metadata={'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/figma.html'}),\n",
       " Document(page_content='And here’s an example of using this in a chat model chat_model = ChatOpenAI ( temperature = 0 ) prompt = ChatPromptTemplate ( messages = [ HumanMessagePromptTemplate . from_template ( \"answer the users question as best as possible. \\\\n {format_instructions} \\\\n {question} \" ) ], input_variables = [ \"question\" ], partial_variables = { \"format_instructions\" : format_instructions } ) _input = prompt .', metadata={'url': 'https://python.langchain.com/en/latest/modules/prompts/output_parsers/examples/structured.html'})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb.similarity_search(query=context_answer, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the map_reduce chain type?\"\n",
    "print(\"Base-Model:\")\n",
    "base_answer = llm(prompt=f\"Question: {question}? Answer: \")\n",
    "print(base_answer)\n",
    "print(\"Context-Model:\")\n",
    "context_answer = qa.run(question)\n",
    "print(context_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:chromadb:Using embedded DuckDB without persistence: data will be transient\n",
      "WARNING:chromadb.api.models.Collection:No embedding_function provided, using default embedding function: SentenceTransformerEmbeddingFunction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394.82080485952923\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import chromadb\n",
    "\n",
    "client = chromadb.Client()\n",
    "\n",
    "collection = client.create_collection('sample_collection')\n",
    "\n",
    "documents = []\n",
    "metadatas = []\n",
    "\n",
    "for url, chunk in langchain_doc_chunks:\n",
    "        documents.append(chunk)\n",
    "        metadatas.append({'url': url})\n",
    "\n",
    "collection.add(\n",
    "    documents=documents, # we embed for you, or bring your own\n",
    "    metadatas=metadatas, # filter on arbitrary metadata!\n",
    "    ids=[str(x) for x in range(len(documents))], # must be unique for each doc \n",
    ")\n",
    "\n",
    "print(np.mean([len(x) for x in documents]))\n",
    "print(np.max([len(x) for x in documents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['404 Not Found\\n | Read the Docs Read the Docs 404 Not Found You are browsing the documentation of langchain . The documentation version you are looking for at /en/deployments.html was not found. Documentation changes over time and pages are moved around. You can try to navigate to the index page of the project and use its navigation, or search for a similar page. Try searching? Search all the docs', 'Search all the docs Are you the project owner? Here are some tips to address 404 errors: Use your own custom 404 page: Read more » Create redirects when you move contents: Read more » Stay Updated Blog Sign up for our newsletter to get our latest blog updates delivered to your inbox weekly. Email Subscribe Loading... Thank you! You have successfully joined our subscriber list. Resources Tutorial', 'Resources Tutorial Team Documentation Going Ad-free Site Support Site Status Company Jobs Advertise with Us Read the Docs for Business Branding & Media Kit Privacy Policy Terms of Service © Copyright 2023, Read the Docs, Inc & contributors Version 9.12.0 AWS Cloud Computing Cloudflare DNS & SSL Sentry Monitoring Elastic Search New Relic Performance PagerDuty Monitoring', '404 Not Found\\n | Read the Docs Read the Docs 404 Not Found You are browsing the documentation of langchain . The documentation version you are looking for at /en/ecosystem.html was not found. Documentation changes over time and pages are moved around. You can try to navigate to the index page of the project and use its navigation, or search for a similar page. Try searching? Search all the docs', 'Search all the docs Are you the project owner? Here are some tips to address 404 errors: Use your own custom 404 page: Read more » Create redirects when you move contents: Read more » Stay Updated Blog Sign up for our newsletter to get our latest blog updates delivered to your inbox weekly. Email Subscribe Loading... Thank you! You have successfully joined our subscriber list. Resources Tutorial', 'Resources Tutorial Team Documentation Going Ad-free Site Support Site Status Company Jobs Advertise with Us Read the Docs for Business Branding & Media Kit Privacy Policy Terms of Service © Copyright 2023, Read the Docs, Inc & contributors Version 9.12.0 AWS Cloud Computing Cloudflare DNS & SSL Sentry Monitoring Elastic Search New Relic Performance PagerDuty Monitoring', '404 Not Found\\n | Read the Docs Read the Docs 404 Not Found You are browsing the documentation of langchain . The documentation version you are looking for at /en/ecosystem/ai21.html was not found. Documentation changes over time and pages are moved around. You can try to navigate to the index page of the project and use its navigation, or search for a similar page. Try searching? Search all the', 'Search all the docs Are you the project owner? Here are some tips to address 404 errors: Use your own custom 404 page: Read more » Create redirects when you move contents: Read more » Stay Updated Blog Sign up for our newsletter to get our latest blog updates delivered to your inbox weekly. Email Subscribe Loading... Thank you! You have successfully joined our subscriber list. Resources Tutorial', 'Resources Tutorial Team Documentation Going Ad-free Site Support Site Status Company Jobs Advertise with Us Read the Docs for Business Branding & Media Kit Privacy Policy Terms of Service © Copyright 2023, Read the Docs, Inc & contributors Version 9.12.0 AWS Cloud Computing Cloudflare DNS & SSL Sentry Monitoring Elastic Search New Relic Performance PagerDuty Monitoring', '404 Not Found\\n | Read the Docs Read the Docs 404 Not Found You are browsing the documentation of langchain . The documentation version you are looking for at /en/ecosystem/aim_tracking.html was not found. Documentation changes over time and pages are moved around. You can try to navigate to the index page of the project and use its navigation, or search for a similar page. Try searching? Search']\n"
     ]
    }
   ],
   "source": [
    "print(documents[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "results = collection.query(\n",
    "    query_texts=\"What is a document loader?\",\n",
    "    n_results=5,\n",
    "    # where={\"metadata_field\": \"is_equal_to_this\"}, # optional filter\n",
    "    # where_document={\"$contains\":\"search_string\"}  # optional filter\n",
    ")\n",
    "print(len(results['documents'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['192', '183', '187', '222', '182']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['and explore other prompts, chains, and agents. Glossary : A glossary of all related terms, papers, methods, etc. Whether implemented in LangChain or not! Gallery : A collection of our favorite projects that use LangChain. Useful for finding inspiration or seeing how things were done in other applications. Deployments : A collection of instructions, code snippets, and template repositories for',\n",
       "   'optimization, and prompt serialization. Memory : Memory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory. Indexes : Language models are often more powerful when combined with your own text data - this module covers best practices for doing',\n",
       "   'each other or to events can be an interesting way to observe their long-term memory abilities. Personal Assistants : The main LangChain use case. Personal assistants need to take actions, remember interactions, and have knowledge about your data. Question Answering : The second big LangChain use case. Answering questions over specific documents, only utilizing the information in those documents to',\n",
       "   '404 Not Found\\n | Read the Docs Read the Docs 404 Not Found You are browsing the documentation of langchain . The documentation version you are looking for at /en/modules/agents/agent_executors/examples/sharedmemory_for_tools.html was not found. Documentation changes over time and pages are moved around. You can try to navigate to the index page of the project and use its navigation, or search for',\n",
       "   'Documentation Modules # There are several main modules that LangChain provides support for.\\nFor each module we provide some examples to get started, how-to guides, reference docs, and conceptual guides.\\nThese modules are, in increasing order of complexity: Models : The various model types and model integrations LangChain supports. Prompts : This includes prompt management, prompt optimization, and']],\n",
       " 'metadatas': [[{'url': 'https://python.langchain.com/en/latest'},\n",
       "   {'url': 'https://python.langchain.com/en/latest'},\n",
       "   {'url': 'https://python.langchain.com/en/latest'},\n",
       "   {'url': 'https://python.langchain.com/en/modules/agents/agent_executors/examples/sharedmemory_for_tools.html'},\n",
       "   {'url': 'https://python.langchain.com/en/latest'}]],\n",
       " 'distances': [[1.4229007959365845,\n",
       "   1.4280325174331665,\n",
       "   1.5446809530258179,\n",
       "   1.5470739603042603,\n",
       "   1.562438726425171]]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
