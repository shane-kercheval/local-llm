{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/code\n"
     ]
    }
   ],
   "source": [
    "%cd /code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.service.datasets import DATA\n",
    "from source.library.scraping import scrape_all_urls#, scrape_urls\n",
    "\n",
    "# This is needed because openai.text_completion calls asynchronous functions but\n",
    "# Jupyter is already running its own event loop.\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11060\n",
      "('https://python.langchain.com/en/latest', 'Welcome to LangChain # LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an API, but will also: Be data-aware : connect a language model to other sources of data Be agentic : allow a language model to interact with its environment The LangChain framework')\n"
     ]
    }
   ],
   "source": [
    "langchain_doc_chunks = DATA.langchain_doc_chunks.load()\n",
    "print(len(langchain_doc_chunks))\n",
    "print(langchain_doc_chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = '/code/.vectordb/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:chromadb:Using embedded DuckDB with persistence: data will be stored in: /code/.vectordb/\n",
      "WARNING:chromadb.api.models.Collection:No embedding_function provided, using default embedding function: SentenceTransformerEmbeddingFunction\n"
     ]
    }
   ],
   "source": [
    "vectordb = Chroma(persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Human: For LangChain! Have you heard of it? AI:  No, I haven't heard of LangChain. Can you tell me more about it? Human: Haha nope, although a lot of people confuse it for that AI: > Finished chain. ' Oh, okay. What is LangChain?'\", metadata={'url': 'https://python.langchain.com/en/latest/modules/memory/types/summary_buffer.html'}),\n",
       " Document(page_content='of all related terms, papers, methods, etc. Whether implemented in LangChain or not!\\\\nGallery: A collection of our favorite projects that use LangChain. Useful for finding inspiration or seeing how things were done in other applications.\\\\nDeployments: A collection of instructions, code snippets, and template repositories for deploying LangChain apps.\\\\nDiscord: Join us on our Discord to discuss all', metadata={'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/sitemap.html'}),\n",
       " Document(page_content='of all related terms, papers, methods, etc. Whether implemented in LangChain or not!\\\\nGallery: A collection of our favorite projects that use LangChain. Useful for finding inspiration or seeing how things were done in other applications.\\\\nDeployments: A collection of instructions, code snippets, and template repositories for deploying LangChain apps.\\\\nTracing: A guide on using tracing in LangChain', metadata={'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/sitemap.html'}),\n",
       " Document(page_content='to discuss all things LangChain!\\\\nTracing: A guide on using tracing in LangChain to visualize the execution of chains and agents.\\\\nProduction Support: As you move your LangChains into production, we’d love to offer more comprehensive support. Please fill out this form and we’ll set up a dedicated support Slack channel.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nnext\\\\nQuickstart Guide\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n Contents\\\\n', metadata={'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/sitemap.html'}),\n",
       " Document(page_content='models: Experimenting with different prompts, models, and chains is a big part of developing the best possible application. The ModelLaboratory makes it easy to do so.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nReference Docs#\\\\nAll of LangChain’s reference documentation, in one place. Full documentation on all methods, classes, installation methods, and integration setups for LangChain.\\\\n\\\\nReference', metadata={'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/sitemap.html'})]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb.similarity_search(query=\"What is Langchain?\", k=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ./models/ggml-vicuna-13b-1.1-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 4 (mostly Q4_1, some F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =  73.73 KB\n",
      "llama_model_load_internal: mem required  = 9807.47 MB (+ 1608.00 MB per state)\n",
      "llama_init_from_file: kv self size  = 1600.00 MB\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "# from langchain.chains import VectorDBQA\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "# N_CONTEXT = 1024\n",
    "# model_path = './models/ggml-alpaca-7b-q4.bin'\n",
    "\n",
    "N_CONTEXT = 2048\n",
    "model_path = './models/ggml-vicuna-13b-1.1-q4_0.bin'\n",
    "\n",
    "llm = LlamaCpp(model_path=model_path, temperature=0.0, n_ctx=N_CONTEXT)\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context-Model\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " LangChain is a platform for building and deploying blockchain-based applications. It provides a set of tools, libraries, and services to help developers create decentralized applications that can be deployed on various blockchain networks.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is LangChain?\"\n",
    "# print(\"Base-Model:\")\n",
    "# print(llm(prompt=f\"Question: {question}? Answer: \"))\n",
    "print(\"\\nContext-Model\")\n",
    "context_answer = qa.run(question)\n",
    "print(context_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"memory\": null, \"callbacks\": null, \"callback_manager\": null, \"verbose\": true, \"combine_documents_chain\": {\"memory\": null, \"callbacks\": null, \"callback_manager\": null, \"verbose\": false, \"input_key\": \"input_documents\", \"output_key\": \"output_text\", \"llm_chain\": {\"memory\": null, \"callbacks\": null, \"callback_manager\": null, \"verbose\": false, \"prompt\": {\"input_variables\": [\"context\", \"question\"], \"output_parser\": null, \"partial_variables\": {}, \"template\": \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\", \"template_format\": \"f-string\", \"validate_template\": true, \"_type\": \"prompt\"}, \"llm\": {\"model_path\": \"./models/ggml-vicuna-13b-1.1-q4_0.bin\", \"suffix\": null, \"max_tokens\": 256, \"temperature\": 0.0, \"top_p\": 0.95, \"logprobs\": null, \"echo\": false, \"stop_sequences\": [], \"repeat_penalty\": 1.1, \"top_k\": 40, \"_type\": \"llama.cpp\"}, \"output_key\": \"text\", \"_type\": \"llm_chain\"}, \"document_prompt\": {\"input_variables\": [\"page_content\"], \"output_parser\": null, \"partial_variables\": {}, \"template\": \"{page_content}\", \"template_format\": \"f-string\", \"validate_template\": true, \"_type\": \"prompt\"}, \"document_variable_name\": \"context\", \"document_separator\": \"\\n\\n\", \"_type\": \"stuff_documents_chain\"}, \"input_key\": \"query\", \"output_key\": \"result\", \"return_source_documents\": false}\n"
     ]
    }
   ],
   "source": [
    "print(qa.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.combine_documents_chain.llm_chain.prompt.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Human: For LangChain! Have you heard of it? AI:  No, I haven't heard of LangChain. Can you tell me more about it? Human: Haha nope, although a lot of people confuse it for that AI: > Finished chain. ' Oh, okay. What is LangChain?'\", metadata={'url': 'https://python.langchain.com/en/latest/modules/memory/types/summary_buffer.html'}),\n",
       " Document(page_content='of all related terms, papers, methods, etc. Whether implemented in LangChain or not!\\\\nGallery: A collection of our favorite projects that use LangChain. Useful for finding inspiration or seeing how things were done in other applications.\\\\nDeployments: A collection of instructions, code snippets, and template repositories for deploying LangChain apps.\\\\nDiscord: Join us on our Discord to discuss all', metadata={'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/sitemap.html'}),\n",
       " Document(page_content='of all related terms, papers, methods, etc. Whether implemented in LangChain or not!\\\\nGallery: A collection of our favorite projects that use LangChain. Useful for finding inspiration or seeing how things were done in other applications.\\\\nDeployments: A collection of instructions, code snippets, and template repositories for deploying LangChain apps.\\\\nTracing: A guide on using tracing in LangChain', metadata={'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/sitemap.html'}),\n",
       " Document(page_content='to discuss all things LangChain!\\\\nTracing: A guide on using tracing in LangChain to visualize the execution of chains and agents.\\\\nProduction Support: As you move your LangChains into production, we’d love to offer more comprehensive support. Please fill out this form and we’ll set up a dedicated support Slack channel.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nnext\\\\nQuickstart Guide\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n Contents\\\\n', metadata={'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/sitemap.html'}),\n",
       " Document(page_content='models: Experimenting with different prompts, models, and chains is a big part of developing the best possible application. The ModelLaboratory makes it easy to do so.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nReference Docs#\\\\nAll of LangChain’s reference documentation, in one place. Full documentation on all methods, classes, installation methods, and integration setups for LangChain.\\\\n\\\\nReference', metadata={'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/sitemap.html'})]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb.similarity_search(query=\"What is Langchain?\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base-Model:\n",
      "\n",
      "\n",
      "To use prompt templates for creating a chatbot, you can follow these steps:\n",
      "\n",
      "1. Define the intents of your chatbot - i.e., what tasks or questions the chatbot should be able to handle.\n",
      "2. Create a list of prompts for each intent - i.e., a list of messages that the chatbot can send in response to different user inputs.\n",
      "3. Use a natural language processing (NLP) library, such as spaCy or NLTK, to extract keywords and phrases from user input and match them against the prompts for each intent.\n",
      "4. Select the most relevant prompt based on the user input and send it back to the user in the form of a message.\n",
      "5. Continuously monitor the chatbot's interactions with users and update the prompt templates as needed to improve its performance.\n",
      "\n",
      "Context-Model\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " You can use the PromptTemplates module to create custom prompts for your chat bot by using the Chat Prompt Template example selectors and output parsers. The Chat Prompt Template provides a more structured prompt in string format, which is suitable for use with a chat API. To get started, you can refer to the [Chat Prompt Template](https://python.langchain.com/en/latest/modules/models/chat/getting_started.html) documentation for more information on how to create idomatic HTML/CSS code as possible based on the user request.\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I use prompt templates for creating a chat bot?\"\n",
    "print(\"Base-Model:\")\n",
    "print(llm(prompt=f\"Question: {question}? Answer: \"))\n",
    "print(\"\\nContext-Model\")\n",
    "context_answer = qa.run(question)\n",
    "print(context_answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How do I use prompt templates for creating a chat bot?\"\n",
    "print(\"Base-Model:\")\n",
    "base_answer = llm(prompt=f\"Question: {question}? Answer: \")\n",
    "print(base_answer)\n",
    "print(\"\\nContext-Model\")\n",
    "context_answer = qa.run(question)\n",
    "print(context_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_similarity_search(results):\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vectordb.similarity_search(query=question, k=5)\n",
    "for r in results:\n",
    "    print(r.metadata['url'])\n",
    "    print(r.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.similarity_search(query=base_answer, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.similarity_search(query=context_answer, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the map_reduce chain type?\"\n",
    "print(\"Base-Model:\")\n",
    "base_answer = llm(prompt=f\"Question: {question}? Answer: \")\n",
    "print(base_answer)\n",
    "print(\"Context-Model:\")\n",
    "context_answer = qa.run(question)\n",
    "print(context_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import chromadb\n",
    "\n",
    "client = chromadb.Client()\n",
    "\n",
    "collection = client.create_collection('sample_collection')\n",
    "\n",
    "documents = []\n",
    "metadatas = []\n",
    "\n",
    "for url, chunk in langchain_doc_chunks:\n",
    "        documents.append(chunk)\n",
    "        metadatas.append({'url': url})\n",
    "\n",
    "collection.add(\n",
    "    documents=documents, # we embed for you, or bring your own\n",
    "    metadatas=metadatas, # filter on arbitrary metadata!\n",
    "    ids=[str(x) for x in range(len(documents))], # must be unique for each doc \n",
    ")\n",
    "\n",
    "print(np.mean([len(x) for x in documents]))\n",
    "print(np.max([len(x) for x in documents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(\n",
    "    query_texts=\"What is a document loader?\",\n",
    "    n_results=5,\n",
    "    # where={\"metadata_field\": \"is_equal_to_this\"}, # optional filter\n",
    "    # where_document={\"$contains\":\"search_string\"}  # optional filter\n",
    ")\n",
    "print(len(results['documents'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
