{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/code\n"
     ]
    }
   ],
   "source": [
    "%cd /code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from source.service.datasets import DATA\n",
    "from source.library.scraping import scrape_all_urls#, scrape_urls\n",
    "import source.config.config as config\n",
    "\n",
    "# This is needed because openai.text_completion calls asynchronous functions but\n",
    "# Jupyter is already running its own event loop.\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12329\n",
      "('https://python.langchain.com/en/latest', 'Welcome to LangChain # LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model, but will also be: Data-aware : connect a language model to other sources of data Agentic : allow a language model to interact with its environment The LangChain framework is designed')\n"
     ]
    }
   ],
   "source": [
    "langchain_doc_chunks = DATA.langchain_doc_chunks.load()\n",
    "print(len(langchain_doc_chunks))\n",
    "print(langchain_doc_chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain.utilities.powerbi:Could not import azure.core python package.\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = '/code/.vectordb/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:chromadb:Using embedded DuckDB with persistence: data will be stored in: /code/.vectordb/\n",
      "WARNING:chromadb.api.models.Collection:No embedding_function provided, using default embedding function: SentenceTransformerEmbeddingFunction\n",
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "vectordb = Chroma(persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Human: For LangChain! Have you heard of it? AI:  No, I haven't heard of LangChain. Can you tell me more about it? Human: Haha nope, although a lot of people confuse it for that AI: > Finished chain. ' Oh, okay. What is LangChain?'\", metadata={'url': 'https://python.langchain.com/en/latest/modules/memory/types/summary_buffer.html'}),\n",
       " Document(page_content='such as OpenAI or Hugging Face. LangChain is a software development framework designed to simplify the creation of applications using large language models (LLMs). Written in: Python and JavaScript. Initial release: October 2022. LangChain - The A.I-native developer toolkit We started LangChain with the intent to build a modular and flexible framework for developing A.I- ... LangChain explained in', metadata={'url': 'https://python.langchain.com/en/latest/modules/chains/examples/flare.html'}),\n",
       " Document(page_content='of all related terms, papers, methods, etc. Whether implemented in LangChain or not!\\\\nGallery: A collection of our favorite projects that use LangChain. Useful for finding inspiration or seeing how things were done in other applications.\\\\nDeployments: A collection of instructions, code snippets, and template repositories for deploying LangChain apps.\\\\nDiscord: Join us on our Discord to discuss all', metadata={'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/sitemap.html'}),\n",
       " Document(page_content='of all related terms, papers, methods, etc. Whether implemented in LangChain or not!\\\\nGallery: A collection of our favorite projects that use LangChain. Useful for finding inspiration or seeing how things were done in other applications.\\\\nDeployments: A collection of instructions, code snippets, and template repositories for deploying LangChain apps.\\\\nTracing: A guide on using tracing in LangChain', metadata={'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/sitemap.html'}),\n",
       " Document(page_content='explained in 3 minutes - LangChain is a ... Duration: 3:03. Posted: Apr 13, 2023. LangChain is a framework built to help you build LLM-powered applications more easily by providing you with the following:. LangChain is a framework that enables quick and easy development of applications that make use of Large Language Models, for example, GPT-3. LangChain is a powerful open-source framework for', metadata={'url': 'https://python.langchain.com/en/latest/modules/chains/examples/flare.html'})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb.similarity_search(query=\"What is Langchain?\", k=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /llms/models/ggml-vic13b-q5_1.bin\n",
      "llama_model_load_internal: format     = ggjt v2 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 9 (mostly Q5_1)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =  90.75 KB\n",
      "llama_model_load_internal: mem required  = 11359.05 MB (+ 1608.00 MB per state)\n",
      "llama_init_from_file: kv self size  = 1600.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "# from langchain.chains import VectorDBQA\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "# N_CONTEXT = 1024\n",
    "# model_path = './models/ggml-alpaca-7b-q4.bin'\n",
    "\n",
    "N_CONTEXT = 2048\n",
    "model_path = config.LLM_VICUNA_13B\n",
    "\n",
    "llm = LlamaCpp(model_path=model_path, temperature=0.0, n_ctx=N_CONTEXT)\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('{\"memory\": null, \"verbose\": false, \"combine_documents_chain\": {\"memory\": '\n",
      " 'null, \"verbose\": false, \"input_key\": \"input_documents\", \"output_key\": '\n",
      " '\"output_text\", \"llm_chain\": {\"memory\": null, \"verbose\": false, \"prompt\": '\n",
      " '{\"input_variables\": [\"context\", \"question\"], \"output_parser\": null, '\n",
      " '\"partial_variables\": {}, \"template\": \"Use the following pieces of context to '\n",
      " \"answer the question at the end. If you don't know the answer, just say that \"\n",
      " \"you don't know, don't try to make up an \"\n",
      " 'answer.\\\\n\\\\n{context}\\\\n\\\\nQuestion: {question}\\\\nHelpful Answer:\", '\n",
      " '\"template_format\": \"f-string\", \"validate_template\": true, \"_type\": '\n",
      " '\"prompt\"}, \"llm\": {\"model_path\": \"/llms/models/ggml-vic13b-q5_1.bin\", '\n",
      " '\"suffix\": null, \"max_tokens\": 256, \"temperature\": 0.0, \"top_p\": 0.95, '\n",
      " '\"logprobs\": null, \"echo\": false, \"stop_sequences\": [], \"repeat_penalty\": '\n",
      " '1.1, \"top_k\": 40, \"_type\": \"llama.cpp\"}, \"output_key\": \"text\", \"_type\": '\n",
      " '\"llm_chain\"}, \"document_prompt\": {\"input_variables\": [\"page_content\"], '\n",
      " '\"output_parser\": null, \"partial_variables\": {}, \"template\": '\n",
      " '\"{page_content}\", \"template_format\": \"f-string\", \"validate_template\": true, '\n",
      " '\"_type\": \"prompt\"}, \"document_variable_name\": \"context\", '\n",
      " '\"document_separator\": \"\\\\n\\\\n\", \"_type\": \"stuff_documents_chain\"}, '\n",
      " '\"input_key\": \"query\", \"output_key\": \"result\", \"return_source_documents\": '\n",
      " 'true}')\n"
     ]
    }
   ],
   "source": [
    "pprint(qa.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.combine_documents_chain.llm_chain.prompt.template"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  1626.76 ms\n",
      "llama_print_timings:      sample time =    45.87 ms /   101 runs   (    0.45 ms per token)\n",
      "llama_print_timings: prompt eval time =  2264.73 ms /    11 tokens (  205.88 ms per token)\n",
      "llama_print_timings:        eval time = 32664.93 ms /   100 runs   (  326.65 ms per token)\n",
      "llama_print_timings:       total time = 43718.69 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" LangChain is a new programming language that combines the power of Python, Java, C++, and JavaScript. It's designed to be easy to learn and use, with a syntax that's simple and intuitive. LangChain can be used for web development, data analysis, machine learning, and more. It's perfect for beginners and experienced programmers alike!  #LangChain #ProgrammingLanguage #PythonJavaC++JavaScript\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is LangChain?\"\n",
    "base_response = llm(prompt=f\"Question: {question}? Answer: \")\n",
    "base_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' LangChain is a new programming language that combines the power of Python, '\n",
      " \"Java, C++, and JavaScript. It's designed to be easy to learn and use, with a \"\n",
      " \"syntax that's simple and intuitive. LangChain can be used for web \"\n",
      " \"development, data analysis, machine learning, and more. It's perfect for \"\n",
      " 'beginners and experienced programmers alike!  #LangChain '\n",
      " '#ProgrammingLanguage #PythonJavaC++JavaScript')\n"
     ]
    }
   ],
   "source": [
    "pprint(base_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context-Model\n",
      "{'query': 'What is LangChain?', 'result': ' LangChain is a software development framework designed to simplify the creation of applications using large language models (LLMs). It is written in Python and JavaScript, and was initially released in October 2022. LangChain aims to provide a modular and flexible toolkit for developing AI-native applications, with a focus on making it easy for developers to build, test, and deploy LLM-based projects. The LangChain framework includes a number of features and tools, including a gallery of project examples, deployment instructions and templates, a Discord community for discussion, and resources for tracing and debugging.', 'source_documents': [Document(page_content=\"Human: For LangChain! Have you heard of it? AI:  No, I haven't heard of LangChain. Can you tell me more about it? Human: Haha nope, although a lot of people confuse it for that AI: > Finished chain. ' Oh, okay. What is LangChain?'\", metadata={'url': 'https://python.langchain.com/en/latest/modules/memory/types/summary_buffer.html'}), Document(page_content='such as OpenAI or Hugging Face. LangChain is a software development framework designed to simplify the creation of applications using large language models (LLMs). Written in: Python and JavaScript. Initial release: October 2022. LangChain - The A.I-native developer toolkit We started LangChain with the intent to build a modular and flexible framework for developing A.I- ... LangChain explained in', metadata={'url': 'https://python.langchain.com/en/latest/modules/chains/examples/flare.html'}), Document(page_content='of all related terms, papers, methods, etc. Whether implemented in LangChain or not!\\\\nGallery: A collection of our favorite projects that use LangChain. Useful for finding inspiration or seeing how things were done in other applications.\\\\nDeployments: A collection of instructions, code snippets, and template repositories for deploying LangChain apps.\\\\nDiscord: Join us on our Discord to discuss all', metadata={'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/sitemap.html'}), Document(page_content='of all related terms, papers, methods, etc. Whether implemented in LangChain or not!\\\\nGallery: A collection of our favorite projects that use LangChain. Useful for finding inspiration or seeing how things were done in other applications.\\\\nDeployments: A collection of instructions, code snippets, and template repositories for deploying LangChain apps.\\\\nTracing: A guide on using tracing in LangChain', metadata={'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/sitemap.html'})]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  1626.76 ms\n",
      "llama_print_timings:      sample time =    63.07 ms /   126 runs   (    0.50 ms per token)\n",
      "llama_print_timings: prompt eval time = 78244.98 ms /   400 tokens (  195.61 ms per token)\n",
      "llama_print_timings:        eval time = 48597.17 ms /   125 runs   (  388.78 ms per token)\n",
      "llama_print_timings:       total time = 138900.16 ms\n"
     ]
    }
   ],
   "source": [
    "# print(\"Base-Model:\")\n",
    "# print(llm(prompt=f\"Question: {question}? Answer: \"))\n",
    "print(\"\\nContext-Model\")\n",
    "context_answer = qa(question)\n",
    "print(context_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' LangChain is a software development framework designed to simplify the '\n",
      " 'creation of applications using large language models (LLMs). It is written '\n",
      " 'in Python and JavaScript, and was initially released in October 2022. '\n",
      " 'LangChain aims to provide a modular and flexible toolkit for developing '\n",
      " 'AI-native applications, with a focus on making it easy for developers to '\n",
      " 'build, test, and deploy LLM-based projects. The LangChain framework includes '\n",
      " 'a number of features and tools, including a gallery of project examples, '\n",
      " 'deployment instructions and templates, a Discord community for discussion, '\n",
      " 'and resources for tracing and debugging.')\n"
     ]
    }
   ],
   "source": [
    "pprint(context_answer['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Human: For LangChain! Have you heard of it? AI:  No, I haven't heard of LangChain. Can you tell me more about it? Human: Haha nope, although a lot of people confuse it for that AI: > Finished chain. ' Oh, okay. What is LangChain?'\", metadata={'url': 'https://python.langchain.com/en/latest/modules/memory/types/summary_buffer.html'}),\n",
       " Document(page_content='such as OpenAI or Hugging Face. LangChain is a software development framework designed to simplify the creation of applications using large language models (LLMs). Written in: Python and JavaScript. Initial release: October 2022. LangChain - The A.I-native developer toolkit We started LangChain with the intent to build a modular and flexible framework for developing A.I- ... LangChain explained in', metadata={'url': 'https://python.langchain.com/en/latest/modules/chains/examples/flare.html'}),\n",
       " Document(page_content='of all related terms, papers, methods, etc. Whether implemented in LangChain or not!\\\\nGallery: A collection of our favorite projects that use LangChain. Useful for finding inspiration or seeing how things were done in other applications.\\\\nDeployments: A collection of instructions, code snippets, and template repositories for deploying LangChain apps.\\\\nDiscord: Join us on our Discord to discuss all', metadata={'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/sitemap.html'}),\n",
       " Document(page_content='of all related terms, papers, methods, etc. Whether implemented in LangChain or not!\\\\nGallery: A collection of our favorite projects that use LangChain. Useful for finding inspiration or seeing how things were done in other applications.\\\\nDeployments: A collection of instructions, code snippets, and template repositories for deploying LangChain apps.\\\\nTracing: A guide on using tracing in LangChain', metadata={'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/sitemap.html'}),\n",
       " Document(page_content='explained in 3 minutes - LangChain is a ... Duration: 3:03. Posted: Apr 13, 2023. LangChain is a framework built to help you build LLM-powered applications more easily by providing you with the following:. LangChain is a framework that enables quick and easy development of applications that make use of Large Language Models, for example, GPT-3. LangChain is a powerful open-source framework for', metadata={'url': 'https://python.langchain.com/en/latest/modules/chains/examples/flare.html'})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb.similarity_search(query=\"What is Langchain?\", k=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base-Model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  1626.76 ms\n",
      "llama_print_timings:      sample time =   117.29 ms /   256 runs   (    0.46 ms per token)\n",
      "llama_print_timings: prompt eval time =  3114.97 ms /    16 tokens (  194.69 ms per token)\n",
      "llama_print_timings:        eval time = 83151.58 ms /   256 runs   (  324.81 ms per token)\n",
      "llama_print_timings:       total time = 108608.18 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\n'\n",
      " '\\n'\n",
      " 'To create a chatbot using prompt templates, you can follow these steps:\\n'\n",
      " '\\n'\n",
      " '1. Create a new file in your project directory and name it `prompts.json`.\\n'\n",
      " '2. Define the prompts that your chatbot will ask the user. Each prompt '\n",
      " 'should be an object with a `text` property for the message to display and a '\n",
      " '`value` property for the expected response from the user. For example:\\n'\n",
      " '```perl\\n'\n",
      " '{\\n'\n",
      " '  \"text\": \"What is your name?\",\\n'\n",
      " '  \"value\": \"user_name\"\\n'\n",
      " '}\\n'\n",
      " '```\\n'\n",
      " '3. Save the `prompts.json` file.\\n'\n",
      " '4. In your chatbot code, load the prompts from the `prompts.json` file using '\n",
      " 'the `readFileSync` method:\\n'\n",
      " '```javascript\\n'\n",
      " \"const fs = require('fs');\\n\"\n",
      " \"const prompts = JSON.parse(fs.readFileSync('./prompts.json', 'utf8'));\\n\"\n",
      " '```\\n'\n",
      " '5. Define a function to ask the user for their response to each prompt. This '\n",
      " \"function should display the prompt, wait for the user's response, and then \"\n",
      " 'check if the response matches the `value` property of the prompt object. If '\n",
      " 'the responses match, the')\n",
      "\n",
      "Context-Model\n",
      "(' You can create a custom prompt template by using a ChatPromptTemplate and '\n",
      " 'combining one or more MessagePromptTemplates. The ChatPromptTemplate allows '\n",
      " 'you to format the prompt in a structured way, while the '\n",
      " 'MessagePromptTemplates provide specific prompts for different types of '\n",
      " 'messages. To use the custom prompt template, you can call the '\n",
      " '`format_prompt` method on the ChatPromptTemplate and pass in any necessary '\n",
      " 'variables or data. The resulting PromptValue can then be converted to a '\n",
      " 'string or Message object as needed.')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  1626.76 ms\n",
      "llama_print_timings:      sample time =    52.13 ms /   114 runs   (    0.46 ms per token)\n",
      "llama_print_timings: prompt eval time = 71395.72 ms /   368 tokens (  194.01 ms per token)\n",
      "llama_print_timings:        eval time = 38824.33 ms /   114 runs   (  340.56 ms per token)\n",
      "llama_print_timings:       total time = 120333.10 ms\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I use prompt templates for creating a chat bot?\"\n",
    "print(\"Base-Model:\")\n",
    "pprint(llm(prompt=f\"Question: {question}? Answer: \"))\n",
    "print(\"\\nContext-Model\")\n",
    "context_answer = qa(question)\n",
    "pprint(context_answer['result'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base-Model:\n",
      "('\\n'\n",
      " '\\n'\n",
      " 'To create a chatbot using prompt templates, you can follow these steps:\\n'\n",
      " '\\n'\n",
      " '1. Create a new file in your project directory and name it `prompts.json`.\\n'\n",
      " '2. Define the prompts that your chatbot will ask the user. Each prompt '\n",
      " 'should be an object with a `text` property for the message to display and a '\n",
      " '`value` property for the expected response from the user. For example:\\n'\n",
      " '```perl\\n'\n",
      " '{\\n'\n",
      " '  \"text\": \"What is your name?\",\\n'\n",
      " '  \"value\": \"user_name\"\\n'\n",
      " '}\\n'\n",
      " '```\\n'\n",
      " '3. Save the `prompts.json` file.\\n'\n",
      " '4. In your chatbot code, load the prompts from the `prompts.json` file using '\n",
      " 'the `readFileSync` method:\\n'\n",
      " '```javascript\\n'\n",
      " \"const fs = require('fs');\\n\"\n",
      " \"const prompts = JSON.parse(fs.readFileSync('./prompts.json', 'utf8'));\\n\"\n",
      " '```\\n'\n",
      " '5. Define a function to ask the user for their response to each prompt. This '\n",
      " \"function should display the prompt, wait for the user's response, and then \"\n",
      " 'check if the response matches the `value` property of the prompt object. If '\n",
      " 'the responses match, the')\n",
      "\n",
      "Context-Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  1626.76 ms\n",
      "llama_print_timings:      sample time =   116.70 ms /   256 runs   (    0.46 ms per token)\n",
      "llama_print_timings: prompt eval time =  3037.80 ms /    16 tokens (  189.86 ms per token)\n",
      "llama_print_timings:        eval time = 83086.60 ms /   256 runs   (  324.56 ms per token)\n",
      "llama_print_timings:       total time = 108433.02 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' You can create a custom prompt template by using a ChatPromptTemplate and '\n",
      " 'combining one or more MessagePromptTemplates. The ChatPromptTemplate allows '\n",
      " 'you to format the prompt in a structured way, while the '\n",
      " 'MessagePromptTemplates provide specific prompts for different types of '\n",
      " 'messages. To use the custom prompt template, you can call the '\n",
      " '`format_prompt` method on the ChatPromptTemplate and pass in any necessary '\n",
      " 'variables or data. The resulting PromptValue can then be converted to a '\n",
      " 'string or Message object as needed.')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  1626.76 ms\n",
      "llama_print_timings:      sample time =    52.00 ms /   114 runs   (    0.46 ms per token)\n",
      "llama_print_timings: prompt eval time = 71560.73 ms /   368 tokens (  194.46 ms per token)\n",
      "llama_print_timings:        eval time = 38451.99 ms /   114 runs   (  337.30 ms per token)\n",
      "llama_print_timings:       total time = 120126.44 ms\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I use prompt templates for creating a chat bot?\"\n",
    "print(\"Base-Model:\")\n",
    "base_answer = llm(prompt=f\"Question: {question}? Answer: \")\n",
    "pprint(base_answer)\n",
    "print(\"\\nContext-Model\")\n",
    "context_answer = qa(question)\n",
    "pprint(context_answer['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://python.langchain.com/en/latest/modules/prompts/prompt_templates/examples/custom_prompt_template.html\n",
      "of default prompt templates here . Creating a Custom Prompt Template # There are essentially two distinct prompt templates available - string prompt templates and chat prompt templates. String prompt templates provides a simple prompt in string format, while chat prompt templates produces a more structured prompt to be used with a chat API. In this guide, we will create a custom prompt using a\n",
      "https://python.langchain.com/en/latest/modules/prompts.html\n",
      "retrying if necessary). Getting Started # Getting Started Go Deeper # Prompt Templates Chat Prompt Template Example Selectors Output Parsers\n",
      "https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/figma.html\n",
      "have no idea if the Jon Carmack thing makes for better code. YMMV. # See https://python.langchain.com/en/latest/modules/models/chat/getting_started.html for chat info system_prompt_template = \"\"\"You are expert coder Jon Carmack. Use the provided design context to create idomatic HTML/CSS code as possible based on the user request. Everything must be inline in one file and your response must be\n",
      "https://python.langchain.com/en/latest/modules/models/chat/integrations/openai.html\n",
      ". You can build a ChatPromptTemplate from one or more MessagePromptTemplates . You can use ChatPromptTemplate ’s format_prompt – this returns a PromptValue , which you can convert to a string or Message object, depending on whether you want to use the formatted value as input to an llm or chat model. For convenience, there is a from_template method exposed on the template. If you were to use this\n",
      "https://python.langchain.com/en/latest/getting_started/getting_started.html\n",
      "= \" {text} \" human_message_prompt = HumanMessagePromptTemplate . from_template ( human_template ) chat_prompt = ChatPromptTemplate . from_messages ([ system_message_prompt , human_message_prompt ]) chain = LLMChain ( llm = chat , prompt = chat_prompt ) chain . run ( input_language = \"English\" , output_language = \"French\" , text = \"I love programming.\" ) # -> \"J'aime programmer.\" Agents with Chat\n"
     ]
    }
   ],
   "source": [
    "results = vectordb.similarity_search(query=question, k=5)\n",
    "for r in results:\n",
    "    print(r.metadata['url'])\n",
    "    print(r.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='of default prompt templates here . Creating a Custom Prompt Template # There are essentially two distinct prompt templates available - string prompt templates and chat prompt templates. String prompt templates provides a simple prompt in string format, while chat prompt templates produces a more structured prompt to be used with a chat API. In this guide, we will create a custom prompt using a', metadata={'url': 'https://python.langchain.com/en/latest/modules/prompts/prompt_templates/examples/custom_prompt_template.html'}),\n",
       " Document(page_content='have no idea if the Jon Carmack thing makes for better code. YMMV. # See https://python.langchain.com/en/latest/modules/models/chat/getting_started.html for chat info system_prompt_template = \"\"\"You are expert coder Jon Carmack. Use the provided design context to create idomatic HTML/CSS code as possible based on the user request. Everything must be inline in one file and your response must be', metadata={'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/figma.html'}),\n",
       " Document(page_content='up the prompt correctly. In the below prompt, we have two input keys: one for the actual input, another for the input from the Memory class. Importantly, we make sure the keys in the PromptTemplate and the ConversationBufferMemory match up ( chat_history ). template = \"\"\"You are a chatbot having a conversation with a human. {chat_history} Human: {human_input} Chatbot:\"\"\" prompt = PromptTemplate (', metadata={'url': 'https://python.langchain.com/en/latest/modules/memory/examples/adding_memory.html'}),\n",
       " Document(page_content='. You can build a ChatPromptTemplate from one or more MessagePromptTemplates . You can use ChatPromptTemplate ’s format_prompt – this returns a PromptValue , which you can convert to a string or Message object, depending on whether you want to use the formatted value as input to an llm or chat model. For convenience, there is a from_template method exposed on the template. If you were to use this', metadata={'url': 'https://python.langchain.com/en/latest/modules/models/chat/integrations/openai.html'}),\n",
       " Document(page_content='use of templating by using a MessagePromptTemplate . You can build a ChatPromptTemplate from one or more MessagePromptTemplate s. You can use ChatPromptTemplate ’s format_prompt – this returns a PromptValue , which you can convert to a string or Message object, depending on whether you want to use the formatted value as input to an llm or chat model. For convenience, there is a from_template', metadata={'url': 'https://python.langchain.com/en/latest/getting_started/getting_started.html'})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb.similarity_search(query=base_answer, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='of default prompt templates here . Creating a Custom Prompt Template # There are essentially two distinct prompt templates available - string prompt templates and chat prompt templates. String prompt templates provides a simple prompt in string format, while chat prompt templates produces a more structured prompt to be used with a chat API. In this guide, we will create a custom prompt using a', metadata={'url': 'https://python.langchain.com/en/latest/modules/prompts/prompt_templates/examples/custom_prompt_template.html'}),\n",
       " Document(page_content='retrying if necessary). Getting Started # Getting Started Go Deeper # Prompt Templates Chat Prompt Template Example Selectors Output Parsers', metadata={'url': 'https://python.langchain.com/en/latest/modules/prompts.html'}),\n",
       " Document(page_content='have no idea if the Jon Carmack thing makes for better code. YMMV. # See https://python.langchain.com/en/latest/modules/models/chat/getting_started.html for chat info system_prompt_template = \"\"\"You are expert coder Jon Carmack. Use the provided design context to create idomatic HTML/CSS code as possible based on the user request. Everything must be inline in one file and your response must be', metadata={'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/figma.html'}),\n",
       " Document(page_content='. You can build a ChatPromptTemplate from one or more MessagePromptTemplates . You can use ChatPromptTemplate ’s format_prompt – this returns a PromptValue , which you can convert to a string or Message object, depending on whether you want to use the formatted value as input to an llm or chat model. For convenience, there is a from_template method exposed on the template. If you were to use this', metadata={'url': 'https://python.langchain.com/en/latest/modules/models/chat/integrations/openai.html'}),\n",
       " Document(page_content='= \" {text} \" human_message_prompt = HumanMessagePromptTemplate . from_template ( human_template ) chat_prompt = ChatPromptTemplate . from_messages ([ system_message_prompt , human_message_prompt ]) chain = LLMChain ( llm = chat , prompt = chat_prompt ) chain . run ( input_language = \"English\" , output_language = \"French\" , text = \"I love programming.\" ) # -> \"J\\'aime programmer.\" Agents with Chat', metadata={'url': 'https://python.langchain.com/en/latest/getting_started/getting_started.html'})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb.similarity_search(query=context_answer, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base-Model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  1626.76 ms\n",
      "llama_print_timings:      sample time =    42.80 ms /    94 runs   (    0.46 ms per token)\n",
      "llama_print_timings: prompt eval time =  2862.22 ms /    14 tokens (  204.44 ms per token)\n",
      "llama_print_timings:        eval time = 29810.59 ms /    93 runs   (  320.54 ms per token)\n",
      "llama_print_timings:       total time = 40799.23 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The map\\_reduce chain type is a data structure that represents a sequence of map and reduce operations on a dataset. It is used to represent the intermediate results of a MapReduce job, allowing multiple jobs to be chained together to form a complete workflow. The map\\_reduce chain type is similar to the pipeline chain type in that it allows for the composition of data processing operations, but it is specifically designed for MapReduce workflows.\n",
      "Context-Model:\n",
      "{'query': 'What is the map_reduce chain type?', 'result': ' The map\\\\_reduce chain type is a specific type of chain in the langchain library that is used for summarization. It can be loaded using the load\\\\_summarize\\\\_chain function and is typically used with the OpenAIModerationChain class to pass input through a moderation endpoint before processing.', 'source_documents': [Document(page_content='una task force dedicata ai crimini degli oligarchi russi. Il Presidente ha anche annunciato l\\'approvazione della legge bipartitica sull\\'infrastruttura, che prevede investimenti per la ricostruzione dell\\'America. Questo porterà a creare posti\" The map_reduce Chain # This sections shows results of using the map_reduce Chain to do summarization. chain = load_summarize_chain ( llm , chain_type =', metadata={'url': 'https://python.langchain.com/en/latest/modules/chains/index_examples/summarize.html'}), Document(page_content=', callbacks : Optional [ Union [ List [ langchain.callbacks.base.BaseCallbackHandler ] , langchain.callbacks.base.BaseCallbackManager ] ] = None , ** kwargs : Any ) → langchain.chains.mapreduce.MapReduceChain [source] # Construct a map-reduce chain that uses the chain for map and reduce. pydantic model langchain.chains. OpenAIModerationChain [source] # Pass input through a moderation endpoint. To', metadata={'url': 'https://python.langchain.com/en/latest/reference/modules/chains.html'}), Document(page_content='of these types, please see this notebook . There are two ways to load different chain types. First, you can specify the chain type argument in the from_chain_type method. This allows you to pass in the name of the chain type you want to use. For example, in the below we change the chain type to map_reduce . qa = RetrievalQA . from_chain_type ( llm = OpenAI (), chain_type = \"map_reduce\" , retriever', metadata={'url': 'https://python.langchain.com/en/latest/modules/chains/index_examples/vector_db_qa.html'}), Document(page_content='President Biden and Vice President Harris have passed legislation to help struggling families and rebuild America\\'s infrastructure.\" Intermediate Steps We can also return the intermediate steps for map_reduce chains, should we want to inspect them. This is done with the return_map_steps variable. chain = load_summarize_chain ( OpenAI ( temperature = 0 ), chain_type = \"map_reduce\" ,', metadata={'url': 'https://python.langchain.com/en/latest/modules/chains/index_examples/summarize.html'})]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  1626.76 ms\n",
      "llama_print_timings:      sample time =    29.13 ms /    63 runs   (    0.46 ms per token)\n",
      "llama_print_timings: prompt eval time = 93919.06 ms /   482 tokens (  194.85 ms per token)\n",
      "llama_print_timings:        eval time = 21528.15 ms /    62 runs   (  347.23 ms per token)\n",
      "llama_print_timings:       total time = 121196.83 ms\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the map_reduce chain type?\"\n",
    "print(\"Base-Model:\")\n",
    "base_answer = llm(prompt=f\"Question: {question}? Answer: \")\n",
    "print(base_answer)\n",
    "print(\"Context-Model:\")\n",
    "context_answer = qa(question)\n",
    "print(context_answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:chromadb:Using embedded DuckDB without persistence: data will be transient\n",
      "WARNING:chromadb.api.models.Collection:No embedding_function provided, using default embedding function: SentenceTransformerEmbeddingFunction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383.0272528185579\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import chromadb\n",
    "\n",
    "client = chromadb.Client()\n",
    "\n",
    "collection = client.create_collection('sample_collection')\n",
    "\n",
    "documents = []\n",
    "metadatas = []\n",
    "\n",
    "for url, chunk in langchain_doc_chunks:\n",
    "        documents.append(chunk)\n",
    "        metadatas.append({'url': url})\n",
    "\n",
    "collection.add(\n",
    "    documents=documents, # we embed for you, or bring your own\n",
    "    metadatas=metadatas, # filter on arbitrary metadata!\n",
    "    ids=[str(x) for x in range(len(documents))], # must be unique for each doc \n",
    ")\n",
    "\n",
    "print(np.mean([len(x) for x in documents]))\n",
    "print(np.max([len(x) for x in documents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Welcome to LangChain # LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model, but will also be: Data-aware : connect a language model to other sources of data Agentic : allow a language model to interact with its environment The LangChain framework is designed', 'is designed around these principles. This is the Python specific portion of the documentation. For a purely conceptual guide to LangChain, see here . For the JavaScript documentation, see here . Getting Started # How to get started using LangChain to create an Language Model application. Quickstart Guide Concepts and terminology. Concepts and terminology Tutorials created by community experts and', 'experts and presented on YouTube. Tutorials Modules # These modules are the core abstractions which we view as the building blocks of any LLM-powered application. For each module LangChain provides standard, extendable interfaces. LangChain also provides external integrations and even end-to-end implementations for off-the-shelf use. The docs for each module contain quickstart examples, how-to', 'examples, how-to guides, reference docs, and conceptual guides. The modules are (from least to most complex): Models : Supported model types and integrations. Prompts : Prompt management, optimization, and serialization. Memory : Memory refers to state that is persisted between calls of a chain/agent. Indexes : Language models become much more powerful when combined with application-specific data', 'data - this module contains interfaces and integrations for loading, querying and updating external data. Chains : Chains are structured sequences of calls (to an LLM or to a different utility). Agents : An agent is a Chain in which an LLM, given a high-level directive and a set of tools, repeatedly decides an action, executes the action and observes the outcome until the high-level directive is', 'directive is complete. Callbacks : Callbacks let you log and stream the intermediate steps of any chain, making it easy to observe, debug, and evaluate the internals of an application. Use Cases # Best practices and built-in implementations for common LangChain use cases: Autonomous Agents : Autonomous agents are long-running agents that take many steps in an attempt to accomplish an objective.', 'an objective. Examples include AutoGPT and BabyAGI. Agent Simulations : Putting agents in a sandbox and observing how they interact with each other and react to events can be an effective way to evaluate their long-range reasoning and planning abilities. Personal Assistants : One of the primary LangChain use cases. Personal assistants need to take actions, remember interactions, and have knowledge', 'and have knowledge about your data. Question Answering : Another common LangChain use case. Answering questions over specific documents, only utilizing the information in those documents to construct an answer. Chatbots : Language models love to chat, making this a very natural use of them. Querying Tabular Data : Recommended reading if you want to use language models to query structured data', 'structured data (CSVs, SQL, dataframes, etc). Code Understanding : Recommended reading if you want to use language models to analyze code. Interacting with APIs : Enabling language models to interact with APIs is extremely powerful. It gives them access to up-to-date information and allows them to take actions. Extraction : Extract structured information from text. Summarization : Compressing', ': Compressing longer documents. A type of Data-Augmented Generation. Evaluation : Generative models are hard to evaluate with traditional metrics. One promising approach is to use language models themselves to do the evaluation. Reference Docs # Full documentation on all methods, classes, installation methods, and integration setups for LangChain. Reference Documentation Ecosystem # LangChain']\n"
     ]
    }
   ],
   "source": [
    "print(documents[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "results = collection.query(\n",
    "    query_texts=\"What is a document loader?\",\n",
    "    n_results=5,\n",
    "    # where={\"metadata_field\": \"is_equal_to_this\"}, # optional filter\n",
    "    # where_document={\"$contains\":\"search_string\"}  # optional filter\n",
    ")\n",
    "print(len(results['documents'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['4289', '6653', '6956', '6858', '6655']],\n",
       " 'embeddings': None,\n",
       " 'documents': [[', the loader\\nwill track additional metadata like the page number and text type (i.e. title, narrative text)\\nwhen that information is available.',\n",
       "   'the functionality we dive deep on those topics. For an overview of everything related to this, please see the below notebook for getting started: Getting Started We then provide a deep dive on the four main components. Document Loaders How to load documents from a variety of sources. Text Splitters An overview of the abstractions and implementions around splitting text. VectorStores An overview of',\n",
       "   \". load () page_data [Document(page_content='Introduction to GitBook\\\\nGitBook is a modern documentation platform where teams can document everything from products to internal knowledge bases and APIs.\\\\nWe want to help \\\\nteams to work more efficiently\\\\n by creating a simple yet powerful platform for them to \\\\nshare their knowledge\\\\n.\\\\nOur mission is to make a \\\\nuser-friendly\\\\n and \\\\ncollaborative\\\\n\",\n",
       "   'Use: Docugami Loader for Document QA # You can use the Docugami Loader like a standard loader for Document QA over multiple docs, albeit with much better chunks that follow the natural contours of the document. There are many great tutorials on how to do this, e.g. this one . We can just use the same code, but use the DocugamiLoader for better chunking, instead of loading text or PDF files',\n",
       "   'Document Loaders # Note Conceptual Guide Combining language models with your own text data is a powerful way to differentiate them.\\nThe first step in doing this is to load the data into “Documents” - a fancy way of say some pieces of text.\\nThe document loader is aimed at making this easy. The following document loaders are provided: Transform loaders # These transform loaders transform data from a']],\n",
       " 'metadatas': [[{'url': 'https://python.langchain.com/en/latest/integrations/unstructured.html'},\n",
       "   {'url': 'https://python.langchain.com/en/latest/modules/indexes.html'},\n",
       "   {'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/gitbook.html'},\n",
       "   {'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/docugami.html'},\n",
       "   {'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders.html'}]],\n",
       " 'distances': [[0.8244873285293579,\n",
       "   0.8418466448783875,\n",
       "   0.8489056825637817,\n",
       "   0.8808059096336365,\n",
       "   0.8957613706588745]]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
