{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Making Document-Level Information Extraction\\nRight for the Right Reasons\\nLiyan Tang1, Dhruv Rajan1, Suyash Mohan2, Abhijeet Pradhan3, R. Nick Bryan1, Greg Durrett1\\n1The University of Texas at Austin\\n2University of Pennsylvania\\n3Galileo CDS Inc.\\nlytang@utexas.edu, dhruv.rajan@utexas.edu\\nSuyash.Mohan@pennmedicine.upenn.edu, ap@galileocds.com\\nnick.bryan@austin.utexas.edu, gdurrett@cs.utexas.edu\\nAbstract\\nDocument-level models for information ex-\\ntraction tasks like slot-ﬁlling are ﬂexible: they\\ncan be applied to settings where information\\nis not necessarily localized in a single sen-\\ntence. For example, key features of a diag-\\nnosis in a radiology report may not be ex-\\nplicitly stated in one place, but nevertheless\\ncan be inferred from parts of the report’s text.\\nHowever, these models can easily learn spuri-\\nous correlations between labels and irrelevant\\ninformation. This work studies how to en-\\nsure that these models make correct inferences\\nfrom complex text and make those inferences\\nin an auditable way: beyond just being right,\\nare these models “right for the right reasons?”\\nWe experiment with post-hoc evidence extrac-\\ntion in a predict-select-verify framework using\\nfeature attribution techniques. We show that\\nregularization with small amounts of evidence\\nsupervision during training can substantially\\nimprove the quality of extracted evidence. We\\nevaluate on two domains: a small-scale la-\\nbeled dataset of brain MRI reports and a large-\\nscale modiﬁed version of DocRED (Yao et al.,\\n2019) and show that models’ plausibility can\\nbe improved with no loss in accuracy.1\\n1 Introduction\\nDocument-level information extraction (Yao et al.,\\n2019; Christopoulou et al., 2019; Xiao et al., 2020;\\nGuoshun et al., 2020) has seen great strides due to\\nthe rise of pre-trained models (Devlin et al., 2019).\\nBut in high-stakes domains like medical informa-\\ntion extraction (Irvin et al., 2019; McDermott et al.,\\n2020; Smit et al., 2020), machine learning models\\nare still too error-prone to use broadly. Since they\\nare not perfect, they typically play the role of as-\\nsisting users in tasks like building cohorts (Pons\\net al., 2016) or in providing clinical decision sup-\\nport (Demner-Fushman et al., 2009).\\n1Code available at https://github.com/\\nLiyan06/DocumentIE .\\nTransformer\\n[0]Severe encephalomalacia in the temporal lobes a nd\\nfrontal lobes bilaterally with reactive gliosis in the left\\nfrontal lobe. [1] Moderate enlargement of the ventricular\\nsystem. [2] No abnormal enhancement. [3] Near completeopaciﬁcation of the left maxillary sinus. …evidence\\tsents: \\t0,\\t1\\nInterpretlabel\\t(mass\\teﬀect):\\t\\nnegative\\nAccurate?Plausible? (model\\nevidence agrees w/human-\\nlabeled evidence)Faithful?  (model predicti on\\non evidence agrees w/full doc)Figure 1: Our basic model setup. A Transformer-based\\nmodel makes document-level predictions on an exam-\\nple of our brain MRI reports. An interpretation method\\nextracts the evidence sentences used by the model. Our\\nsystem is evaluated according to the criteria of accu-\\nracy, faithfulness, and plausibility.\\nTo be most usable in conjunction with users,\\nthese systems should not just produce a decision,\\nbut a justiﬁcation for their answer. The ideal system\\ntherefore obtains high predictive accuracy, but also\\nreturns a rationale that allows a human to verify the\\npredicted label (Rudie et al., 2019).\\nOur goal is to study document-level informa-\\ntion extraction systems that are both accurate and\\nwhich make predictions based on the correct infor-\\nmation (Doshi-Velez and Kim, 2017). This process\\ninvolves identifying what evidence the model actu-\\nally used, verifying the model’s prediction based on\\nthat evidence, and checking whether that evidence\\naligns with what humans would use, which would\\nallow a user to quickly see if the system is correct.\\nFor example, in Figure 1, localizing the prediction\\nofmass effect (a feature expressing whether there\\nis evidence of brain displacement by a mass like aarXiv:2110.07686v2  [cs.CL]  18 May 2022', metadata={'source': '/tmp/tmp7z053w48', 'page': 0}),\n",
       " Document(page_content='tumor) to the ﬁrst two sentences allows a trained\\nuser in a clinical decision support setting to easily\\nverify what was extracted here. Our evidence ex-\\ntraction hews to principles of both faithfulness and\\nplausibility (Jain et al., 2020; Jacovi and Goldberg,\\n2020; Miller, 2019).\\nRather than use complex approaches with inter-\\nmediate latent variables for extraction (Lei et al.,\\n2016), we focus on what can be done with off-\\nthe-shelf pre-trained models (Liu et al., 2019) us-\\ning post-hoc interpretation. We explore various\\ninterpretation methods to ﬁnd key parts of each\\ndocument that were used by the model. We ask\\ntwo questions: ﬁrst, can we identify the document\\nsentences that truly contributed to the prediction\\n(faithfulness)? Using the ranking of sentences pro-\\nvided by an interpretation method, we extract a\\nset of sentences where the model returns nearly\\nthe same prediction as before, thus verifying that\\nthese sentences are a sufﬁcient explanation for the\\nmodel. Second, do these document sentences align\\nwith what users annotated (plausibility)? Unsur-\\nprisingly, we ﬁnd that this alignment is low in a\\nbasic Transformer model.\\nTo further improve the alignment with human\\nannotation, we consider injecting small amounts\\nof sentence-level supervision. Critically, in the\\nbrain MRI extraction setting we consider (see Ta-\\nble 1), large-scale sentence-level annotation is\\nnot available; most instances in the dataset only\\nhave document-level labels from existing clini-\\ncal decision support systems, making it a weakly-\\nsupervised setting (Pruthi et al., 2020; Patel et al.,\\n2020). We explore two methods for using this small\\namount of annotation, chieﬂy based around super-\\nvising or regularizing the model’s behavior. One\\nnotion is entropy maximization: the model should\\nbe uncertain when it isn’t exposed to sufﬁcient evi-\\ndence (Feng et al., 2019). Another is attention regu-\\nlarization where the model is encouraged to attend\\nto key pieces of evidence. While attention is not\\nentirely connected with what the model uses (Jain\\nand Wallace, 2019), we can investigate whether\\nthis leads to a model whose explanations leverage\\nthis information more heavily.\\nWe validate our methods ﬁrst on a small dataset\\nof radiologists’ observations from brain MRIs.\\nThese reports are annotated with document-level\\nkey features related to different aspects of the re-\\nport, which we want to extract in a faithful way. We\\nsee positive results here even in a small-data con-Report Finding\\n[0]Severe encephalomalacia in the temporal lobes and\\nfrontal lobes bilaterally with reactive gliosis in the left\\nfrontal lobe. [1]Moderate enlargement of the ventric-\\nular system .[2]No abnormal enhancement .[3]Near\\ncomplete opaciﬁcation of the left maxillary sinus. ...\\nmass_effect : negative evid:[0, 1] implicit\\nside: bilateral evid:[0] explicit\\nt2: increased evid:[0] implicit\\ncontrast_enhancement : No evid:[2] explicit\\nTable 1: Example from annotated brain MRI reports.\\nLabels and supporting evidence for 4key features are\\nannotated for this example report presented. “Explicit”\\nmeans the label of given key feature can be directly in-\\nferred by the highlighted terms; “implicit” instead indi-\\ncates that it requires domain knowledge and potential\\nreasoning skills to label. We want the model to identify\\nimplicit features while not leveraging dataset biases or\\nreasoning incorrectly about explicit ones.\\ndition, but to understand how this method would\\nscale with larger amounts of data, we adapt the Do-\\ncRED relation extraction task (Yao et al., 2019) to\\nbe a document-level classiﬁcation task. The ques-\\ntion of which sentence in the document describes\\nthe relation between the two entities, if there even\\nis one, is still quite challenging, and we show our\\ntechniques can lead to improvements in a weakly-\\nlabeled setting here as well.\\nOur contributions are (1) We apply evidence\\nextraction methods to document-level classiﬁca-\\ntion and slot-ﬁlling tasks, emphasizing a new brain\\nMRI dataset that we annotate. (2) We explore us-\\ning weak sentence-level supervision in two tech-\\nniques adapted from prior work; (3) We evaluate\\npre-trained models and evidence extraction through\\nvarious interpretation methods for plausibility com-\\npared to human annotation, while ensuring faithful-\\nness of the evidence.\\n2 Background\\n2.1 Motivation\\nWe start with an example from a brain MRI report\\nin Table 1. Medical information extraction involves\\ntasks such as identifying important medical terms\\nfrom text (Irvin et al., 2019; Smit et al., 2020) and\\nnormalizing names into standard concepts using\\ndomain-speciﬁc ontologies (Cho et al., 2017). One\\napplication in clinical decision support, shown here,\\nrequires extracting the values of certain key fea-\\ntures (clinical ﬁndings) from these reports or medi-\\ncal images (Rudie et al., 2021; Duong et al., 2019).', metadata={'source': '/tmp/tmp7z053w48', 'page': 1}),\n",
       " Document(page_content='This extraction should be accurate, but it should\\nalso make predictions that are correctly sourced, to\\nfacilitate review by a radiologist or someone else\\nusing the system (Rauschecker et al., 2020; Cook\\net al., 2018).\\nThe ﬁnding section of a brain MRI report often\\ndescribes these key features in both explicit and\\nimplicit ways. For instance, contrast enhancement,\\none of our key features, is mentioned explicitly\\nmuch of the time; see no abnormal enhancement in\\nthe third sentence. A rule-based system can detect\\nthis type of evidence easily. But some key features\\nare harder to identify and require reasoning over\\ncontext and draw on implicit cues. For example,\\nsevere encephalomalacia in the ﬁrst sentence and\\nenlargement of the ventricular system in the fol-\\nlowing sentence are both implicit signs of positive\\nmass effect and either is sufﬁcient to infer the label.\\nIt is signiﬁcantly harder to built a rule-based extrac-\\ntor for this case. Learning-based systems have the\\npotential to do much better here, but lack of under-\\nstanding about their behavior can lead to hard-to-\\npredict failure modes, such as acausal prediction\\nof key features (e.g., inferring evidence about mass\\neffect from a hypothesized diagnosis somewhere\\nin the report, where the causality is backwards).\\nOur work aims to leverage the ability of learning-\\nbased systems to capture implicit features while\\nimproving their ability to make predictions that are\\nsourced from the correct evidence and can be easily\\nveriﬁed.\\n2.2 Problem Setting\\nThe problem we tackle in this work can be\\nviewed as document-level classiﬁcation. Let D=\\nfx1;:::;x ngbe a document consisting of nsen-\\ntences. The document is annotated with a set of\\nlabels (ti;yi)wheretiis an auxiliary input spec-\\nifying a particular task for this document (e.g.,\\nmass effect) and yiis the label associated with\\nthat task from a discrete label space f1;:::;dg. In\\nour adaptation of the DocRED task, we consider\\nt= (e1;e2)to classify the relationship (if any)\\nbetween a pair of entities (e1;e2)in a document,\\ndeﬁned in Section 4.1.2.\\nOur method takes a pair (D;t)and then com-\\nputes the label ^ytfrom a predictor ^yt=f(D;t).\\nWe can then extract evidence , a set of sentences,\\npost-hoc using a separate procedure gsuch as a\\nfeature attribution method: ^Et=g(f;D;t )Supervision In addition to the labels yt, we as-\\nsume access to a small number of examples with\\nadditional supervision in each domain. That is, for\\na(D;t;y t)triple, we also assume we are given a\\nsetE=fxi1;:::;x imgof ground-truth evidence\\nwith sentence indices fi1;:::;i mg. This evidence\\nshould be sufﬁcient to compute the label, but not al-\\nways necessary; for example, if multiple sentences\\ncan contribute to the prediction, they might all be\\nlisted as supporting evidence here. See Section 3.3\\nfor more details.\\n2.3 Related Work\\nOur work ﬁts into a broader thread of work on in-\\nformation extraction with partial annotation (Han\\net al., 2020). Due to the cost of collecting large-\\nscale data with good quality, distant supervision\\n(DS) (Mintz et al., 2009) and ways to denoise auto-\\nlabeled data from DS (Surdeanu et al., 2012; Wang\\net al., 2018) have been widely explored. However,\\nthe sentence-level setting typically features much\\nless ambiguity about evidence needed to predict\\na relation compared to the document-level setting\\nwe explore. Several document-level RE datasets\\n(Li et al., 2016a; Peng et al., 2017) have been\\nproposed as well as efforts to tackle these tasks\\n(Christopoulou et al., 2019; Xiao et al., 2020; Gu-\\noshun et al., 2020), which we explicitly build from.\\nExplanation techniques To identify the sen-\\ntences that the model considers as evidence, we\\ndraw on a recent body of work in explainable\\nNLP focused on identifying salient features of\\nthe input. These primarily consist of input attri-\\nbution techniques, such as LIME (Ribeiro et al.,\\n2016), input reductions (Li et al., 2016b; Feng\\net al., 2018), attention-based explanations (Bah-\\ndanau et al., 2015) and gradient-based methods\\n(Simonyan et al., 2014; Selvaraju et al., 2017; Sun-\\ndararajan et al., 2017; Shrikumar et al., 2017). In\\npresent work, we extract rationales using com-\\nmonly used model interpretation methods (de-\\nscribed in Section 3.2) and focus on doing a thor-\\nough evaluation of the capabilities of DeepLIFT\\n(Shrikumar et al., 2017) given its competitive per-\\nformance in our interpretation methods comparison\\n(Appendix B).\\nFrameworks for interpretable pipelines Our\\ngoal of building a system grounded in evidence\\ndraws heavily on recent work on attribution tech-\\nniques and model explanations, particularly notions\\nof faithfulness and plausibility. Faithfulness refers', metadata={'source': '/tmp/tmp7z053w48', 'page': 2}),\n",
       " Document(page_content='to how accurately the explanation provided by the\\nmodel truly reﬂects the information it used in the\\nreasoning process (Jain et al., 2020). On the other\\nhand, plausibility indicates to what extent the inter-\\npretation provided by the model makes sense to a\\nperson.2\\n“Select-then-predict ” approaches are one way to\\nenforce faithfulness in pipelines (Jain et al., 2020):\\nimportant snippets from inputs are extracted and\\npassed through a classiﬁer to make predictions.\\nPast work has used hard (Lei et al., 2016) or soft\\n(Zhang et al., 2016) rationales, and other work has\\nexplicitly looked at tradeoffs in the amount of text\\nextracted (Paranjape et al., 2020).\\nJacovi and Goldberg (2020) note several prob-\\nlems with this setup. Our work aims to align model\\nbehavior with what cues we expect a model to\\nuse (plausibility), but uses the predict-select-verify\\nparadigm (Jacovi and Goldberg, 2020) to ensure\\nthat these are actually sufﬁcient cues for the model.\\nLike our work, Pruthi et al. (2020) simultaneously\\ntrained a BERT-based model (Devlin et al., 2019)\\nfor the prediction task and a linear-CRF (Lafferty\\net al., 2001) module on top of it for the evidence ex-\\ntraction task with shared parameters. Compared to\\ntheir work, we focus explicitly on what can be done\\nwith pre-trained models alone, not augmenting the\\nmodel for evidence extraction.\\n3 Methods\\nThe systems we devise take (D;t)pairs as input\\nand return (a) predicted labels ^ytfor eacht; (b)\\nsets of extracted evidence sentences ^Etfrom an\\ninterpretation method. Figure 1 shows the basic\\nsetting.\\n3.1 Transformer Classiﬁcation Model\\nWe use RoBERTa (Liu et al., 2019) as our docu-\\nment classiﬁer. RoBERTa is a strong method that\\nholds up even against more recent baselines with\\narchitectures designed for DocRED (Zhou et al.,\\n2021). For each of our two domains, we use differ-\\nent pre-trained weights, as described in the train-\\ning details in Appendix A. The task inputs are de-\\nscribed in Section 4.1.\\n2The ERASER benchmark (DeYoung et al., 2020) is a\\nnotable recent effort to evaluate explanation plausibility. How-\\never, we do not consider it here; we focus on the document-\\nlevel classiﬁcation setting, and many of the ERASER tasks are\\nnot suitable or relevant for the approaches we consider, either\\nbeing not natural (FEVER) or not having the same challenges\\nas document-level classiﬁcation.3.2 Interpretation for Evidence Extraction\\nGiven any interpretation method as well as our\\nmodel ^yt=f(D;t), we compute attribution scores\\nwith respect to the predicted class ytfor each token\\nin the RoBERTa input representation. We then\\naverage over the absolute value of attribution score\\nfor each token in that sentence to give sentence-\\nlevel scoresfs1;:::;s ng. These give us a ranking\\nof the sentences. Given a ﬁxed number of evidence\\nsentenceskto extract, we can extract the top k\\nsentences by these scores.\\nWe experiment with the following four widely\\nused interpretation techniques in the present work.\\nLIME (Ribeiro et al., 2016) offers explanations of\\nan input by approximating the model’s predictions\\nlocally with an interpretable model. Input Gra-\\ndient (Hechtlinger, 2016) and Integrated Gradi-\\nents (Sundararajan et al., 2017) use gradients of\\nthe label with respect to the input to assess input\\nimportance; Integrated Gradients approximates the\\nintegral of this gradient with respect to the input\\nalong a straight path from a reference baseline.3\\nDeepLIFT (Shrikumar et al., 2017) attributes the\\nchange in the output from a reference output in\\nterms of the difference in input from the reference\\ninput. Unless stated otherwise, we use DeepLIFT\\nas our interpretation method, since it achieves the\\nbest results (comparable to Input Gradient) among\\nthe four interpretation options. Full comparison of\\ninterpretation methods is in Appendix B.\\nTo verify the extracted evidence (Jacovi\\nand Goldberg, 2020), our main technique\\n(SUFFICIENT ) feeds the model increasingly larger\\nsubsets of the document ranked by attribution\\nscores (e.g., ﬁrst fsmaxg, thenfsmax;s2nd-maxg,\\netc.) until it (a) makes the same prediction as when\\ntaking the whole document as input and (b) as-\\nsigns that prediction at least \\x15times the probabil-\\nity4when the whole document is taken as input.\\nWe consider this attribution faithful: it is a subset\\nof the input supporting the model’s decision judged\\nas important by the attribution method.\\n3.3 Improving Evidence Extraction\\nWhile many document-level extraction settings do\\nnot have sentence-level attributions labeled for ev-\\n3We use the most typical baseline that consists of replacing\\nthe inputs in Dwith[MASK] tokens from RoBERTa.\\n4The value of\\x15is a tolerance hyper-parameter for selecting\\nsentences and it set to 0:8throughout the experiments. Our\\nmethod is robust to the choice of \\x15in a reasonable range, as\\nshown in Appendix B.', metadata={'source': '/tmp/tmp7z053w48', 'page': 3}),\n",
       " Document(page_content='ery decision, one can in practice annotate a small\\nfraction of a dataset with such ground-truth ratio-\\nnales. This is indeed the case for our brain MRI\\ncase study. Past work has shown signiﬁcant bene-\\nﬁts from integrating this supervision into learning\\n(Strout et al., 2019; Dua et al., 2020; Pruthi et al.,\\n2021).\\nAssume that a subset of our labeled data consists\\nof(D;t;y t;Et)tuples with ground truth evidence\\nsentence indices Et=fi1;:::;i mg. We consider\\ntwo modiﬁcations to our model training, namely at-\\ntention regularization (Pruthi et al., 2021), entropy\\nmaximization (Feng et al., 2018), and their combi-\\nnation. An illustration of both methods is shown in\\nFigure 2.\\nAttention regularization Attention regulariza-\\ntion encourages our model f(D;t)to leverage\\nmore information from Et. Speciﬁcally, let\\nA=f\\x0b1;:::;\\x0b ngbe the attention vector from\\nthe[CLS] token in the ﬁnal layer to all tokens\\ninD. During learning, we add the following loss\\nto the training objective: `attn=\\x00logP\\ni2Et\\x0bi,\\nencouraging the model to attend to any token iin\\nthe labeled sentence-level evidence set.\\nEntropy maximization When there is no sufﬁ-\\ncient information contained in the text to infer any\\npredictions, entropy maximization encourages a\\nmodel to be uncertain, represented by a uniform\\nprobability distribution across all classes (DeYoung\\net al., 2020; Feng et al., 2019). Doing so should\\nencourage the model to notmake predictions based\\non irrelevant sentences. We can achieve this by\\ntaking a reduced document D0=DnEtas in-\\nput by removing evidence Etfrom original doc-\\numentD. We treat (D0;t)pairs as extra training\\nexamples where we aim to maximize the entropy\\n\\x00P\\nyP(yjD0) logP(yjD0)over all possible y.5\\n4 Experiments\\n4.1 Datasets and Evaluation Metrics\\nWe investigate our methods on (a) a small collec-\\ntion of brain MRI reports from radiologists’ obser-\\nvations; and (b) a modiﬁed version of the DocRED\\ndatatset. The statistics for both datatsets are in-\\ncluded in Appendix D. For both datasets, we evalu-\\nate on task accuracy (captured by either accuracy or\\nprediction macro-F1) as well as evidence selection\\n5We found this to work better than enforcing a uniform\\ndistribution over attention, which is much harder for the model\\nto achieve.\\nTransformer[CLS] [0] Severe encephalomalacia … [1] Moderate enlargement … [2] No abnormal … [3] Near complete …Final attention layer\\nLabel Distribution P(y|D)\\nEntropy maximization: (ENTROPY)\\u2028Attention regularization  (ATTN)\\nTransformer\\ndeletes relevant sentence; maximizes prediction entropy\\nTransformerencourages attentions on supporting evidence\\n[CLS] [0] Severe… [1] Moderate … [2] No … [3] Near…\\n[CLS] [0] Severe… [1] Moderate … [2] No … [3] Near…\\nStandard Supervised  Training:  max log P(y|D)Figure 2: An illustration of attention regularization and\\nentropy maximization using the example in Table 1.\\nThe model is predicting the label for key feature t2.\\naccuracy (macro-F1) or precision, measuring how\\nwell the model’s evidence selection aligns with\\nhuman annotations. We will use the SUFFICIENT\\nmethod deﬁned in Section 3.2 to select evidence\\nsentences which guarantee that our predictions on\\nthe given evidence subsets will match the model’s\\npredictions on the full document. For the brain\\nMRI report dataset, we evaluate evidence extrac-\\ntion by precision since human annotators typically\\nonly need to refer to one sentence to reach the con-\\nclusion but our model and baselines may extract\\nmore than one sentence.\\n4.1.1 Brain MRI Reports\\nWe present a new dataset of de-identiﬁed radiol-\\nogy reports from brain MRIs. It consists of the\\n“ﬁndings” sections of reports, which present ob-\\nservations about the image, with labels for pre-\\nselected key features by attending physicians and\\nfellows. Crucially, these features are labeled based\\non the original radiology image , not the report.\\nThe document-level labels are therefore noisy be-\\ncause the radiologists’ labels may disagree with the\\nﬁndings written in the report.\\nA key feature is an observable variable t, which\\ncan take on dtpossible values. We focus on the\\nevaluation of two key features, namely contrast\\nenhancement andmass effect , since they appear\\nin most of manually annotated reports. For our\\nRoBERTa classiﬁcation model, we only feed the\\ndocument and train separate classiﬁers for each key\\nfeature, with no shared parameters between these.\\nAnnotation We have a moderate number (327)\\nof reports that have noisy labels from the process', metadata={'source': '/tmp/tmp7z053w48', 'page': 4}),\n",
       " Document(page_content='above. We treat these as our training set. However,\\nall of these labels are document-level.\\nTo evaluate models’ performance on more ﬁne-\\ngrained evidence labels, we randomly select 86\\nunlabeled reports (not overlapping with the 327\\nfor training) and asked four radiology residents to\\n(1) assign key feature labels and reach consensus,\\nwhile (2) highlighting sentences that support their\\ndecision making. We use Prodigy6as our annota-\\ntion interface. See Appendix E for more details\\nabout our annotation instructions.\\nPseudo sentence-level supervision Since we\\nhave limited number of annotated reports for eval-\\nuation, we need a way to prepare weak sentence-\\nlevel supervision ( Et) while training. To achieve\\nthis, we use sentences selected by our rule-based\\nsystem as pseudo evidence to supervise models’\\nbehavior. We use 10% of this as supervision while\\ntraining for consistency with the DocRED setting.\\nRule-based system Our rule-based system uses\\nkeyword matching to identify instances of mass\\neffect and contrast enhancement in the reports, and\\nnegspaCy to detect negations of these key features.\\nData split For the results in Section 5, we evalu-\\nate on reports that contain ground truth ﬁne-grained\\nannotations for either contrast enhancement or\\nmass effect, respectively. There are 64 and 68 out\\nof 86 documents total in each of these categories.\\nWe call this the BRAIN MRI set. When we restrict\\nto this set for evaluation, all of the documents we\\nstudy where the annotators labeled something re-\\nlated to contrast enhancement end up having an\\nexplicit mention of it. However, for mass effect ,\\nthis is not always the case; Table 9 in Appendix\\nshows an example where mass effect is discussed\\nimplicitly in the ﬁrst sentence.\\n4.1.2 Adapted DocRED\\nDocRED (Yao et al., 2019) is a document-level rela-\\ntion extraction (RE) dataset with large scale human\\nannotation of relevant evidence sentences. Unlike\\nsentence-level RE tasks (Qin et al., 2018; Alt et al.,\\n2020), it requires reading multiple sentences and\\nreasoning about complex interactions between en-\\ntities. We adapt this to a document-level relation\\nclassiﬁcation task: a document Dand two entity\\nmentionse1;e2within the document are provided\\nand the task is to predict the relation rbetweene1\\nande2. We synthesize these examples from the\\n6https://prodi.gyModel Names Input Text\\nDIRECT None\\nFULLDOC Full document\\nENT Sentences containing at least one of\\nthe two query entities\\nFIRST 2 First two sentences from a doc.\\nFIRST 3 First three sentences from a doc.\\nBESTPAIR Two sentences yielding highest pre-\\ndiction prob. (incl. variants using reg-\\nularization)\\nSUFFICIENT Sufﬁcient sentences selected by in-\\nterpretation methods (incl. variants\\nusing regularization)\\nTable 2: Model names used in the experiments and\\ntheir associated evidence given as inputs.\\noriginal dataset and sample random entity pairs\\nfrom documents to which we assign an NAclass to\\nconstruct negative pairs exhibiting no relation.\\nThe model input is represented as:\\n[CLS]<ent-1>[SEP]<ent-2>[SEP]<doc>[SEP] .\\nWe use the encoding of [CLS] in the last layer to\\nmake predictions.\\nTo make the setting more realistic, we do not\\nuse the large-scale evidence annotation and assume\\nthere is limited sentence-level supervision avail-\\nable. To be speciﬁc, we include 10% ﬁne-grained\\nannotations in our adapted DocRED dataset.\\n4.2 Models\\nDue to richer and higher-quality supervisions in the\\nDocRED setting, we conduct a larger set of abla-\\ntions and comparisons there. We compare against\\na subset of these models in the radiology setting.\\nBaselines We consider a number of baselines for\\nadapted DocRED which return both predicted la-\\nbels and evidence. (1) DIRECT predicts the relation\\ndirectly from the entity pairs without any sentences\\nas input, using a model trained with just these in-\\nputs. (2) FULLDOCtakes the full document as se-\\nlected evidence and uses the base RoBERTa model\\n(3)ENTtakes all sentences with entity mentions e1\\nande2as input; (4) FIRST 2, F IRST 3retrieve the\\nﬁrst2and3sentences from a document, respec-\\ntively; and (5) BESTPAIRchooses the best sentence\\npair by ﬁrst taking each individual sentence as in-\\nput to the model and then picking top two sentences\\nhaving highest probabilities on their predictions.\\nSUFFICIENT is our main method for both\\ndatasets, which we then augment with additional\\nsupervision as described in Section 3.3. We use\\nsubscripts attn, entropy, both andnone\\nto represent attention regularization, entropy max-', metadata={'source': '/tmp/tmp7z053w48', 'page': 5}),\n",
       " Document(page_content='imization, the combination of two, and neither.\\nBoth BESTPAIRandSUFFICIENT methods lever-\\nage backbone RoBERTa models trained with loss\\nfunctions mentioned above, differing only in their\\nevidence selection.\\nTable 2 summarizes the abbreviated names of\\nmodels and their inputs. Training details are de-\\nscribed in Appendix A.\\nMetrics We report both the accuracy and F 1for\\nthe model ( Full Doc ) as well as evaluation of Ev-\\nidence selection compared to human judgments,\\neither precision or F 1. We also report results in\\ntheReduced Doc setting, where only the selected\\nevidence sentences are fed to the RoBERTa model\\n(trained over whole documents) as input. For our\\nSUFFICIENT method, this accuracy is the same as\\nthe full method by construction, but note that it\\ncan differ for other methods. This reduced setting\\nserves as a sanity check for the faithfulness of our\\nexplanation techniques.\\nNote once again that accuracy in the Full Doc\\ncase can differ for our methods that are trained with\\ndifferent regularization schemes, as these yield dif-\\nferent models that return different predicted labels\\nin addition to different evidence.\\n5 Results\\n5.1 Results on Brain MRI\\nTable 3 shows the performance of our models and\\nbaselines in terms of label prediction and evidence\\nextraction. For each result, we perform a paired\\nbootstrap test comparing to SUFFICIENT none. We\\nunderline results that are better at a signiﬁcance\\nlevel ofp= 0:05on the corresponding metrics. In\\nthemass effect setting, our SUFFICIENT bothmodel\\nachieves the highest evidence extraction precision\\nof the learning-based models, exceeds FULLDOC,\\nFIRST 2/3, and BESTPAIRon the metric by a large\\nmargin, and nearly matches that of the rule-based\\nsystem. It is difﬁcult to be more reliable than a\\nrule-based system, which will nearly always make\\ncorrectly-sourced predictions. But this model is\\nable to combine that reliability with the higher\\nF1of a learned model . Note that due to the high\\nbase rates of certain ﬁndings, we focus on F 1in-\\nstead of accuracy. We see a similar pattern on con-\\ntrast enhancement as well, although the evidence\\nprecision is lower in that case.\\nThese results show that learning-based systems\\nmake accurate predictions in this domain, and thatModelLabel Evidence\\nFull Doc Reduced Doc\\nAcc F1 Acc F1 Pre Len\\nMass Effect\\nFULLDOC 66.6 42.1 66.6 42.1 16.5 10.1\\nFIRST 2 \\x00 \\x00 82.4 45.2 21.3 2.00\\nFIRST 3 \\x00 \\x00 82.4 45.2 24.0 3.00\\nRULE 77.9 11.8 77.9 11.8 84.8 1.46\\nBESTPAIR none 66.6 42.1 82.4 52.2 24.3 2.00\\nBESTPAIR both 76.7 60.0 79.4 44.3 50.7 2.00\\nSUFFICIENT none 66.6 42.1\\nIdentical to\\nFull Doc16.5 2.84\\nSUFFICIENT attn 69.2 47.6 65.6 2.31\\nSUFFICIENT entropy 45.3 0.0 15.8 2.50\\nSUFFICIENT both 76.7 60.0 77.8 1.51\\nContrast Enhancement\\nFULLDOC 69.5 60.9 69.5 60.9 13.5 10.1\\nFIRST 2 \\x00 \\x00 67.2 55.3 14.1 2.00\\nFIRST 3 \\x00 \\x00 70.3 62.4 14.6 3.00\\nRULE 68.8 56.5 68.8 56.5 87.1 1.67\\nBESTPAIR none 69.5 60.9 73.4 67.7 10.9 2.00\\nBESTPAIR both 90.8 87.2 89.1 88.4 54.7 2.00\\nSUFFICIENT none 69.5 60.9\\nIdentical to\\nFull Doc33.5 2.84\\nSUFFICIENT attn 85.8 81.0 60.7 2.48\\nSUFFICIENT entropy 71.5 59.5 25.2 2.55\\nSUFFICIENT both 90.8 87.2 71.7 1.50\\nTable 3: Model performance on B RAIN MRI. Models\\nare evaluated under two settings by taking (a) full docu-\\nment (Full Doc); (b) selected evidence (Reduced Doc)\\nas inputs. R ULE is the baseline mentioned in Section\\n4.1.1. Prestands for the precision of evidence selection,\\nandLenis the average number of sentences extracted.\\nUnderlined results are better than S UFFICIENT noneon\\nthe corresponding metric according to a paired boot-\\nstrap test with p= 0:05.\\nModelLabel Evidence\\nFull Doc Reduced Doc\\nAcc F1 Acc F1 F1 Len\\nDIRECT \\x00 \\x00 66.4 45.3 \\x00 \\x00\\nFULLDOC 83.0 66.0 83.0 66.0 34.9 8.03\\nFIRST 2 \\x00 \\x00 75.3 58.1 47.9 2.00\\nFIRST 3 \\x00 \\x00 77.5 60.7 44.6 3.00\\nENT \\x00 \\x00 82.4 65.4 61.5 3.93\\nBESTPAIR none 83.0 66.0 73.9 55.3 39.2 2.00\\nBESTPAIR attn 83.2 65.0 73.4 53.5 43.9 2.00\\nBESTPAIR entropy 81.8 64.2 78.5 58.2 52.3 2.00\\nBESTPAIR both 82.7 66.5 81.6 65.3 66.2 2.00\\nSUFFICIENT none 83.0 66.0\\nIdentical to\\nFull Doc67.2 1.42\\nSUFFICIENT attn 83.2 65.0 70.3 1.45\\nSUFFICIENT entropy 81.8 64.2 69.9 1.65\\nSUFFICIENT both 82.7 66.5 73.1 1.65\\nhuman \\x00 \\x00 \\x00 \\x00 \\x00 1.59\\nTable 4: Model performance on adapted Do-\\ncRED. Models are evaluated under two settings as in\\nBRAIN MRI. Underlined results are better than S UFFI-\\nCIENT noneon the corresponding metric according to a\\npaired bootstrap test with p= 0:05.', metadata={'source': '/tmp/tmp7z053w48', 'page': 6}),\n",
       " Document(page_content='ModelMass Effect Ctr. Enhance.\\nMean Max Mean Max\\nSUFFICIENT none 7.3 7.4 28.6 29.8\\nSUFFICIENT both 18.9 19.2 37.9 42.0\\nTable 5: Distributions of attribution mass over ex-\\nplicit cues (“enhancement” for contrast enhancement\\nand “effect” for mass effect ) for our best model and\\nthe baseline. Mean/Max is the mean of instance-wise\\naverage/maximum of the normalized attribution mass\\nfalling on the given token.\\ntheir evidence extraction can be improved with bet-\\nter training, even in spite of the small size of the\\ntraining set. In section 5.2, we focus on the adapted\\nDocRED setting, which allows us to examine our\\nmodel’s performance in a higher-data regime.\\nAttribution scores are more peaked at the oc-\\ncurrence of key terms. We conduct analysis on\\nhow the attribution scores from SUFFICIENT both\\nare peaked around the correct evidence compare to\\nthat from SUFFICIENT noneusing our manually an-\\nnotated set BRAIN MRI . We compute the mean of\\nthe instance-wise average and maximum of the nor-\\nmalized attribution mass falling into a few explicit\\ntokens: enhancement for contrast enhancement and\\neffect for mass effect, which are common explicit\\nindicators in the context of speciﬁed key features.\\nThe results in Table 5 show attribution scores being\\npeaked around the correct terms, highlighting that\\nthese models can be guided to not only make cor-\\nrect predictions but attend to the right information.\\nTable 9 in the Appendix shows visualizations of\\nattribution scores for an example in BRAIN MRI us-\\ning DeepLIFT. Even though baseline models make\\ncorrect predictions, their attribution mass is dif-\\nfused over the document. With the help of regular-\\nization, our model is capable of capturing implicit\\ncues such as downward displacement of the brain\\nstem, although it is trained on an extremely small\\ntraining set with only explicit cues like mass effect\\nin a weak sentence-level supervision framework.\\n5.2 Results on Adapted DocRED\\nComparison to baselines Table 4 shows that the\\nENTbaseline is quite strong at DocRED evidence\\nextraction. However, our best method still exceeds\\nthis method on both label accuracy as well as ev-\\nidence extraction while extracting more succinct\\nexplanations. We see that the ability to extract a\\nvariable-length explanation is key, with FIRST 2,\\nFIRST 3andBESTPAIR performing poorly. No-\\ntably, these methods exhibit a drop in accuracy inthe reduced doc setting for each method compared\\nto the full doc setting, showing that the explana-\\ntions extracted are not faithful.\\nLearning-based models with appropriate reg-\\nularization perform relatively better in this\\nlarger-data setting From Table 3 and Table 4,\\nwe can observe that various regularization tech-\\nniques applied to SUFFICIENT models maintain or\\nimprove overall model performance on both key\\nfeature and relation classiﬁcation. We see that our\\nSUFFICIENT methods do not compromise on ac-\\ncuracy but make predictions based on plausible\\nevidence sets, which is more evident when we have\\nricher training data. We perform further error anal-\\nysis in Appendix F.\\nFaithfulness of techniques One may be con-\\ncerned that, like attention values (Jain and Wallace,\\n2019), our feature attribution methods may not\\nfaithfully reﬂect the computation of the model. We\\nemphasize again that the SUFFICIENT paradigm on\\ntop of the DeepLIFT method isfaithful by our deﬁ-\\nnition. For a model f, we measure the faithfulness\\nby checking the agreement between ^y=f(D;t)\\nandy0=f(^Et;t), where ^Etis the extracted evi-\\ndence we feed into the same model under the re-\\nduced document setting. This is shown for all meth-\\nods in the “Reduced doc” columns in Tables 3 and\\n4. We see a drop in performance from techniques\\nsuch as BESTPAIR: the full model does not make\\nthe same judgment on these evidence subsets, but\\nby deﬁnition it does in the S UFFICIENT setting.\\nAs further evidence of faithfulness, we note that\\nonly a relatively small number of evidence sen-\\ntences, in line with human annotations, are ex-\\ntracted in the SUFFICIENT method. These small\\nsubsets are indicated by feature attribution meth-\\nodsandsufﬁcient to reproduce the original model\\npredictions with high conﬁdence, suggesting that\\nthese explanations are faithful.\\n6 Conclusion\\nIn this work, we develop techniques to employ\\nsmall amount of data to improve reliability of\\ndocument-level IE systems in two domains. We\\nsystematically evaluate our model from perspec-\\ntives of faithfulness and plausibility and show that\\nwe can substantially improve models’ capability in\\nfocusing on supporting evidence while maintain-\\ning their predictive performance, leading to models\\nthat are “right for the right reasons.”', metadata={'source': '/tmp/tmp7z053w48', 'page': 7}),\n",
       " Document(page_content='Acknowledgments\\nThis work was partially supported by NSF Grant\\nIIS-1814522 and a Texas Health Catalyst grant.\\nThanks to Scott Rudkin, Gregory Mittl, Raghav\\nMattay, and Chuan Liang for assistance with the\\nannotation.\\nReferences\\nChristoph Alt, Aleksandra Gabryszak, and Leonhard\\nHennig. 2020. Probing linguistic features of\\nsentence-level representations in neural relation ex-\\ntraction. In Proceedings of ACL .\\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\\ngio. 2015. Neural machine translation by jointly\\nlearning to align and translate. In Proceedings of\\nthe International Conference on Learning Represen-\\ntations (ICLR) .\\nHyejin Cho, Wonjun Choi, and Hyunju Lee. 2017. A\\nmethod for named entity normalization in biomedi-\\ncal articles: application to diseases and plants. BMC\\nBioinformatics , 18(1).\\nFenia Christopoulou, Makoto Miwa, and Sophia Ana-\\nniadou. 2019. Connecting the dots: Document-level\\nneural relation extraction with edge-oriented graphs.\\nInProceedings of the 2019 Conference on Empiri-\\ncal Methods in Natural Language Processing and\\nthe 9th International Joint Conference on Natural\\nLanguage Processing (EMNLP-IJCNLP) . Associa-\\ntion for Computational Linguistics.\\nTessa Cook, James C. Gee, R. Nick Bryan, Jeffrey T.\\nDuda, Po-Hao Chen, Emmanuel Botzolakis, Suyash\\nMohan, Andreas Rauschecker, Jeffrey Rudie, and\\nIlya Nasrallah. 2018. Bayesian network interface\\nfor assisting radiology interpretation and education.\\nInMedical Imaging 2018: Imaging Informatics for\\nHealthcare, Research, and Applications . SPIE.\\nDina Demner-Fushman, Wendy W. Chapman, and\\nClement J. McDonald. 2009. What can natural lan-\\nguage processing do for clinical decision support?\\nJournal of Biomedical Informatics , 42(5):760–772.\\nBiomedical Natural Language Processing.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In Proceedings of the 2019 Conference\\nof the North American Chapter of the Association\\nfor Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long and Short Papers) ,\\npages 4171–4186, Minneapolis, Minnesota. Associ-\\nation for Computational Linguistics.\\nJay DeYoung, Sarthak Jain, Nazneen Fatema Rajani,\\nEric Lehman, Caiming Xiong, Richard Socher, and\\nByron C. Wallace. 2020. ERASER: A benchmark\\nto evaluate rationalized NLP models. In Proceed-\\nings of the 58th Annual Meeting of the Associationfor Computational Linguistics . Association for Com-\\nputational Linguistics.\\nFinale Doshi-Velez and Been Kim. 2017. Towards a\\nrigorous science of interpretable machine learning.\\narXiv: Machine Learning .\\nDheeru Dua, Sameer Singh, and Matt Gardner. 2020.\\nBeneﬁts of intermediate annotations in reading com-\\nprehension. In Proceedings of the 58th Annual Meet-\\ning of the Association for Computational Linguistics ,\\npages 5627–5634, Online. Association for Computa-\\ntional Linguistics.\\nM.T. Duong, J.D. Rudie, J. Wang, L. Xie, S. Mohan,\\nJ.C. Gee, and A.M. Rauschecker. 2019. Convolu-\\ntional neural network for automated FLAIR lesion\\nsegmentation on clinical brain MR imaging. Ameri-\\ncan Journal of Neuroradiology , 40(8):1282–1290.\\nShi Feng, Eric Wallace, and Jordan Boyd-Graber. 2019.\\nMisleading failures of partial-input baselines. In\\nProceedings of the 57th Annual Meeting of the Asso-\\nciation for Computational Linguistics . Association\\nfor Computational Linguistics.\\nShi Feng, Eric Wallace, Alvin Grissom II, Mohit Iyyer,\\nPedro Rodriguez, and Jordan Boyd-Graber. 2018.\\nPathologies of neural models make interpretations\\ndifﬁcult. In Proceedings of the 2018 Conference on\\nEmpirical Methods in Natural Language Processing .\\nAssociation for Computational Linguistics.\\nNan Guoshun, Guo Zhijiang, Sekuli ´c Ivan, and Lu Wei.\\n2020. Reasoning with latent structure reﬁnement for\\ndocument-level relation extraction. In Proceedings\\nof ACL .\\nSuchin Gururangan, Ana Marasovi ´c, Swabha\\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\\nand Noah A. Smith. 2020. Don’t stop pretraining:\\nAdapt language models to domains and tasks. In\\nProceedings of ACL .\\nXu Han, Tianyu Gao, Yankai Lin, Hao Peng, Yao-\\nliang Yang, Chaojun Xiao, Zhiyuan Liu, Peng Li,\\nJie Zhou, and Maosong Sun. 2020. More data, more\\nrelations, more context and more openness: A re-\\nview and outlook for relation extraction. In Proceed-\\nings of the 1st Conference of the Asia-Paciﬁc Chap-\\nter of the Association for Computational Linguistics\\nand the 10th International Joint Conference on Nat-\\nural Language Processing , pages 745–758, Suzhou,\\nChina. Association for Computational Linguistics.\\nYotam Hechtlinger. 2016. Interpretation of prediction\\nmodels using the input gradient. In Proceedings of\\nthe NeurIPS 2016 Workshop on Interpretable Ma-\\nchine Learning in Complex Systems .\\nJeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu,\\nSilviana Ciurea-Ilcus, Chris Chute, Henrik Mark-\\nlund, Behzad Haghgoo, Robyn Ball, Katie Shpan-\\nskaya, Jayne Seekins, David Mong, Safwan Halabi,\\nJesse Sandberg, Ricky Jones, David Larson, Curtis', metadata={'source': '/tmp/tmp7z053w48', 'page': 8}),\n",
       " Document(page_content='Langlotz, Bhavik Patel, Matthew Lungren, and An-\\ndrew Ng. 2019. Chexpert: A large chest radiograph\\ndataset with uncertainty labels and expert compari-\\nson. Proceedings of the AAAI Conference on Artiﬁ-\\ncial Intelligence , 33:590–597.\\nAlon Jacovi and Yoav Goldberg. 2020. Towards faith-\\nfully interpretable NLP systems: How should we de-\\nﬁne and evaluate faithfulness? In Proceedings of the\\n58th Annual Meeting of the Association for Compu-\\ntational Linguistics . Association for Computational\\nLinguistics.\\nSarthak Jain and Byron C. Wallace. 2019. Attention is\\nnot Explanation. In Proceedings of the 2019 Con-\\nference of the North American Chapter of the Asso-\\nciation for Computational Linguistics: Human Lan-\\nguage Technologies, Volume 1 (Long and Short Pa-\\npers) , pages 3543–3556, Minneapolis, Minnesota.\\nAssociation for Computational Linguistics.\\nSarthak Jain, Sarah Wiegreffe, Yuval Pinter, and By-\\nron C. Wallace. 2020. Learning to faithfully rational-\\nize by construction. In Proceedings of the 58th An-\\nnual Meeting of the Association for Computational\\nLinguistics . Association for Computational Linguis-\\ntics.\\nJohn D. Lafferty, Andrew McCallum, and Fernando\\nC. N. Pereira. 2001. Conditional random ﬁelds:\\nProbabilistic models for segmenting and labeling se-\\nquence data. In Proceedings of the Eighteenth Inter-\\nnational Conference on Machine Learning , ICML\\n’01, page 282–289, San Francisco, CA, USA. Mor-\\ngan Kaufmann Publishers Inc.\\nTao Lei, Regina Barzilay, and Tommi Jaakkola. 2016.\\nRationalizing neural predictions. In Proceedings of\\nthe 2016 Conference on Empirical Methods in Natu-\\nral Language Processing . Association for Computa-\\ntional Linguistics.\\nJiao Li, Yueping Sun, Robin J. Johnson, Daniela Sci-\\naky, Chih-Hsuan Wei, Robert Leaman, Allan Peter\\nDavis, Carolyn J. Mattingly, Thomas C. Wiegers,\\nand Zhiyong Lu. 2016a. BioCreative v CDR task\\ncorpus: a resource for chemical disease relation ex-\\ntraction. Database , 2016:baw068.\\nJiwei Li, Will Monroe, and Dan Jurafsky. 2016b. Un-\\nderstanding neural networks through representation\\nerasure. ArXiv , abs/1612.08220.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\\nRoBERTa: A Robustly Optimized BERT Pretrain-\\ning Approach. In arXiv .\\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\\nweight decay regularization. In International Con-\\nference on Learning Representations .\\nMatthew B. A. McDermott, Tzu-Ming Harry Hsu,\\nWei-Hung Weng, Marzyeh Ghassemi, and Peter\\nSzolovits. 2020. Chexpert++: Approximating thechexpert labeler for speed, differentiability, and\\nprobabilistic output. CoRR , abs/2006.15229.\\nTim Miller. 2019. Explanation in artiﬁcial intelligence:\\nInsights from the social sciences. Artiﬁcial Intelli-\\ngence , 267:1–38.\\nMike Mintz, Steven Bills, Rion Snow, and Daniel Ju-\\nrafsky. 2009. Distant supervision for relation ex-\\ntraction without labeled data. In Proceedings of\\nthe Joint Conference of the 47th Annual Meeting of\\nthe ACL and the 4th International Joint Conference\\non Natural Language Processing of the AFNLP ,\\npages 1003–1011, Suntec, Singapore. Association\\nfor Computational Linguistics.\\nBhargavi Paranjape, Mandar Joshi, John Thickstun,\\nHannaneh Hajishirzi, and Luke Zettlemoyer. 2020.\\nAn information bottleneck approach for controlling\\nconciseness in rationale extraction. In Proceedings\\nof the 2020 Conference on Empirical Methods in\\nNatural Language Processing (EMNLP) . Associa-\\ntion for Computational Linguistics.\\nDhruvesh Patel, Sandeep Konam, and Sai P. Sel-\\nvaraj. 2020. Weakly supervised medication regi-\\nmen extraction from medical conversations. In Clin-\\nicalNLP@EMNLP .\\nNanyun Peng, Hoifung Poon, Chris Quirk, Kristina\\nToutanova, and Wen tau Yih. 2017. Cross-sentence\\nn-ary relation extraction with graph LSTMs. Trans-\\nactions of the Association for Computational Lin-\\nguistics , 5:101–115.\\nEwoud Pons, Loes M. M. Braun, M. G. Myriam\\nHunink, and Jan A. Kors. 2016. Natural language\\nprocessing in radiology: A systematic review. Radi-\\nology , 279(2):329–343. PMID: 27089187.\\nDanish Pruthi, Bhuwan Dhingra, Graham Neubig,\\nand Zachary C. Lipton. 2020. Weakly- and semi-\\nsupervised evidence extraction. In Findings of the\\nAssociation for Computational Linguistics: EMNLP\\n2020 . Association for Computational Linguistics.\\nDanish Pruthi, Bhuwan Dhingra, Livio Baldini Soares,\\nM. Collins, Zachary C. Lipton, Graham Neubig, and\\nWilliam W. Cohen. 2021. Evaluating explanations:\\nHow much do explanations from the teacher aid stu-\\ndents? Transactions of the Association for Compu-\\ntational Linguistics .\\nPengda Qin, Weiran Xu, and William Yang Wang.\\n2018. Robust distant supervision relation extraction\\nvia deep reinforcement learning. In Proceedings of\\nthe 56th Annual Meeting of the Association for Com-\\nputational Linguistics (Volume 1: Long Papers) . As-\\nsociation for Computational Linguistics.\\nAndreas M. Rauschecker, Jeffrey D. Rudie, Long\\nXie, Jiancong Wang, Michael Tran Duong, Em-\\nmanuel J. Botzolakis, Asha M. Kovalovich, John\\nEgan, Tessa C. Cook, R. Nick Bryan, Ilya M.\\nNasrallah, Suyash Mohan, and James C. Gee.\\n2020. Artiﬁcial intelligence system approaching', metadata={'source': '/tmp/tmp7z053w48', 'page': 9}),\n",
       " Document(page_content='neuroradiologist-level differential diagnosis accu-\\nracy at brain MRI. Radiology , 295(3):626–637.\\nMarco Tulio Ribeiro, Sameer Singh, and Carlos\\nGuestrin. 2016. \"why should i trust you?\": Explain-\\ning the predictions of any classiﬁer. In Proceedings\\nof the 22nd ACM SIGKDD International Conference\\non Knowledge Discovery and Data Mining , KDD\\n’16, page 1135–1144, New York, NY , USA. Asso-\\nciation for Computing Machinery.\\nJeffrey Rudie, Long Xie, Jiancong Wang, Jeffrey Duda,\\nJoshua Choi, Raghav Mattay, Po-Hao Chen, R Nick\\nBryan, Emmanuel Botzolakis, Ilya Nasrallah, Tessa\\nCook, Suyash Mohan, James Gee, and Andreas\\nRauschecker. 2019. Artiﬁcial Intelligence System\\nfor Automated Brain MR Diagnosis Performs at\\nLevel of Academic Neuroradiologists and Augments\\nResident Performance. In Proceedings of the Soci-\\nety for Imaging Informatics in Medicine (SIIM) .\\nJeffrey D. Rudie, Jeffrey Duda, Michael Tran Duong,\\nPo-Hao Chen, Long Xie, Robert Kurtz, Jeffrey B.\\nWare, Joshua Choi, Raghav R. Mattay, Emmanuel J.\\nBotzolakis, James C. Gee, R. Nick Bryan, Tessa S.\\nCook, Suyash Mohan, Ilya M. Nasrallah, and An-\\ndreas M. Rauschecker. 2021. Brain MRI deep learn-\\ning and bayesian inference system augments radiol-\\nogy resident performance. Journal of Digital Imag-\\ning, 34(4):1049–1058.\\nRamprasaath R. Selvaraju, Michael Cogswell, Ab-\\nhishek Das, Ramakrishna Vedantam, Devi Parikh,\\nand Dhruv Batra. 2017. Grad-CAM: Visual expla-\\nnations from deep networks via gradient-based lo-\\ncalization. In 2017 IEEE International Conference\\non Computer Vision (ICCV) . IEEE.\\nAvanti Shrikumar, Peyton Greenside, and Anshul Kun-\\ndaje. 2017. Learning important features through\\npropagating activation differences. In Proceedings\\nof the 34th International Conference on Machine\\nLearning - Volume 70 , ICML’17, page 3145–3153.\\nJMLR.org.\\nKaren Simonyan, Andrea Vedaldi, and Andrew Zisser-\\nman. 2014. Deep inside convolutional networks: Vi-\\nsualising image classiﬁcation models and saliency\\nmaps. In Workshop at International Conference on\\nLearning Representations .\\nAkshay Smit, Saahil Jain, Pranav Rajpurkar, Anuj\\nPareek, Andrew Ng, and Matthew Lungren. 2020.\\nCombining automatic labelers and expert annota-\\ntions for accurate radiology report labeling using\\nBERT. In Proceedings of the 2020 Conference on\\nEmpirical Methods in Natural Language Process-\\ning (EMNLP) , pages 1500–1519, Online. Associa-\\ntion for Computational Linguistics.\\nJulia Strout, Ye Zhang, and Raymond Mooney. 2019.\\nDo human rationales improve machine explana-\\ntions? In Proceedings of the 2019 ACL Workshop\\nBlackboxNLP: Analyzing and Interpreting Neural\\nNetworks for NLP , pages 56–62, Florence, Italy. As-\\nsociation for Computational Linguistics.Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.\\nAxiomatic attribution for deep networks. In Pro-\\nceedings of the 34th International Conference on\\nMachine Learning , volume 70 of Proceedings of Ma-\\nchine Learning Research , pages 3319–3328. PMLR.\\nMihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,\\nand Christopher D. Manning. 2012. Multi-instance\\nmulti-label learning for relation extraction. In Pro-\\nceedings of the 2012 Joint Conference on Empirical\\nMethods in Natural Language Processing and Com-\\nputational Natural Language Learning , pages 455–\\n465, Jeju Island, Korea. Association for Computa-\\ntional Linguistics.\\nXiaozhi Wang, Xu Han, Yankai Lin, Zhiyuan Liu, and\\nMaosong Sun. 2018. Adversarial multi-lingual neu-\\nral relation extraction. In Proceedings of the 27th\\nInternational Conference on Computational Linguis-\\ntics, pages 1156–1166, Santa Fe, New Mexico, USA.\\nAssociation for Computational Linguistics.\\nChaojun Xiao, Yuan Yao, Ruobing Xie, Xu Han,\\nZhiyuan Liu, Maosong Sun, Fen Lin, and Leyu Lin.\\n2020. Denoising relation extraction from document-\\nlevel distant supervision. In Proceedings of the 2020\\nConference on Empirical Methods in Natural Lan-\\nguage Processing (EMNLP) . Association for Com-\\nputational Linguistics.\\nYuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin,\\nZhenghao Liu, Zhiyuan Liu, Lixin Huang, Jie Zhou,\\nand Maosong Sun. 2019. DocRED: A large-scale\\ndocument-level relation extraction dataset. In Pro-\\nceedings of the 57th Annual Meeting of the Associa-\\ntion for Computational Linguistics . Association for\\nComputational Linguistics.\\nYe Zhang, Iain Marshall, and Byron C. Wallace. 2016.\\nRationale-augmented convolutional neural networks\\nfor text classiﬁcation. In Proceedings of the 2016\\nConference on Empirical Methods in Natural Lan-\\nguage Processing . Association for Computational\\nLinguistics.\\nWenxuan Zhou, Kevin Huang, Tengyu Ma, and Jing\\nHuang. 2021. Document-level relation extraction\\nwith adaptive thresholding and localized context\\npooling. In Proceedings of the AAAI Conference on\\nArtiﬁcial Intelligence .', metadata={'source': '/tmp/tmp7z053w48', 'page': 10}),\n",
       " Document(page_content='A Implementation Details\\nWe train all RoBERTa models for 15 epochs with\\nearly stopping using 1TITAN-Xp GPU. We use\\nAdamW (Loshchilov and Hutter, 2019) as our op-\\ntimizer and initialize the model with roberta-\\nbase for DocRED and biomed-roberta-\\nbase (Gururangan et al., 2020) for brain MRI data,\\nboth with 125M parameters. The batch size is set to\\n16 for RoBERTa models trained with both attention\\nregularization and entropy maximization and 8 for\\nmodels with other loss functions, and the learning\\nrate is 1e-5 with linear schedule warmup.\\nThe maximum number of tokens in each docu-\\nment is capped at 296 for modiﬁed DocRED and\\n360 for radiology reports. These numbers are cho-\\nsen such that the number of tokens for around 95%\\nof the documents is within these limits. Remaining\\ntokens are clipped from the input. The hidden state\\nof the [CLS] token from the ﬁnal layer is fed as\\ninput to a linear projection head to make predic-\\ntions. The average training time for each model is\\naround 4 GPU hours.\\nB Interpretation Methods Comparison\\nWe evaluate four interpretation methods on SUF-\\nFICIENT none and SUFFICIENT bothusing adapted\\nDocRED. These methods are widely used in the\\nliterature, namely Integrated Gradients, LIME,\\nDeepLIFT, and Input Gradient, as discussed in Sec-\\ntion 3.2. We compare their evidence extraction\\ncapabilities by selecting a wide range of \\x15, which\\ncontrols the number of sentences to be selected.\\nResults are shown in Figure 3. The four tech-\\nniques generally perform similarly, with DeepLIFT\\nand Input Gradient performing slightly better. For\\neach interpretation method, the result of SUFFI-\\nCIENT bothis signiﬁcantly better than that of SUF-\\nFICIENT none. Similar values of \\x15between 0.8 and\\n0.9 (preferring to select more sentences) work well\\nacross all methods. Table 6 shows the comparison\\nover the threshold ( \\x15= 0:8) we choose for our\\nexperiments in Section 5. In general, our method\\nis robust to model interpretation techniques and\\nevidence selection threshold \\x15.\\nSentence ranking step mentioned in Section 3.2\\nrequires 0.3 GPU hour for Input Gradient and\\nDeepLIFT, 2.5 GPU hours for Integrated Gradi-\\nents, and 14 GPU hours for LIME. We choose 30\\nsteps to approximate the integral for Integrated Gra-\\ndients and 100 samples for each input to train the\\nsurrogate interpretable model (a linear model in\\nFigure 3: Evidence F1 on adapted DocRED under four\\nmodel interpretation methods for S UFFICIENT bothover\\na wide range of \\x15.\\nour case) for LIME.\\nSUFFICIENT none SUFFICIENT both\\nLIME 54.0 70.8\\nIntegrated Gradients 60.6 70.3\\nDeepLIFT 67.3 73.1\\nInput Gradient 68.3 73.5\\nTable 6: Evidence F1 on adapted DocRED under four\\nmodel interpretation methods for S UFFICIENT noneand\\nSUFFICIENT bothwhen\\x15= 0:8.\\nC Limitations and Risks\\nThere are a few limitations of our work. First,\\nwe currently test our methods on document-level\\nclassiﬁcation and slot-ﬁlling tasks, but there are\\nother task formats like span extraction that we do\\nnot investigate here. Second, we focus on off-the-\\nshelf pre-trained models (i.e. RoBERTa) in this\\npaper, though we believe our methods could also\\nbe applied and adopted to other models. Finally,\\nand most critically, the interpretation techniques\\nwe use are all fundamentally approximate; while\\nvisualizing model rationales can be useful in the\\ncontext of clinical decision support systems, our\\nevidence sets are not proof positive that a model’s\\npredictions are reliable. Such systems need to be\\ncarefully deployed to avoid misleading practition-\\ners into trusting them too readily. We view this as\\nthe principal risk of our work.\\nD Dataset statistics\\nWe provide the statistics for both adapted DocRED\\nand brain MRI reports dataset in Table 7. Both\\ndatasets are in English and the DocRED dataset is\\npublicly available at https://github.com/\\nthunlp/DocRED .', metadata={'source': '/tmp/tmp7z053w48', 'page': 11}),\n",
       " Document(page_content='E Annotation Instructions\\nWe recruited four radiology residents to make anno-\\ntations. They did not receive compensation for this\\nproject speciﬁcally. The annotation instructions\\nfor the BrainMRI dataset are provided in Figure 4.\\nThese were developed jointly with the annotators.\\nIn particular, decisions to exclude normal brain ac-\\ntivity and confounders such as SVID were made to\\nincrease interannotator agreement after an initial\\nround of annotation, making it easier for the label-\\ning to focus on a single core disease or diagnosis\\nper report.\\nF Error Analysis\\nThe ﬁrst example in Table 8 shows a representa-\\ntive case where our model predicts the correct re-\\nlation and extracts reasonable supporting evidence.\\nUnsurprisingly, this happens most often in simple\\ncases when reasoning over the interaction of sen-\\ntences is not required.\\nWe observe a few common types of errors. First,\\nwe see potential alternatives for relations or ev-\\nidence extraction . From around 60% of our ran-\\ndomly selected error cases, our model either pre-\\ndicts debatably correct relations or picks sentences\\nthat are related but not perfectly aligned with hu-\\nman annotations. The second row in Table 8 il-\\nlustrates an example where the two entities ex-\\nhibit multiple relationships; the model’s prediction\\nis correct ( Vienna is place where Martinelli was\\nboth born and died), but differs from the annotated\\nground truth and supporting evidence. Such rela-\\ntions are relatively frequent in this dataset; a more\\ncomplex multi-label prediction format is necessary\\nto fully support these.\\nAnother type of error is complex logical reason-\\ning. Even if our model can extract right evidence,\\nit still fails in around 10% of random error cases\\nrequiring sophisticated reasoning. For example,\\nto correctly predict the relation between Theobald\\nTiger and21 December 1935 in the third exam-\\nple in Table 8, a model needs to recognize that\\nTheobald Tiger andKurt Tucholsky are in fact the\\nsame entity by referring to pseudonym , which is a\\nchallenging relation to recognize.\\nFinally, the model sometimes selects more sen-\\ntences than we truly need . Interestingly, this is\\nan error in terms of evidence plausibility but not in\\nterms of prediction . The number of extracted sen-\\ntences is very high in around 25% of the random\\nerror cases. The last row from Table 8 is one ofrepresentative examples with this kind of error. Al-\\nthough our model possibly has already successfully\\nextracted right evidence in the ﬁrst two steps, it con-\\ntinues selecting unnecessary sentences because the\\nprediction conﬁdence is not high enough, a draw-\\nback in our way of selecting evidence mentioned\\nin Section 4.2. Moreover, our model extracts one\\nmore sentence on average when predicting incor-\\nrect relations, suggesting that in these cases it does\\nnot cleanly focus on the correct information.', metadata={'source': '/tmp/tmp7z053w48', 'page': 12}),\n",
       " Document(page_content='Dataset Setting # doc. # inst. # word/inst. # sent./inst. # relation # NA%\\nAdapted DocREDtrain 3053 38180 203 8.1 96+1 33\\nval 1000 12323 203 8.1 96+1 33\\nBrain MRItrain 327 327 177 11.6 \\x00 \\x00\\nval 86 86 132 10.1 \\x00 \\x00\\nTable 7: Statistics of the two document-level IE datasets. Each document may have multiple entity pairs of interest,\\ngiving rise to multiple instances in the adapted DocRED setting. For adapted DocRED, we have 96 relations from\\nthe data plus an NArelation that we introduce for 1/3 of the data.\\nFigure 4: Annotation instructions.', metadata={'source': '/tmp/tmp7z053w48', 'page': 13}),\n",
       " Document(page_content='Type Example\\nPredicts correctly\\nand extracts right\\nevidence[0]Delphine “Delphi” Greenlaw is a ﬁctional character on the New Zealand soap opera Shortland\\nStreet , who was portrayed by Anna Hutchison between 2002 and 2004. ...\\nPredicted relation : country of origin Relation : country of origin\\nExtracted Evidence :[0] Annotated Evidence :[0]\\nPredicts debatably\\ncorrect answer,\\nextracts reasonable\\nevidence[0]Anton Erhard Martinelli (1684 – September 15 , 1747) was an Austrian architect and master\\n- builder of Italian descent. [1]Martinelli was born in Vienna . ... [3]Anton Erhard Martinelli\\nsupervised the construction of several important buildings in Vienna , such as ... [4]Hedesigned ...\\n[6]Hedied in Vienna in 1747.\\nPredicted relation : place of birth Relation : place of death\\nExtracted Evidence :[1] Annotated Evidence :[0, 6]\\nPredict incorrect\\nexample on\\nexamples requiring\\nhigh amount of\\nreasoning[0]Kurt Tucholsky (9 January 1890 – 21 December 1935 ) was a German - Jewish journalist, satirist,\\nand writer. [1]He also wrote under the pseudonyms Kaspar Hauser (after the historical ﬁgure), Peter\\nPanter, Theobald Tiger and Ignaz Wrobel. ...\\nPredicted relation :NA Relation : date of death\\nExtracted Evidence :[0] Annotated Evidence :[0]\\nSelecting more\\nsentences than are\\nneeded[0]Henri de Boulainvilliers ... was a French nobleman, writer and historian. ... [2]Primarily\\nremembered as an early modern historian of the French State ,Boulainvilliers also published an early\\nFrench translation of Spinoza’s Ethics and ... [3]TheComte de Boulainvilliers traced his lineage to\\n...[5]Much of Boulainvilliers ’ historical work ...\\nPredicted relation : country of citizenship Relation : country of citizenship\\nExtracted Evidence :[2, 0, 1, 5, 4, 3] Annotated Evidence :[0, 2]\\nTable 8: Four types of representative examples that show models’ behavior. In our adapted DocRED task, models\\nare asked to predict relations among heads andtails. Here we use model S UFFICIENT bothfor illustrations, which\\nhas the best evidence extraction performance. Sentences in extracted evidence are ranked by DL.\\nModel An Example of mass effect , label: positive , evidence: 0 or 6\\nSUFFICIENT none [0] These images show evidence of downward displacement of the brain stem with collapse of the\\ninterpeduncular cistern and caudal displacement of the mammary bodies typical for intracran ial\\nhypertension .[1] There is diffuse pachymeningeal enhancement evident .[2] B ilateral extra axial col-\\nlections are evident the do not conform to the imaging characteristics ofCSF are seen over lying the\\nhemispheres. [3] These likely reﬂect blood t inged hygromas and there does appear to beablood products\\ninthedeep tendon portion of the right sided collection onthe patient ’sleft seeimage 14series 2.\\n[4]There does appear to be a discrete linear subd ural he matoma along the right tentorial leaf. [5] Subdural\\ncollection is noted on both sides of the falx as well .[6]There is mass effect at the level of the tentorial\\nincisure due to transtentorial her niation with deformity of the mid brain .[7] There is no evidence\\nan acute infarct.[8] No parenchymal hemorrh age is evident .[9]Apart from the meningeal enhancement\\nthere is no abnormal enhancement noted.\\nSUFFICIENT both [0] These images show evidence of downward displacement of thebrain stem with collapse of the\\ninterpeduncular cistern and caudal displacement of the mammary bodies typical for intrac ranial\\nhypertension . [1] There is diffuse pachymeningeal enhancement evident. [2] Bilateral extra axial collections\\nare evident the do not conform to the imaging characteristics of CSF are seen overlying the hemispheres.\\n[3] These likely reﬂect blood tinged hyg romas and there does appear to be ablood products in the deep\\ntendon portion of the right sided collection on the patient’s left seeimage 14 series 2. [4] There does appear\\nto be a discrete linear subdural hematoma along the right tentorial leaf. [5] Subdural collection is noted\\non both sides of the falx as well. [6] There is mass effect at the level of the tentorial inc isure due to\\ntranstentorial her niation with deformity of the midbrain .[7] There is no evidence an acute infarct.\\n[8] No parenchymal hemorrhage is evident. [9] Apart from the meningeal enhancement there is no abnormal\\nenhancement noted.\\nTable 9: An illustration of models’ attribution scores over a report from B RAIN MRI using DeepLift with and w/o\\nregularization techniques. S UFFICIENT bothappears to leverage more information from right sentences.', metadata={'source': '/tmp/tmp7z053w48', 'page': 14})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFLoader('https://arxiv.org/pdf/2110.07686.pdf')\n",
    "pages = loader.load()\n",
    "print(len(pages))\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in pages:\n",
    "    page.page_content = page.page_content.replace('-\\n', '').replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Making Document-Level Information Extraction Right for the Right Reasons Liyan Tang1, Dhruv Rajan1, Suyash Mohan2, Abhijeet Pradhan3, R. Nick Bryan1, Greg Durrett1 1The University of Texas at Austin 2University of Pennsylvania 3Galileo CDS Inc. lytang@utexas.edu, dhruv.rajan@utexas.edu Suyash.Mohan@pennmedicine.upenn.edu, ap@galileocds.com nick.bryan@austin.utexas.edu, gdurrett@cs.utexas.edu Abstract Document-level models for information extraction tasks like slot-ﬁlling are ﬂexible: they can be applied to settings where information is not necessarily localized in a single sentence. For example, key features of a diagnosis in a radiology report may not be explicitly stated in one place, but nevertheless can be inferred from parts of the report’s text. However, these models can easily learn spurious correlations between labels and irrelevant information. This work studies how to ensure that these models make correct inferences from complex text and make those inferences in an auditable way: beyond just being right, are these models “right for the right reasons?” We experiment with post-hoc evidence extraction in a predict-select-verify framework using feature attribution techniques. We show that regularization with small amounts of evidence supervision during training can substantially improve the quality of extracted evidence. We evaluate on two domains: a small-scale labeled dataset of brain MRI reports and a largescale modiﬁed version of DocRED (Yao et al., 2019) and show that models’ plausibility can be improved with no loss in accuracy.1 1 Introduction Document-level information extraction (Yao et al., 2019; Christopoulou et al., 2019; Xiao et al., 2020; Guoshun et al., 2020) has seen great strides due to the rise of pre-trained models (Devlin et al., 2019). But in high-stakes domains like medical information extraction (Irvin et al., 2019; McDermott et al., 2020; Smit et al., 2020), machine learning models are still too error-prone to use broadly. Since they are not perfect, they typically play the role of assisting users in tasks like building cohorts (Pons et al., 2016) or in providing clinical decision support (Demner-Fushman et al., 2009). 1Code available at https://github.com/ Liyan06/DocumentIE . Transformer [0]Severe encephalomalacia in the temporal lobes a nd frontal lobes bilaterally with reactive gliosis in the left frontal lobe. [1] Moderate enlargement of the ventricular system. [2] No abnormal enhancement. [3] Near completeopaciﬁcation of the left maxillary sinus. …evidence\\tsents: \\t0,\\t1 Interpretlabel\\t(mass\\teﬀect):\\t negative Accurate?Plausible? (model evidence agrees w/humanlabeled evidence)Faithful?  (model predicti on on evidence agrees w/full doc)Figure 1: Our basic model setup. A Transformer-based model makes document-level predictions on an example of our brain MRI reports. An interpretation method extracts the evidence sentences used by the model. Our system is evaluated according to the criteria of accuracy, faithfulness, and plausibility. To be most usable in conjunction with users, these systems should not just produce a decision, but a justiﬁcation for their answer. The ideal system therefore obtains high predictive accuracy, but also returns a rationale that allows a human to verify the predicted label (Rudie et al., 2019). Our goal is to study document-level information extraction systems that are both accurate and which make predictions based on the correct information (Doshi-Velez and Kim, 2017). This process involves identifying what evidence the model actually used, verifying the model’s prediction based on that evidence, and checking whether that evidence aligns with what humans would use, which would allow a user to quickly see if the system is correct. For example, in Figure 1, localizing the prediction ofmass effect (a feature expressing whether there is evidence of brain displacement by a mass like aarXiv:2110.07686v2  [cs.CL]  18 May 2022', metadata={'source': '/tmp/tmp7z053w48', 'page': 0}),\n",
       " Document(page_content='tumor) to the ﬁrst two sentences allows a trained user in a clinical decision support setting to easily verify what was extracted here. Our evidence extraction hews to principles of both faithfulness and plausibility (Jain et al., 2020; Jacovi and Goldberg, 2020; Miller, 2019). Rather than use complex approaches with intermediate latent variables for extraction (Lei et al., 2016), we focus on what can be done with offthe-shelf pre-trained models (Liu et al., 2019) using post-hoc interpretation. We explore various interpretation methods to ﬁnd key parts of each document that were used by the model. We ask two questions: ﬁrst, can we identify the document sentences that truly contributed to the prediction (faithfulness)? Using the ranking of sentences provided by an interpretation method, we extract a set of sentences where the model returns nearly the same prediction as before, thus verifying that these sentences are a sufﬁcient explanation for the model. Second, do these document sentences align with what users annotated (plausibility)? Unsurprisingly, we ﬁnd that this alignment is low in a basic Transformer model. To further improve the alignment with human annotation, we consider injecting small amounts of sentence-level supervision. Critically, in the brain MRI extraction setting we consider (see Table 1), large-scale sentence-level annotation is not available; most instances in the dataset only have document-level labels from existing clinical decision support systems, making it a weaklysupervised setting (Pruthi et al., 2020; Patel et al., 2020). We explore two methods for using this small amount of annotation, chieﬂy based around supervising or regularizing the model’s behavior. One notion is entropy maximization: the model should be uncertain when it isn’t exposed to sufﬁcient evidence (Feng et al., 2019). Another is attention regularization where the model is encouraged to attend to key pieces of evidence. While attention is not entirely connected with what the model uses (Jain and Wallace, 2019), we can investigate whether this leads to a model whose explanations leverage this information more heavily. We validate our methods ﬁrst on a small dataset of radiologists’ observations from brain MRIs. These reports are annotated with document-level key features related to different aspects of the report, which we want to extract in a faithful way. We see positive results here even in a small-data con-Report Finding [0]Severe encephalomalacia in the temporal lobes and frontal lobes bilaterally with reactive gliosis in the left frontal lobe. [1]Moderate enlargement of the ventricular system .[2]No abnormal enhancement .[3]Near complete opaciﬁcation of the left maxillary sinus. ... mass_effect : negative evid:[0, 1] implicit side: bilateral evid:[0] explicit t2: increased evid:[0] implicit contrast_enhancement : No evid:[2] explicit Table 1: Example from annotated brain MRI reports. Labels and supporting evidence for 4key features are annotated for this example report presented. “Explicit” means the label of given key feature can be directly inferred by the highlighted terms; “implicit” instead indicates that it requires domain knowledge and potential reasoning skills to label. We want the model to identify implicit features while not leveraging dataset biases or reasoning incorrectly about explicit ones. dition, but to understand how this method would scale with larger amounts of data, we adapt the DocRED relation extraction task (Yao et al., 2019) to be a document-level classiﬁcation task. The question of which sentence in the document describes the relation between the two entities, if there even is one, is still quite challenging, and we show our techniques can lead to improvements in a weaklylabeled setting here as well. Our contributions are (1) We apply evidence extraction methods to document-level classiﬁcation and slot-ﬁlling tasks, emphasizing a new brain MRI dataset that we annotate. (2) We explore using weak sentence-level supervision in two techniques adapted from prior work; (3) We evaluate pre-trained models and evidence extraction through various interpretation methods for plausibility compared to human annotation, while ensuring faithfulness of the evidence. 2 Background 2.1 Motivation We start with an example from a brain MRI report in Table 1. Medical information extraction involves tasks such as identifying important medical terms from text (Irvin et al., 2019; Smit et al., 2020) and normalizing names into standard concepts using domain-speciﬁc ontologies (Cho et al., 2017). One application in clinical decision support, shown here, requires extracting the values of certain key features (clinical ﬁndings) from these reports or medical images (Rudie et al., 2021; Duong et al., 2019).', metadata={'source': '/tmp/tmp7z053w48', 'page': 1}),\n",
       " Document(page_content='This extraction should be accurate, but it should also make predictions that are correctly sourced, to facilitate review by a radiologist or someone else using the system (Rauschecker et al., 2020; Cook et al., 2018). The ﬁnding section of a brain MRI report often describes these key features in both explicit and implicit ways. For instance, contrast enhancement, one of our key features, is mentioned explicitly much of the time; see no abnormal enhancement in the third sentence. A rule-based system can detect this type of evidence easily. But some key features are harder to identify and require reasoning over context and draw on implicit cues. For example, severe encephalomalacia in the ﬁrst sentence and enlargement of the ventricular system in the following sentence are both implicit signs of positive mass effect and either is sufﬁcient to infer the label. It is signiﬁcantly harder to built a rule-based extractor for this case. Learning-based systems have the potential to do much better here, but lack of understanding about their behavior can lead to hard-topredict failure modes, such as acausal prediction of key features (e.g., inferring evidence about mass effect from a hypothesized diagnosis somewhere in the report, where the causality is backwards). Our work aims to leverage the ability of learningbased systems to capture implicit features while improving their ability to make predictions that are sourced from the correct evidence and can be easily veriﬁed. 2.2 Problem Setting The problem we tackle in this work can be viewed as document-level classiﬁcation. Let D= fx1;:::;x ngbe a document consisting of nsentences. The document is annotated with a set of labels (ti;yi)wheretiis an auxiliary input specifying a particular task for this document (e.g., mass effect) and yiis the label associated with that task from a discrete label space f1;:::;dg. In our adaptation of the DocRED task, we consider t= (e1;e2)to classify the relationship (if any) between a pair of entities (e1;e2)in a document, deﬁned in Section 4.1.2. Our method takes a pair (D;t)and then computes the label ^ytfrom a predictor ^yt=f(D;t). We can then extract evidence , a set of sentences, post-hoc using a separate procedure gsuch as a feature attribution method: ^Et=g(f;D;t )Supervision In addition to the labels yt, we assume access to a small number of examples with additional supervision in each domain. That is, for a(D;t;y t)triple, we also assume we are given a setE=fxi1;:::;x imgof ground-truth evidence with sentence indices fi1;:::;i mg. This evidence should be sufﬁcient to compute the label, but not always necessary; for example, if multiple sentences can contribute to the prediction, they might all be listed as supporting evidence here. See Section 3.3 for more details. 2.3 Related Work Our work ﬁts into a broader thread of work on information extraction with partial annotation (Han et al., 2020). Due to the cost of collecting largescale data with good quality, distant supervision (DS) (Mintz et al., 2009) and ways to denoise autolabeled data from DS (Surdeanu et al., 2012; Wang et al., 2018) have been widely explored. However, the sentence-level setting typically features much less ambiguity about evidence needed to predict a relation compared to the document-level setting we explore. Several document-level RE datasets (Li et al., 2016a; Peng et al., 2017) have been proposed as well as efforts to tackle these tasks (Christopoulou et al., 2019; Xiao et al., 2020; Guoshun et al., 2020), which we explicitly build from. Explanation techniques To identify the sentences that the model considers as evidence, we draw on a recent body of work in explainable NLP focused on identifying salient features of the input. These primarily consist of input attribution techniques, such as LIME (Ribeiro et al., 2016), input reductions (Li et al., 2016b; Feng et al., 2018), attention-based explanations (Bahdanau et al., 2015) and gradient-based methods (Simonyan et al., 2014; Selvaraju et al., 2017; Sundararajan et al., 2017; Shrikumar et al., 2017). In present work, we extract rationales using commonly used model interpretation methods (described in Section 3.2) and focus on doing a thorough evaluation of the capabilities of DeepLIFT (Shrikumar et al., 2017) given its competitive performance in our interpretation methods comparison (Appendix B). Frameworks for interpretable pipelines Our goal of building a system grounded in evidence draws heavily on recent work on attribution techniques and model explanations, particularly notions of faithfulness and plausibility. Faithfulness refers', metadata={'source': '/tmp/tmp7z053w48', 'page': 2}),\n",
       " Document(page_content='to how accurately the explanation provided by the model truly reﬂects the information it used in the reasoning process (Jain et al., 2020). On the other hand, plausibility indicates to what extent the interpretation provided by the model makes sense to a person.2 “Select-then-predict ” approaches are one way to enforce faithfulness in pipelines (Jain et al., 2020): important snippets from inputs are extracted and passed through a classiﬁer to make predictions. Past work has used hard (Lei et al., 2016) or soft (Zhang et al., 2016) rationales, and other work has explicitly looked at tradeoffs in the amount of text extracted (Paranjape et al., 2020). Jacovi and Goldberg (2020) note several problems with this setup. Our work aims to align model behavior with what cues we expect a model to use (plausibility), but uses the predict-select-verify paradigm (Jacovi and Goldberg, 2020) to ensure that these are actually sufﬁcient cues for the model. Like our work, Pruthi et al. (2020) simultaneously trained a BERT-based model (Devlin et al., 2019) for the prediction task and a linear-CRF (Lafferty et al., 2001) module on top of it for the evidence extraction task with shared parameters. Compared to their work, we focus explicitly on what can be done with pre-trained models alone, not augmenting the model for evidence extraction. 3 Methods The systems we devise take (D;t)pairs as input and return (a) predicted labels ^ytfor eacht; (b) sets of extracted evidence sentences ^Etfrom an interpretation method. Figure 1 shows the basic setting. 3.1 Transformer Classiﬁcation Model We use RoBERTa (Liu et al., 2019) as our document classiﬁer. RoBERTa is a strong method that holds up even against more recent baselines with architectures designed for DocRED (Zhou et al., 2021). For each of our two domains, we use different pre-trained weights, as described in the training details in Appendix A. The task inputs are described in Section 4.1. 2The ERASER benchmark (DeYoung et al., 2020) is a notable recent effort to evaluate explanation plausibility. However, we do not consider it here; we focus on the documentlevel classiﬁcation setting, and many of the ERASER tasks are not suitable or relevant for the approaches we consider, either being not natural (FEVER) or not having the same challenges as document-level classiﬁcation.3.2 Interpretation for Evidence Extraction Given any interpretation method as well as our model ^yt=f(D;t), we compute attribution scores with respect to the predicted class ytfor each token in the RoBERTa input representation. We then average over the absolute value of attribution score for each token in that sentence to give sentencelevel scoresfs1;:::;s ng. These give us a ranking of the sentences. Given a ﬁxed number of evidence sentenceskto extract, we can extract the top k sentences by these scores. We experiment with the following four widely used interpretation techniques in the present work. LIME (Ribeiro et al., 2016) offers explanations of an input by approximating the model’s predictions locally with an interpretable model. Input Gradient (Hechtlinger, 2016) and Integrated Gradients (Sundararajan et al., 2017) use gradients of the label with respect to the input to assess input importance; Integrated Gradients approximates the integral of this gradient with respect to the input along a straight path from a reference baseline.3 DeepLIFT (Shrikumar et al., 2017) attributes the change in the output from a reference output in terms of the difference in input from the reference input. Unless stated otherwise, we use DeepLIFT as our interpretation method, since it achieves the best results (comparable to Input Gradient) among the four interpretation options. Full comparison of interpretation methods is in Appendix B. To verify the extracted evidence (Jacovi and Goldberg, 2020), our main technique (SUFFICIENT ) feeds the model increasingly larger subsets of the document ranked by attribution scores (e.g., ﬁrst fsmaxg, thenfsmax;s2nd-maxg, etc.) until it (a) makes the same prediction as when taking the whole document as input and (b) assigns that prediction at least \\x15times the probability4when the whole document is taken as input. We consider this attribution faithful: it is a subset of the input supporting the model’s decision judged as important by the attribution method. 3.3 Improving Evidence Extraction While many document-level extraction settings do not have sentence-level attributions labeled for ev3We use the most typical baseline that consists of replacing the inputs in Dwith[MASK] tokens from RoBERTa. 4The value of\\x15is a tolerance hyper-parameter for selecting sentences and it set to 0:8throughout the experiments. Our method is robust to the choice of \\x15in a reasonable range, as shown in Appendix B.', metadata={'source': '/tmp/tmp7z053w48', 'page': 3}),\n",
       " Document(page_content='ery decision, one can in practice annotate a small fraction of a dataset with such ground-truth rationales. This is indeed the case for our brain MRI case study. Past work has shown signiﬁcant beneﬁts from integrating this supervision into learning (Strout et al., 2019; Dua et al., 2020; Pruthi et al., 2021). Assume that a subset of our labeled data consists of(D;t;y t;Et)tuples with ground truth evidence sentence indices Et=fi1;:::;i mg. We consider two modiﬁcations to our model training, namely attention regularization (Pruthi et al., 2021), entropy maximization (Feng et al., 2018), and their combination. An illustration of both methods is shown in Figure 2. Attention regularization Attention regularization encourages our model f(D;t)to leverage more information from Et. Speciﬁcally, let A=f\\x0b1;:::;\\x0b ngbe the attention vector from the[CLS] token in the ﬁnal layer to all tokens inD. During learning, we add the following loss to the training objective: `attn=\\x00logP i2Et\\x0bi, encouraging the model to attend to any token iin the labeled sentence-level evidence set. Entropy maximization When there is no sufﬁcient information contained in the text to infer any predictions, entropy maximization encourages a model to be uncertain, represented by a uniform probability distribution across all classes (DeYoung et al., 2020; Feng et al., 2019). Doing so should encourage the model to notmake predictions based on irrelevant sentences. We can achieve this by taking a reduced document D0=DnEtas input by removing evidence Etfrom original documentD. We treat (D0;t)pairs as extra training examples where we aim to maximize the entropy \\x00P yP(yjD0) logP(yjD0)over all possible y.5 4 Experiments 4.1 Datasets and Evaluation Metrics We investigate our methods on (a) a small collection of brain MRI reports from radiologists’ observations; and (b) a modiﬁed version of the DocRED datatset. The statistics for both datatsets are included in Appendix D. For both datasets, we evaluate on task accuracy (captured by either accuracy or prediction macro-F1) as well as evidence selection 5We found this to work better than enforcing a uniform distribution over attention, which is much harder for the model to achieve. Transformer[CLS] [0] Severe encephalomalacia … [1] Moderate enlargement … [2] No abnormal … [3] Near complete …Final attention layer Label Distribution P(y|D) Entropy maximization: (ENTROPY)\\u2028Attention regularization  (ATTN) Transformer deletes relevant sentence; maximizes prediction entropy Transformerencourages attentions on supporting evidence [CLS] [0] Severe… [1] Moderate … [2] No … [3] Near… [CLS] [0] Severe… [1] Moderate … [2] No … [3] Near… Standard Supervised  Training:  max log P(y|D)Figure 2: An illustration of attention regularization and entropy maximization using the example in Table 1. The model is predicting the label for key feature t2. accuracy (macro-F1) or precision, measuring how well the model’s evidence selection aligns with human annotations. We will use the SUFFICIENT method deﬁned in Section 3.2 to select evidence sentences which guarantee that our predictions on the given evidence subsets will match the model’s predictions on the full document. For the brain MRI report dataset, we evaluate evidence extraction by precision since human annotators typically only need to refer to one sentence to reach the conclusion but our model and baselines may extract more than one sentence. 4.1.1 Brain MRI Reports We present a new dataset of de-identiﬁed radiology reports from brain MRIs. It consists of the “ﬁndings” sections of reports, which present observations about the image, with labels for preselected key features by attending physicians and fellows. Crucially, these features are labeled based on the original radiology image , not the report. The document-level labels are therefore noisy because the radiologists’ labels may disagree with the ﬁndings written in the report. A key feature is an observable variable t, which can take on dtpossible values. We focus on the evaluation of two key features, namely contrast enhancement andmass effect , since they appear in most of manually annotated reports. For our RoBERTa classiﬁcation model, we only feed the document and train separate classiﬁers for each key feature, with no shared parameters between these. Annotation We have a moderate number (327) of reports that have noisy labels from the process', metadata={'source': '/tmp/tmp7z053w48', 'page': 4}),\n",
       " Document(page_content='above. We treat these as our training set. However, all of these labels are document-level. To evaluate models’ performance on more ﬁnegrained evidence labels, we randomly select 86 unlabeled reports (not overlapping with the 327 for training) and asked four radiology residents to (1) assign key feature labels and reach consensus, while (2) highlighting sentences that support their decision making. We use Prodigy6as our annotation interface. See Appendix E for more details about our annotation instructions. Pseudo sentence-level supervision Since we have limited number of annotated reports for evaluation, we need a way to prepare weak sentencelevel supervision ( Et) while training. To achieve this, we use sentences selected by our rule-based system as pseudo evidence to supervise models’ behavior. We use 10% of this as supervision while training for consistency with the DocRED setting. Rule-based system Our rule-based system uses keyword matching to identify instances of mass effect and contrast enhancement in the reports, and negspaCy to detect negations of these key features. Data split For the results in Section 5, we evaluate on reports that contain ground truth ﬁne-grained annotations for either contrast enhancement or mass effect, respectively. There are 64 and 68 out of 86 documents total in each of these categories. We call this the BRAIN MRI set. When we restrict to this set for evaluation, all of the documents we study where the annotators labeled something related to contrast enhancement end up having an explicit mention of it. However, for mass effect , this is not always the case; Table 9 in Appendix shows an example where mass effect is discussed implicitly in the ﬁrst sentence. 4.1.2 Adapted DocRED DocRED (Yao et al., 2019) is a document-level relation extraction (RE) dataset with large scale human annotation of relevant evidence sentences. Unlike sentence-level RE tasks (Qin et al., 2018; Alt et al., 2020), it requires reading multiple sentences and reasoning about complex interactions between entities. We adapt this to a document-level relation classiﬁcation task: a document Dand two entity mentionse1;e2within the document are provided and the task is to predict the relation rbetweene1 ande2. We synthesize these examples from the 6https://prodi.gyModel Names Input Text DIRECT None FULLDOC Full document ENT Sentences containing at least one of the two query entities FIRST 2 First two sentences from a doc. FIRST 3 First three sentences from a doc. BESTPAIR Two sentences yielding highest prediction prob. (incl. variants using regularization) SUFFICIENT Sufﬁcient sentences selected by interpretation methods (incl. variants using regularization) Table 2: Model names used in the experiments and their associated evidence given as inputs. original dataset and sample random entity pairs from documents to which we assign an NAclass to construct negative pairs exhibiting no relation. The model input is represented as: [CLS]<ent-1>[SEP]<ent-2>[SEP]<doc>[SEP] . We use the encoding of [CLS] in the last layer to make predictions. To make the setting more realistic, we do not use the large-scale evidence annotation and assume there is limited sentence-level supervision available. To be speciﬁc, we include 10% ﬁne-grained annotations in our adapted DocRED dataset. 4.2 Models Due to richer and higher-quality supervisions in the DocRED setting, we conduct a larger set of ablations and comparisons there. We compare against a subset of these models in the radiology setting. Baselines We consider a number of baselines for adapted DocRED which return both predicted labels and evidence. (1) DIRECT predicts the relation directly from the entity pairs without any sentences as input, using a model trained with just these inputs. (2) FULLDOCtakes the full document as selected evidence and uses the base RoBERTa model (3)ENTtakes all sentences with entity mentions e1 ande2as input; (4) FIRST 2, F IRST 3retrieve the ﬁrst2and3sentences from a document, respectively; and (5) BESTPAIRchooses the best sentence pair by ﬁrst taking each individual sentence as input to the model and then picking top two sentences having highest probabilities on their predictions. SUFFICIENT is our main method for both datasets, which we then augment with additional supervision as described in Section 3.3. We use subscripts attn, entropy, both andnone to represent attention regularization, entropy max-', metadata={'source': '/tmp/tmp7z053w48', 'page': 5}),\n",
       " Document(page_content='imization, the combination of two, and neither. Both BESTPAIRandSUFFICIENT methods leverage backbone RoBERTa models trained with loss functions mentioned above, differing only in their evidence selection. Table 2 summarizes the abbreviated names of models and their inputs. Training details are described in Appendix A. Metrics We report both the accuracy and F 1for the model ( Full Doc ) as well as evaluation of Evidence selection compared to human judgments, either precision or F 1. We also report results in theReduced Doc setting, where only the selected evidence sentences are fed to the RoBERTa model (trained over whole documents) as input. For our SUFFICIENT method, this accuracy is the same as the full method by construction, but note that it can differ for other methods. This reduced setting serves as a sanity check for the faithfulness of our explanation techniques. Note once again that accuracy in the Full Doc case can differ for our methods that are trained with different regularization schemes, as these yield different models that return different predicted labels in addition to different evidence. 5 Results 5.1 Results on Brain MRI Table 3 shows the performance of our models and baselines in terms of label prediction and evidence extraction. For each result, we perform a paired bootstrap test comparing to SUFFICIENT none. We underline results that are better at a signiﬁcance level ofp= 0:05on the corresponding metrics. In themass effect setting, our SUFFICIENT bothmodel achieves the highest evidence extraction precision of the learning-based models, exceeds FULLDOC, FIRST 2/3, and BESTPAIRon the metric by a large margin, and nearly matches that of the rule-based system. It is difﬁcult to be more reliable than a rule-based system, which will nearly always make correctly-sourced predictions. But this model is able to combine that reliability with the higher F1of a learned model . Note that due to the high base rates of certain ﬁndings, we focus on F 1instead of accuracy. We see a similar pattern on contrast enhancement as well, although the evidence precision is lower in that case. These results show that learning-based systems make accurate predictions in this domain, and thatModelLabel Evidence Full Doc Reduced Doc Acc F1 Acc F1 Pre Len Mass Effect FULLDOC 66.6 42.1 66.6 42.1 16.5 10.1 FIRST 2 \\x00 \\x00 82.4 45.2 21.3 2.00 FIRST 3 \\x00 \\x00 82.4 45.2 24.0 3.00 RULE 77.9 11.8 77.9 11.8 84.8 1.46 BESTPAIR none 66.6 42.1 82.4 52.2 24.3 2.00 BESTPAIR both 76.7 60.0 79.4 44.3 50.7 2.00 SUFFICIENT none 66.6 42.1 Identical to Full Doc16.5 2.84 SUFFICIENT attn 69.2 47.6 65.6 2.31 SUFFICIENT entropy 45.3 0.0 15.8 2.50 SUFFICIENT both 76.7 60.0 77.8 1.51 Contrast Enhancement FULLDOC 69.5 60.9 69.5 60.9 13.5 10.1 FIRST 2 \\x00 \\x00 67.2 55.3 14.1 2.00 FIRST 3 \\x00 \\x00 70.3 62.4 14.6 3.00 RULE 68.8 56.5 68.8 56.5 87.1 1.67 BESTPAIR none 69.5 60.9 73.4 67.7 10.9 2.00 BESTPAIR both 90.8 87.2 89.1 88.4 54.7 2.00 SUFFICIENT none 69.5 60.9 Identical to Full Doc33.5 2.84 SUFFICIENT attn 85.8 81.0 60.7 2.48 SUFFICIENT entropy 71.5 59.5 25.2 2.55 SUFFICIENT both 90.8 87.2 71.7 1.50 Table 3: Model performance on B RAIN MRI. Models are evaluated under two settings by taking (a) full document (Full Doc); (b) selected evidence (Reduced Doc) as inputs. R ULE is the baseline mentioned in Section 4.1.1. Prestands for the precision of evidence selection, andLenis the average number of sentences extracted. Underlined results are better than S UFFICIENT noneon the corresponding metric according to a paired bootstrap test with p= 0:05. ModelLabel Evidence Full Doc Reduced Doc Acc F1 Acc F1 F1 Len DIRECT \\x00 \\x00 66.4 45.3 \\x00 \\x00 FULLDOC 83.0 66.0 83.0 66.0 34.9 8.03 FIRST 2 \\x00 \\x00 75.3 58.1 47.9 2.00 FIRST 3 \\x00 \\x00 77.5 60.7 44.6 3.00 ENT \\x00 \\x00 82.4 65.4 61.5 3.93 BESTPAIR none 83.0 66.0 73.9 55.3 39.2 2.00 BESTPAIR attn 83.2 65.0 73.4 53.5 43.9 2.00 BESTPAIR entropy 81.8 64.2 78.5 58.2 52.3 2.00 BESTPAIR both 82.7 66.5 81.6 65.3 66.2 2.00 SUFFICIENT none 83.0 66.0 Identical to Full Doc67.2 1.42 SUFFICIENT attn 83.2 65.0 70.3 1.45 SUFFICIENT entropy 81.8 64.2 69.9 1.65 SUFFICIENT both 82.7 66.5 73.1 1.65 human \\x00 \\x00 \\x00 \\x00 \\x00 1.59 Table 4: Model performance on adapted DocRED. Models are evaluated under two settings as in BRAIN MRI. Underlined results are better than S UFFICIENT noneon the corresponding metric according to a paired bootstrap test with p= 0:05.', metadata={'source': '/tmp/tmp7z053w48', 'page': 6}),\n",
       " Document(page_content='ModelMass Effect Ctr. Enhance. Mean Max Mean Max SUFFICIENT none 7.3 7.4 28.6 29.8 SUFFICIENT both 18.9 19.2 37.9 42.0 Table 5: Distributions of attribution mass over explicit cues (“enhancement” for contrast enhancement and “effect” for mass effect ) for our best model and the baseline. Mean/Max is the mean of instance-wise average/maximum of the normalized attribution mass falling on the given token. their evidence extraction can be improved with better training, even in spite of the small size of the training set. In section 5.2, we focus on the adapted DocRED setting, which allows us to examine our model’s performance in a higher-data regime. Attribution scores are more peaked at the occurrence of key terms. We conduct analysis on how the attribution scores from SUFFICIENT both are peaked around the correct evidence compare to that from SUFFICIENT noneusing our manually annotated set BRAIN MRI . We compute the mean of the instance-wise average and maximum of the normalized attribution mass falling into a few explicit tokens: enhancement for contrast enhancement and effect for mass effect, which are common explicit indicators in the context of speciﬁed key features. The results in Table 5 show attribution scores being peaked around the correct terms, highlighting that these models can be guided to not only make correct predictions but attend to the right information. Table 9 in the Appendix shows visualizations of attribution scores for an example in BRAIN MRI using DeepLIFT. Even though baseline models make correct predictions, their attribution mass is diffused over the document. With the help of regularization, our model is capable of capturing implicit cues such as downward displacement of the brain stem, although it is trained on an extremely small training set with only explicit cues like mass effect in a weak sentence-level supervision framework. 5.2 Results on Adapted DocRED Comparison to baselines Table 4 shows that the ENTbaseline is quite strong at DocRED evidence extraction. However, our best method still exceeds this method on both label accuracy as well as evidence extraction while extracting more succinct explanations. We see that the ability to extract a variable-length explanation is key, with FIRST 2, FIRST 3andBESTPAIR performing poorly. Notably, these methods exhibit a drop in accuracy inthe reduced doc setting for each method compared to the full doc setting, showing that the explanations extracted are not faithful. Learning-based models with appropriate regularization perform relatively better in this larger-data setting From Table 3 and Table 4, we can observe that various regularization techniques applied to SUFFICIENT models maintain or improve overall model performance on both key feature and relation classiﬁcation. We see that our SUFFICIENT methods do not compromise on accuracy but make predictions based on plausible evidence sets, which is more evident when we have richer training data. We perform further error analysis in Appendix F. Faithfulness of techniques One may be concerned that, like attention values (Jain and Wallace, 2019), our feature attribution methods may not faithfully reﬂect the computation of the model. We emphasize again that the SUFFICIENT paradigm on top of the DeepLIFT method isfaithful by our deﬁnition. For a model f, we measure the faithfulness by checking the agreement between ^y=f(D;t) andy0=f(^Et;t), where ^Etis the extracted evidence we feed into the same model under the reduced document setting. This is shown for all methods in the “Reduced doc” columns in Tables 3 and 4. We see a drop in performance from techniques such as BESTPAIR: the full model does not make the same judgment on these evidence subsets, but by deﬁnition it does in the S UFFICIENT setting. As further evidence of faithfulness, we note that only a relatively small number of evidence sentences, in line with human annotations, are extracted in the SUFFICIENT method. These small subsets are indicated by feature attribution methodsandsufﬁcient to reproduce the original model predictions with high conﬁdence, suggesting that these explanations are faithful. 6 Conclusion In this work, we develop techniques to employ small amount of data to improve reliability of document-level IE systems in two domains. We systematically evaluate our model from perspectives of faithfulness and plausibility and show that we can substantially improve models’ capability in focusing on supporting evidence while maintaining their predictive performance, leading to models that are “right for the right reasons.”', metadata={'source': '/tmp/tmp7z053w48', 'page': 7}),\n",
       " Document(page_content='Acknowledgments This work was partially supported by NSF Grant IIS-1814522 and a Texas Health Catalyst grant. Thanks to Scott Rudkin, Gregory Mittl, Raghav Mattay, and Chuan Liang for assistance with the annotation. References Christoph Alt, Aleksandra Gabryszak, and Leonhard Hennig. 2020. Probing linguistic features of sentence-level representations in neural relation extraction. In Proceedings of ACL . Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In Proceedings of the International Conference on Learning Representations (ICLR) . Hyejin Cho, Wonjun Choi, and Hyunju Lee. 2017. A method for named entity normalization in biomedical articles: application to diseases and plants. BMC Bioinformatics , 18(1). Fenia Christopoulou, Makoto Miwa, and Sophia Ananiadou. 2019. Connecting the dots: Document-level neural relation extraction with edge-oriented graphs. InProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) . Association for Computational Linguistics. Tessa Cook, James C. Gee, R. Nick Bryan, Jeffrey T. Duda, Po-Hao Chen, Emmanuel Botzolakis, Suyash Mohan, Andreas Rauschecker, Jeffrey Rudie, and Ilya Nasrallah. 2018. Bayesian network interface for assisting radiology interpretation and education. InMedical Imaging 2018: Imaging Informatics for Healthcare, Research, and Applications . SPIE. Dina Demner-Fushman, Wendy W. Chapman, and Clement J. McDonald. 2009. What can natural language processing do for clinical decision support? Journal of Biomedical Informatics , 42(5):760–772. Biomedical Natural Language Processing. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics. Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and Byron C. Wallace. 2020. ERASER: A benchmark to evaluate rationalized NLP models. In Proceedings of the 58th Annual Meeting of the Associationfor Computational Linguistics . Association for Computational Linguistics. Finale Doshi-Velez and Been Kim. 2017. Towards a rigorous science of interpretable machine learning. arXiv: Machine Learning . Dheeru Dua, Sameer Singh, and Matt Gardner. 2020. Beneﬁts of intermediate annotations in reading comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 5627–5634, Online. Association for Computational Linguistics. M.T. Duong, J.D. Rudie, J. Wang, L. Xie, S. Mohan, J.C. Gee, and A.M. Rauschecker. 2019. Convolutional neural network for automated FLAIR lesion segmentation on clinical brain MR imaging. American Journal of Neuroradiology , 40(8):1282–1290. Shi Feng, Eric Wallace, and Jordan Boyd-Graber. 2019. Misleading failures of partial-input baselines. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics. Shi Feng, Eric Wallace, Alvin Grissom II, Mohit Iyyer, Pedro Rodriguez, and Jordan Boyd-Graber. 2018. Pathologies of neural models make interpretations difﬁcult. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics. Nan Guoshun, Guo Zhijiang, Sekuli ´c Ivan, and Lu Wei. 2020. Reasoning with latent structure reﬁnement for document-level relation extraction. In Proceedings of ACL . Suchin Gururangan, Ana Marasovi ´c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Don’t stop pretraining: Adapt language models to domains and tasks. In Proceedings of ACL . Xu Han, Tianyu Gao, Yankai Lin, Hao Peng, Yaoliang Yang, Chaojun Xiao, Zhiyuan Liu, Peng Li, Jie Zhou, and Maosong Sun. 2020. More data, more relations, more context and more openness: A review and outlook for relation extraction. In Proceedings of the 1st Conference of the Asia-Paciﬁc Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing , pages 745–758, Suzhou, China. Association for Computational Linguistics. Yotam Hechtlinger. 2016. Interpretation of prediction models using the input gradient. In Proceedings of the NeurIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems . Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, Jayne Seekins, David Mong, Safwan Halabi, Jesse Sandberg, Ricky Jones, David Larson, Curtis', metadata={'source': '/tmp/tmp7z053w48', 'page': 8}),\n",
       " Document(page_content='Langlotz, Bhavik Patel, Matthew Lungren, and Andrew Ng. 2019. Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison. Proceedings of the AAAI Conference on Artiﬁcial Intelligence , 33:590–597. Alon Jacovi and Yoav Goldberg. 2020. Towards faithfully interpretable NLP systems: How should we deﬁne and evaluate faithfulness? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics. Sarthak Jain and Byron C. Wallace. 2019. Attention is not Explanation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 3543–3556, Minneapolis, Minnesota. Association for Computational Linguistics. Sarthak Jain, Sarah Wiegreffe, Yuval Pinter, and Byron C. Wallace. 2020. Learning to faithfully rationalize by construction. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics. John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning , ICML ’01, page 282–289, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc. Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2016. Rationalizing neural predictions. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics. Jiao Li, Yueping Sun, Robin J. Johnson, Daniela Sciaky, Chih-Hsuan Wei, Robert Leaman, Allan Peter Davis, Carolyn J. Mattingly, Thomas C. Wiegers, and Zhiyong Lu. 2016a. BioCreative v CDR task corpus: a resource for chemical disease relation extraction. Database , 2016:baw068. Jiwei Li, Will Monroe, and Dan Jurafsky. 2016b. Understanding neural networks through representation erasure. ArXiv , abs/1612.08220. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. In arXiv . Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In International Conference on Learning Representations . Matthew B. A. McDermott, Tzu-Ming Harry Hsu, Wei-Hung Weng, Marzyeh Ghassemi, and Peter Szolovits. 2020. Chexpert++: Approximating thechexpert labeler for speed, differentiability, and probabilistic output. CoRR , abs/2006.15229. Tim Miller. 2019. Explanation in artiﬁcial intelligence: Insights from the social sciences. Artiﬁcial Intelligence , 267:1–38. Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP , pages 1003–1011, Suntec, Singapore. Association for Computational Linguistics. Bhargavi Paranjape, Mandar Joshi, John Thickstun, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2020. An information bottleneck approach for controlling conciseness in rationale extraction. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Association for Computational Linguistics. Dhruvesh Patel, Sandeep Konam, and Sai P. Selvaraj. 2020. Weakly supervised medication regimen extraction from medical conversations. In ClinicalNLP@EMNLP . Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina Toutanova, and Wen tau Yih. 2017. Cross-sentence n-ary relation extraction with graph LSTMs. Transactions of the Association for Computational Linguistics , 5:101–115. Ewoud Pons, Loes M. M. Braun, M. G. Myriam Hunink, and Jan A. Kors. 2016. Natural language processing in radiology: A systematic review. Radiology , 279(2):329–343. PMID: 27089187. Danish Pruthi, Bhuwan Dhingra, Graham Neubig, and Zachary C. Lipton. 2020. Weakly- and semisupervised evidence extraction. In Findings of the Association for Computational Linguistics: EMNLP 2020 . Association for Computational Linguistics. Danish Pruthi, Bhuwan Dhingra, Livio Baldini Soares, M. Collins, Zachary C. Lipton, Graham Neubig, and William W. Cohen. 2021. Evaluating explanations: How much do explanations from the teacher aid students? Transactions of the Association for Computational Linguistics . Pengda Qin, Weiran Xu, and William Yang Wang. 2018. Robust distant supervision relation extraction via deep reinforcement learning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics. Andreas M. Rauschecker, Jeffrey D. Rudie, Long Xie, Jiancong Wang, Michael Tran Duong, Emmanuel J. Botzolakis, Asha M. Kovalovich, John Egan, Tessa C. Cook, R. Nick Bryan, Ilya M. Nasrallah, Suyash Mohan, and James C. Gee. 2020. Artiﬁcial intelligence system approaching', metadata={'source': '/tmp/tmp7z053w48', 'page': 9}),\n",
       " Document(page_content='neuroradiologist-level differential diagnosis accuracy at brain MRI. Radiology , 295(3):626–637. Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \"why should i trust you?\": Explaining the predictions of any classiﬁer. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD ’16, page 1135–1144, New York, NY , USA. Association for Computing Machinery. Jeffrey Rudie, Long Xie, Jiancong Wang, Jeffrey Duda, Joshua Choi, Raghav Mattay, Po-Hao Chen, R Nick Bryan, Emmanuel Botzolakis, Ilya Nasrallah, Tessa Cook, Suyash Mohan, James Gee, and Andreas Rauschecker. 2019. Artiﬁcial Intelligence System for Automated Brain MR Diagnosis Performs at Level of Academic Neuroradiologists and Augments Resident Performance. In Proceedings of the Society for Imaging Informatics in Medicine (SIIM) . Jeffrey D. Rudie, Jeffrey Duda, Michael Tran Duong, Po-Hao Chen, Long Xie, Robert Kurtz, Jeffrey B. Ware, Joshua Choi, Raghav R. Mattay, Emmanuel J. Botzolakis, James C. Gee, R. Nick Bryan, Tessa S. Cook, Suyash Mohan, Ilya M. Nasrallah, and Andreas M. Rauschecker. 2021. Brain MRI deep learning and bayesian inference system augments radiology resident performance. Journal of Digital Imaging, 34(4):1049–1058. Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. 2017. Grad-CAM: Visual explanations from deep networks via gradient-based localization. In 2017 IEEE International Conference on Computer Vision (ICCV) . IEEE. Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. 2017. Learning important features through propagating activation differences. In Proceedings of the 34th International Conference on Machine Learning - Volume 70 , ICML’17, page 3145–3153. JMLR.org. Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2014. Deep inside convolutional networks: Visualising image classiﬁcation models and saliency maps. In Workshop at International Conference on Learning Representations . Akshay Smit, Saahil Jain, Pranav Rajpurkar, Anuj Pareek, Andrew Ng, and Matthew Lungren. 2020. Combining automatic labelers and expert annotations for accurate radiology report labeling using BERT. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 1500–1519, Online. Association for Computational Linguistics. Julia Strout, Ye Zhang, and Raymond Mooney. 2019. Do human rationales improve machine explanations? In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , pages 56–62, Florence, Italy. Association for Computational Linguistics.Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution for deep networks. In Proceedings of the 34th International Conference on Machine Learning , volume 70 of Proceedings of Machine Learning Research , pages 3319–3328. PMLR. Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and Christopher D. Manning. 2012. Multi-instance multi-label learning for relation extraction. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning , pages 455– 465, Jeju Island, Korea. Association for Computational Linguistics. Xiaozhi Wang, Xu Han, Yankai Lin, Zhiyuan Liu, and Maosong Sun. 2018. Adversarial multi-lingual neural relation extraction. In Proceedings of the 27th International Conference on Computational Linguistics, pages 1156–1166, Santa Fe, New Mexico, USA. Association for Computational Linguistics. Chaojun Xiao, Yuan Yao, Ruobing Xie, Xu Han, Zhiyuan Liu, Maosong Sun, Fen Lin, and Leyu Lin. 2020. Denoising relation extraction from documentlevel distant supervision. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Association for Computational Linguistics. Yuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin, Zhenghao Liu, Zhiyuan Liu, Lixin Huang, Jie Zhou, and Maosong Sun. 2019. DocRED: A large-scale document-level relation extraction dataset. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics. Ye Zhang, Iain Marshall, and Byron C. Wallace. 2016. Rationale-augmented convolutional neural networks for text classiﬁcation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics. Wenxuan Zhou, Kevin Huang, Tengyu Ma, and Jing Huang. 2021. Document-level relation extraction with adaptive thresholding and localized context pooling. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence .', metadata={'source': '/tmp/tmp7z053w48', 'page': 10}),\n",
       " Document(page_content='A Implementation Details We train all RoBERTa models for 15 epochs with early stopping using 1TITAN-Xp GPU. We use AdamW (Loshchilov and Hutter, 2019) as our optimizer and initialize the model with robertabase for DocRED and biomed-robertabase (Gururangan et al., 2020) for brain MRI data, both with 125M parameters. The batch size is set to 16 for RoBERTa models trained with both attention regularization and entropy maximization and 8 for models with other loss functions, and the learning rate is 1e-5 with linear schedule warmup. The maximum number of tokens in each document is capped at 296 for modiﬁed DocRED and 360 for radiology reports. These numbers are chosen such that the number of tokens for around 95% of the documents is within these limits. Remaining tokens are clipped from the input. The hidden state of the [CLS] token from the ﬁnal layer is fed as input to a linear projection head to make predictions. The average training time for each model is around 4 GPU hours. B Interpretation Methods Comparison We evaluate four interpretation methods on SUFFICIENT none and SUFFICIENT bothusing adapted DocRED. These methods are widely used in the literature, namely Integrated Gradients, LIME, DeepLIFT, and Input Gradient, as discussed in Section 3.2. We compare their evidence extraction capabilities by selecting a wide range of \\x15, which controls the number of sentences to be selected. Results are shown in Figure 3. The four techniques generally perform similarly, with DeepLIFT and Input Gradient performing slightly better. For each interpretation method, the result of SUFFICIENT bothis signiﬁcantly better than that of SUFFICIENT none. Similar values of \\x15between 0.8 and 0.9 (preferring to select more sentences) work well across all methods. Table 6 shows the comparison over the threshold ( \\x15= 0:8) we choose for our experiments in Section 5. In general, our method is robust to model interpretation techniques and evidence selection threshold \\x15. Sentence ranking step mentioned in Section 3.2 requires 0.3 GPU hour for Input Gradient and DeepLIFT, 2.5 GPU hours for Integrated Gradients, and 14 GPU hours for LIME. We choose 30 steps to approximate the integral for Integrated Gradients and 100 samples for each input to train the surrogate interpretable model (a linear model in Figure 3: Evidence F1 on adapted DocRED under four model interpretation methods for S UFFICIENT bothover a wide range of \\x15. our case) for LIME. SUFFICIENT none SUFFICIENT both LIME 54.0 70.8 Integrated Gradients 60.6 70.3 DeepLIFT 67.3 73.1 Input Gradient 68.3 73.5 Table 6: Evidence F1 on adapted DocRED under four model interpretation methods for S UFFICIENT noneand SUFFICIENT bothwhen\\x15= 0:8. C Limitations and Risks There are a few limitations of our work. First, we currently test our methods on document-level classiﬁcation and slot-ﬁlling tasks, but there are other task formats like span extraction that we do not investigate here. Second, we focus on off-theshelf pre-trained models (i.e. RoBERTa) in this paper, though we believe our methods could also be applied and adopted to other models. Finally, and most critically, the interpretation techniques we use are all fundamentally approximate; while visualizing model rationales can be useful in the context of clinical decision support systems, our evidence sets are not proof positive that a model’s predictions are reliable. Such systems need to be carefully deployed to avoid misleading practitioners into trusting them too readily. We view this as the principal risk of our work. D Dataset statistics We provide the statistics for both adapted DocRED and brain MRI reports dataset in Table 7. Both datasets are in English and the DocRED dataset is publicly available at https://github.com/ thunlp/DocRED .', metadata={'source': '/tmp/tmp7z053w48', 'page': 11}),\n",
       " Document(page_content='E Annotation Instructions We recruited four radiology residents to make annotations. They did not receive compensation for this project speciﬁcally. The annotation instructions for the BrainMRI dataset are provided in Figure 4. These were developed jointly with the annotators. In particular, decisions to exclude normal brain activity and confounders such as SVID were made to increase interannotator agreement after an initial round of annotation, making it easier for the labeling to focus on a single core disease or diagnosis per report. F Error Analysis The ﬁrst example in Table 8 shows a representative case where our model predicts the correct relation and extracts reasonable supporting evidence. Unsurprisingly, this happens most often in simple cases when reasoning over the interaction of sentences is not required. We observe a few common types of errors. First, we see potential alternatives for relations or evidence extraction . From around 60% of our randomly selected error cases, our model either predicts debatably correct relations or picks sentences that are related but not perfectly aligned with human annotations. The second row in Table 8 illustrates an example where the two entities exhibit multiple relationships; the model’s prediction is correct ( Vienna is place where Martinelli was both born and died), but differs from the annotated ground truth and supporting evidence. Such relations are relatively frequent in this dataset; a more complex multi-label prediction format is necessary to fully support these. Another type of error is complex logical reasoning. Even if our model can extract right evidence, it still fails in around 10% of random error cases requiring sophisticated reasoning. For example, to correctly predict the relation between Theobald Tiger and21 December 1935 in the third example in Table 8, a model needs to recognize that Theobald Tiger andKurt Tucholsky are in fact the same entity by referring to pseudonym , which is a challenging relation to recognize. Finally, the model sometimes selects more sentences than we truly need . Interestingly, this is an error in terms of evidence plausibility but not in terms of prediction . The number of extracted sentences is very high in around 25% of the random error cases. The last row from Table 8 is one ofrepresentative examples with this kind of error. Although our model possibly has already successfully extracted right evidence in the ﬁrst two steps, it continues selecting unnecessary sentences because the prediction conﬁdence is not high enough, a drawback in our way of selecting evidence mentioned in Section 4.2. Moreover, our model extracts one more sentence on average when predicting incorrect relations, suggesting that in these cases it does not cleanly focus on the correct information.', metadata={'source': '/tmp/tmp7z053w48', 'page': 12}),\n",
       " Document(page_content='Dataset Setting # doc. # inst. # word/inst. # sent./inst. # relation # NA% Adapted DocREDtrain 3053 38180 203 8.1 96+1 33 val 1000 12323 203 8.1 96+1 33 Brain MRItrain 327 327 177 11.6 \\x00 \\x00 val 86 86 132 10.1 \\x00 \\x00 Table 7: Statistics of the two document-level IE datasets. Each document may have multiple entity pairs of interest, giving rise to multiple instances in the adapted DocRED setting. For adapted DocRED, we have 96 relations from the data plus an NArelation that we introduce for 1/3 of the data. Figure 4: Annotation instructions.', metadata={'source': '/tmp/tmp7z053w48', 'page': 13}),\n",
       " Document(page_content='Type Example Predicts correctly and extracts right evidence[0]Delphine “Delphi” Greenlaw is a ﬁctional character on the New Zealand soap opera Shortland Street , who was portrayed by Anna Hutchison between 2002 and 2004. ... Predicted relation : country of origin Relation : country of origin Extracted Evidence :[0] Annotated Evidence :[0] Predicts debatably correct answer, extracts reasonable evidence[0]Anton Erhard Martinelli (1684 – September 15 , 1747) was an Austrian architect and master - builder of Italian descent. [1]Martinelli was born in Vienna . ... [3]Anton Erhard Martinelli supervised the construction of several important buildings in Vienna , such as ... [4]Hedesigned ... [6]Hedied in Vienna in 1747. Predicted relation : place of birth Relation : place of death Extracted Evidence :[1] Annotated Evidence :[0, 6] Predict incorrect example on examples requiring high amount of reasoning[0]Kurt Tucholsky (9 January 1890 – 21 December 1935 ) was a German - Jewish journalist, satirist, and writer. [1]He also wrote under the pseudonyms Kaspar Hauser (after the historical ﬁgure), Peter Panter, Theobald Tiger and Ignaz Wrobel. ... Predicted relation :NA Relation : date of death Extracted Evidence :[0] Annotated Evidence :[0] Selecting more sentences than are needed[0]Henri de Boulainvilliers ... was a French nobleman, writer and historian. ... [2]Primarily remembered as an early modern historian of the French State ,Boulainvilliers also published an early French translation of Spinoza’s Ethics and ... [3]TheComte de Boulainvilliers traced his lineage to ...[5]Much of Boulainvilliers ’ historical work ... Predicted relation : country of citizenship Relation : country of citizenship Extracted Evidence :[2, 0, 1, 5, 4, 3] Annotated Evidence :[0, 2] Table 8: Four types of representative examples that show models’ behavior. In our adapted DocRED task, models are asked to predict relations among heads andtails. Here we use model S UFFICIENT bothfor illustrations, which has the best evidence extraction performance. Sentences in extracted evidence are ranked by DL. Model An Example of mass effect , label: positive , evidence: 0 or 6 SUFFICIENT none [0] These images show evidence of downward displacement of the brain stem with collapse of the interpeduncular cistern and caudal displacement of the mammary bodies typical for intracran ial hypertension .[1] There is diffuse pachymeningeal enhancement evident .[2] B ilateral extra axial collections are evident the do not conform to the imaging characteristics ofCSF are seen over lying the hemispheres. [3] These likely reﬂect blood t inged hygromas and there does appear to beablood products inthedeep tendon portion of the right sided collection onthe patient ’sleft seeimage 14series 2. [4]There does appear to be a discrete linear subd ural he matoma along the right tentorial leaf. [5] Subdural collection is noted on both sides of the falx as well .[6]There is mass effect at the level of the tentorial incisure due to transtentorial her niation with deformity of the mid brain .[7] There is no evidence an acute infarct.[8] No parenchymal hemorrh age is evident .[9]Apart from the meningeal enhancement there is no abnormal enhancement noted. SUFFICIENT both [0] These images show evidence of downward displacement of thebrain stem with collapse of the interpeduncular cistern and caudal displacement of the mammary bodies typical for intrac ranial hypertension . [1] There is diffuse pachymeningeal enhancement evident. [2] Bilateral extra axial collections are evident the do not conform to the imaging characteristics of CSF are seen overlying the hemispheres. [3] These likely reﬂect blood tinged hyg romas and there does appear to be ablood products in the deep tendon portion of the right sided collection on the patient’s left seeimage 14 series 2. [4] There does appear to be a discrete linear subdural hematoma along the right tentorial leaf. [5] Subdural collection is noted on both sides of the falx as well. [6] There is mass effect at the level of the tentorial inc isure due to transtentorial her niation with deformity of the midbrain .[7] There is no evidence an acute infarct. [8] No parenchymal hemorrhage is evident. [9] Apart from the meningeal enhancement there is no abnormal enhancement noted. Table 9: An illustration of models’ attribution scores over a report from B RAIN MRI using DeepLift with and w/o regularization techniques. S UFFICIENT bothappears to leverage more information from right sentences.', metadata={'source': '/tmp/tmp7z053w48', 'page': 14})]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making Document-Level Information Extraction\n",
      "Right for the Right Reasons\n",
      "Liyan Tang1, Dhruv Rajan1, Suyash Mohan2, Abhijeet Pradhan3, R. Nick Bryan1, Greg Durrett1\n",
      "1The University of Texas at Austin\n",
      "2University of Pennsylvania\n",
      "3Galileo CDS Inc.\n",
      "lytang@utexas.edu, dhruv.rajan@utexas.edu\n",
      "Suyash.Mohan@pennmedicine.upenn.edu, ap@galileocds.com\n",
      "nick.bryan@austin.utexas.edu, gdurrett@cs.utexas.edu\n",
      "Abstract\n",
      "Document-level models for information ex-\n",
      "traction tasks like slot-ﬁlling are ﬂexible: they\n",
      "can be applied to settings where information\n",
      "is not necessarily localized in a single sen-\n",
      "tence. For example, key features of a diag-\n",
      "nosis in a radiology report may not be ex-\n",
      "plicitly stated in one place, but nevertheless\n",
      "can be inferred from parts of the report’s text.\n",
      "However, these models can easily learn spuri-\n",
      "ous correlations between labels and irrelevant\n",
      "information. This work studies how to en-\n",
      "sure that these models make correct inferences\n",
      "from complex text and make those inferences\n",
      "in an auditable way: beyond just being right,\n",
      "are these models “right for the right reasons?”\n",
      "We experiment with post-hoc evidence extrac-\n",
      "tion in a predict-select-verify framework using\n",
      "feature attribution techniques. We show that\n",
      "regularization with small amounts of evidence\n",
      "supervision during training can substantially\n",
      "improve the quality of extracted evidence. We\n",
      "evaluate on two domains: a small-scale la-\n",
      "beled dataset of brain MRI reports and a large-\n",
      "scale modiﬁed version of DocRED (Yao et al.,\n",
      "2019) and show that models’ plausibility can\n",
      "be improved with no loss in accuracy.1\n",
      "1 Introduction\n",
      "Document-level information extraction (Yao et al.,\n",
      "2019; Christopoulou et al., 2019; Xiao et al., 2020;\n",
      "Guoshun et al., 2020) has seen great strides due to\n",
      "the rise of pre-trained models (Devlin et al., 2019).\n",
      "But in high-stakes domains like medical informa-\n",
      "tion extraction (Irvin et al., 2019; McDermott et al.,\n",
      "2020; Smit et al., 2020), machine learning models\n",
      "are still too error-prone to use broadly. Since they\n",
      "are not perfect, they typically play the role of as-\n",
      "sisting users in tasks like building cohorts (Pons\n",
      "et al., 2016) or in providing clinical decision sup-\n",
      "port (Demner-Fushman et al., 2009).\n",
      "1Code available at https://github.com/\n",
      "Liyan06/DocumentIE .\n",
      "Transformer\n",
      "[0]Severe encephalomalacia in the temporal lobes a nd\n",
      "frontal lobes bilaterally with reactive gliosis in the left\n",
      "frontal lobe. [1] Moderate enlargement of the ventricular\n",
      "system. [2] No abnormal enhancement. [3] Near completeopaciﬁcation of the left maxillary sinus. …evidence\tsents: \t0,\t1\n",
      "Interpretlabel\t(mass\teﬀect):\t\n",
      "negative\n",
      "Accurate?Plausible? (model\n",
      "evidence agrees w/human-\n",
      "labeled evidence)Faithful?  (model predicti on\n",
      "on evidence agrees w/full doc)Figure 1: Our basic model setup. A Transformer-based\n",
      "model makes document-level predictions on an exam-\n",
      "ple of our brain MRI reports. An interpretation method\n",
      "extracts the evidence sentences used by the model. Our\n",
      "system is evaluated according to the criteria of accu-\n",
      "racy, faithfulness, and plausibility.\n",
      "To be most usable in conjunction with users,\n",
      "these systems should not just produce a decision,\n",
      "but a justiﬁcation for their answer. The ideal system\n",
      "therefore obtains high predictive accuracy, but also\n",
      "returns a rationale that allows a human to verify the\n",
      "predicted label (Rudie et al., 2019).\n",
      "Our goal is to study document-level informa-\n",
      "tion extraction systems that are both accurate and\n",
      "which make predictions based on the correct infor-\n",
      "mation (Doshi-Velez and Kim, 2017). This process\n",
      "involves identifying what evidence the model actu-\n",
      "ally used, verifying the model’s prediction based on\n",
      "that evidence, and checking whether that evidence\n",
      "aligns with what humans would use, which would\n",
      "allow a user to quickly see if the system is correct.\n",
      "For example, in Figure 1, localizing the prediction\n",
      "ofmass effect (a feature expressing whether there\n",
      "is evidence of brain displacement by a mass like aarXiv:2110.07686v2  [cs.CL]  18 May 2022\n"
     ]
    }
   ],
   "source": [
    "print(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Making Document-Level Information Extraction Right for the Right Reasons Liyan Tang1, Dhruv Rajan1, Suyash Mohan2, Abhijeet Pradhan3, R. Nick Bryan1, Greg Durrett1 1The University of Texas at Austin 2University of Pennsylvania 3Galileo CDS Inc. lytang@utexas.edu, dhruv.rajan@utexas.edu Suyash.Mohan@pennmedicine.upenn.edu, ap@galileocds.com nick.bryan@austin.utexas.edu, gdurrett@cs.utexas.edu Abstract Document-level models for information extraction tasks like slot-ﬁlling are ﬂexible: they can be applied to settings where information is not necessarily localized in a single sentence. For example, key features of a diagnosis in a radiology report may not be explicitly stated in one place, but nevertheless can be inferred from parts of the report’s text. However, these models can easily learn spurious correlations between labels and irrelevant information. This work studies how to ensure that these models make correct inferences from complex text and make those inferences in an auditable way: beyond just being right, are these models “right for the right reasons?” We experiment with post-hoc evidence extraction in a predict-select-verify framework using feature attribution techniques. We show that regularization with small amounts of evidence supervision during training can substantially improve the quality of extracted evidence. We evaluate on two domains: a small-scale labeled dataset of brain MRI reports and a largescale modiﬁed version of DocRED (Yao et al., 2019) and show that models’ plausibility can be improved with no loss in accuracy.1 1 Introduction Document-level information extraction (Yao et al., 2019; Christopoulou et al., 2019; Xiao et al., 2020; Guoshun et al., 2020) has seen great strides due to the rise of pre-trained models (Devlin et al., 2019). But in high-stakes domains like medical information extraction (Irvin et al., 2019; McDermott et al., 2020; Smit et al., 2020), machine learning models are still too error-prone to use broadly. Since they', metadata={'source': '/tmp/tmp7z053w48', 'page': 0}),\n",
       " Document(page_content='Smit et al., 2020), machine learning models are still too error-prone to use broadly. Since they are not perfect, they typically play the role of assisting users in tasks like building cohorts (Pons et al., 2016) or in providing clinical decision support (Demner-Fushman et al., 2009). 1Code available at https://github.com/ Liyan06/DocumentIE . Transformer [0]Severe encephalomalacia in the temporal lobes a nd frontal lobes bilaterally with reactive gliosis in the left frontal lobe. [1] Moderate enlargement of the ventricular system. [2] No abnormal enhancement. [3] Near completeopaciﬁcation of the left maxillary sinus. …evidence\\tsents: \\t0,\\t1 Interpretlabel\\t(mass\\teﬀect):\\t negative Accurate?Plausible? (model evidence agrees w/humanlabeled evidence)Faithful?  (model predicti on on evidence agrees w/full doc)Figure 1: Our basic model setup. A Transformer-based model makes document-level predictions on an example of our brain MRI reports. An interpretation method extracts the evidence sentences used by the model. Our system is evaluated according to the criteria of accuracy, faithfulness, and plausibility. To be most usable in conjunction with users, these systems should not just produce a decision, but a justiﬁcation for their answer. The ideal system therefore obtains high predictive accuracy, but also returns a rationale that allows a human to verify the predicted label (Rudie et al., 2019). Our goal is to study document-level information extraction systems that are both accurate and which make predictions based on the correct information (Doshi-Velez and Kim, 2017). This process involves identifying what evidence the model actually used, verifying the model’s prediction based on that evidence, and checking whether that evidence aligns with what humans would use, which would allow a user to quickly see if the system is correct. For example, in Figure 1, localizing the prediction ofmass effect (a feature expressing whether there is evidence of brain displacement by a', metadata={'source': '/tmp/tmp7z053w48', 'page': 0}),\n",
       " Document(page_content='prediction ofmass effect (a feature expressing whether there is evidence of brain displacement by a mass like aarXiv:2110.07686v2  [cs.CL]  18 May 2022', metadata={'source': '/tmp/tmp7z053w48', 'page': 0}),\n",
       " Document(page_content='tumor) to the ﬁrst two sentences allows a trained user in a clinical decision support setting to easily verify what was extracted here. Our evidence extraction hews to principles of both faithfulness and plausibility (Jain et al., 2020; Jacovi and Goldberg, 2020; Miller, 2019). Rather than use complex approaches with intermediate latent variables for extraction (Lei et al., 2016), we focus on what can be done with offthe-shelf pre-trained models (Liu et al., 2019) using post-hoc interpretation. We explore various interpretation methods to ﬁnd key parts of each document that were used by the model. We ask two questions: ﬁrst, can we identify the document sentences that truly contributed to the prediction (faithfulness)? Using the ranking of sentences provided by an interpretation method, we extract a set of sentences where the model returns nearly the same prediction as before, thus verifying that these sentences are a sufﬁcient explanation for the model. Second, do these document sentences align with what users annotated (plausibility)? Unsurprisingly, we ﬁnd that this alignment is low in a basic Transformer model. To further improve the alignment with human annotation, we consider injecting small amounts of sentence-level supervision. Critically, in the brain MRI extraction setting we consider (see Table 1), large-scale sentence-level annotation is not available; most instances in the dataset only have document-level labels from existing clinical decision support systems, making it a weaklysupervised setting (Pruthi et al., 2020; Patel et al., 2020). We explore two methods for using this small amount of annotation, chieﬂy based around supervising or regularizing the model’s behavior. One notion is entropy maximization: the model should be uncertain when it isn’t exposed to sufﬁcient evidence (Feng et al., 2019). Another is attention regularization where the model is encouraged to attend to key pieces of evidence. While attention is not entirely connected with what', metadata={'source': '/tmp/tmp7z053w48', 'page': 1}),\n",
       " Document(page_content='encouraged to attend to key pieces of evidence. While attention is not entirely connected with what the model uses (Jain and Wallace, 2019), we can investigate whether this leads to a model whose explanations leverage this information more heavily. We validate our methods ﬁrst on a small dataset of radiologists’ observations from brain MRIs. These reports are annotated with document-level key features related to different aspects of the report, which we want to extract in a faithful way. We see positive results here even in a small-data con-Report Finding [0]Severe encephalomalacia in the temporal lobes and frontal lobes bilaterally with reactive gliosis in the left frontal lobe. [1]Moderate enlargement of the ventricular system .[2]No abnormal enhancement .[3]Near complete opaciﬁcation of the left maxillary sinus. ... mass_effect : negative evid:[0, 1] implicit side: bilateral evid:[0] explicit t2: increased evid:[0] implicit contrast_enhancement : No evid:[2] explicit Table 1: Example from annotated brain MRI reports. Labels and supporting evidence for 4key features are annotated for this example report presented. “Explicit” means the label of given key feature can be directly inferred by the highlighted terms; “implicit” instead indicates that it requires domain knowledge and potential reasoning skills to label. We want the model to identify implicit features while not leveraging dataset biases or reasoning incorrectly about explicit ones. dition, but to understand how this method would scale with larger amounts of data, we adapt the DocRED relation extraction task (Yao et al., 2019) to be a document-level classiﬁcation task. The question of which sentence in the document describes the relation between the two entities, if there even is one, is still quite challenging, and we show our techniques can lead to improvements in a weaklylabeled setting here as well. Our contributions are (1) We apply evidence extraction methods to document-level classiﬁcation and', metadata={'source': '/tmp/tmp7z053w48', 'page': 1}),\n",
       " Document(page_content='Our contributions are (1) We apply evidence extraction methods to document-level classiﬁcation and slot-ﬁlling tasks, emphasizing a new brain MRI dataset that we annotate. (2) We explore using weak sentence-level supervision in two techniques adapted from prior work; (3) We evaluate pre-trained models and evidence extraction through various interpretation methods for plausibility compared to human annotation, while ensuring faithfulness of the evidence. 2 Background 2.1 Motivation We start with an example from a brain MRI report in Table 1. Medical information extraction involves tasks such as identifying important medical terms from text (Irvin et al., 2019; Smit et al., 2020) and normalizing names into standard concepts using domain-speciﬁc ontologies (Cho et al., 2017). One application in clinical decision support, shown here, requires extracting the values of certain key features (clinical ﬁndings) from these reports or medical images (Rudie et al., 2021; Duong et al., 2019).', metadata={'source': '/tmp/tmp7z053w48', 'page': 1}),\n",
       " Document(page_content='This extraction should be accurate, but it should also make predictions that are correctly sourced, to facilitate review by a radiologist or someone else using the system (Rauschecker et al., 2020; Cook et al., 2018). The ﬁnding section of a brain MRI report often describes these key features in both explicit and implicit ways. For instance, contrast enhancement, one of our key features, is mentioned explicitly much of the time; see no abnormal enhancement in the third sentence. A rule-based system can detect this type of evidence easily. But some key features are harder to identify and require reasoning over context and draw on implicit cues. For example, severe encephalomalacia in the ﬁrst sentence and enlargement of the ventricular system in the following sentence are both implicit signs of positive mass effect and either is sufﬁcient to infer the label. It is signiﬁcantly harder to built a rule-based extractor for this case. Learning-based systems have the potential to do much better here, but lack of understanding about their behavior can lead to hard-topredict failure modes, such as acausal prediction of key features (e.g., inferring evidence about mass effect from a hypothesized diagnosis somewhere in the report, where the causality is backwards). Our work aims to leverage the ability of learningbased systems to capture implicit features while improving their ability to make predictions that are sourced from the correct evidence and can be easily veriﬁed. 2.2 Problem Setting The problem we tackle in this work can be viewed as document-level classiﬁcation. Let D= fx1;:::;x ngbe a document consisting of nsentences. The document is annotated with a set of labels (ti;yi)wheretiis an auxiliary input specifying a particular task for this document (e.g., mass effect) and yiis the label associated with that task from a discrete label space f1;:::;dg. In our adaptation of the DocRED task, we consider t= (e1;e2)to classify the relationship (if any) between a pair of', metadata={'source': '/tmp/tmp7z053w48', 'page': 2}),\n",
       " Document(page_content='of the DocRED task, we consider t= (e1;e2)to classify the relationship (if any) between a pair of entities (e1;e2)in a document, deﬁned in Section 4.1.2. Our method takes a pair (D;t)and then computes the label ^ytfrom a predictor ^yt=f(D;t). We can then extract evidence , a set of sentences, post-hoc using a separate procedure gsuch as a feature attribution method: ^Et=g(f;D;t )Supervision In addition to the labels yt, we assume access to a small number of examples with additional supervision in each domain. That is, for a(D;t;y t)triple, we also assume we are given a setE=fxi1;:::;x imgof ground-truth evidence with sentence indices fi1;:::;i mg. This evidence should be sufﬁcient to compute the label, but not always necessary; for example, if multiple sentences can contribute to the prediction, they might all be listed as supporting evidence here. See Section 3.3 for more details. 2.3 Related Work Our work ﬁts into a broader thread of work on information extraction with partial annotation (Han et al., 2020). Due to the cost of collecting largescale data with good quality, distant supervision (DS) (Mintz et al., 2009) and ways to denoise autolabeled data from DS (Surdeanu et al., 2012; Wang et al., 2018) have been widely explored. However, the sentence-level setting typically features much less ambiguity about evidence needed to predict a relation compared to the document-level setting we explore. Several document-level RE datasets (Li et al., 2016a; Peng et al., 2017) have been proposed as well as efforts to tackle these tasks (Christopoulou et al., 2019; Xiao et al., 2020; Guoshun et al., 2020), which we explicitly build from. Explanation techniques To identify the sentences that the model considers as evidence, we draw on a recent body of work in explainable NLP focused on identifying salient features of the input. These primarily consist of input attribution techniques, such as LIME (Ribeiro et al., 2016), input reductions (Li et al., 2016b; Feng et al., 2018),', metadata={'source': '/tmp/tmp7z053w48', 'page': 2}),\n",
       " Document(page_content='such as LIME (Ribeiro et al., 2016), input reductions (Li et al., 2016b; Feng et al., 2018), attention-based explanations (Bahdanau et al., 2015) and gradient-based methods (Simonyan et al., 2014; Selvaraju et al., 2017; Sundararajan et al., 2017; Shrikumar et al., 2017). In present work, we extract rationales using commonly used model interpretation methods (described in Section 3.2) and focus on doing a thorough evaluation of the capabilities of DeepLIFT (Shrikumar et al., 2017) given its competitive performance in our interpretation methods comparison (Appendix B). Frameworks for interpretable pipelines Our goal of building a system grounded in evidence draws heavily on recent work on attribution techniques and model explanations, particularly notions of faithfulness and plausibility. Faithfulness refers', metadata={'source': '/tmp/tmp7z053w48', 'page': 2}),\n",
       " Document(page_content='to how accurately the explanation provided by the model truly reﬂects the information it used in the reasoning process (Jain et al., 2020). On the other hand, plausibility indicates to what extent the interpretation provided by the model makes sense to a person.2 “Select-then-predict ” approaches are one way to enforce faithfulness in pipelines (Jain et al., 2020): important snippets from inputs are extracted and passed through a classiﬁer to make predictions. Past work has used hard (Lei et al., 2016) or soft (Zhang et al., 2016) rationales, and other work has explicitly looked at tradeoffs in the amount of text extracted (Paranjape et al., 2020). Jacovi and Goldberg (2020) note several problems with this setup. Our work aims to align model behavior with what cues we expect a model to use (plausibility), but uses the predict-select-verify paradigm (Jacovi and Goldberg, 2020) to ensure that these are actually sufﬁcient cues for the model. Like our work, Pruthi et al. (2020) simultaneously trained a BERT-based model (Devlin et al., 2019) for the prediction task and a linear-CRF (Lafferty et al., 2001) module on top of it for the evidence extraction task with shared parameters. Compared to their work, we focus explicitly on what can be done with pre-trained models alone, not augmenting the model for evidence extraction. 3 Methods The systems we devise take (D;t)pairs as input and return (a) predicted labels ^ytfor eacht; (b) sets of extracted evidence sentences ^Etfrom an interpretation method. Figure 1 shows the basic setting. 3.1 Transformer Classiﬁcation Model We use RoBERTa (Liu et al., 2019) as our document classiﬁer. RoBERTa is a strong method that holds up even against more recent baselines with architectures designed for DocRED (Zhou et al., 2021). For each of our two domains, we use different pre-trained weights, as described in the training details in Appendix A. The task inputs are described in Section 4.1. 2The ERASER benchmark (DeYoung et al., 2020) is a', metadata={'source': '/tmp/tmp7z053w48', 'page': 3}),\n",
       " Document(page_content='A. The task inputs are described in Section 4.1. 2The ERASER benchmark (DeYoung et al., 2020) is a notable recent effort to evaluate explanation plausibility. However, we do not consider it here; we focus on the documentlevel classiﬁcation setting, and many of the ERASER tasks are not suitable or relevant for the approaches we consider, either being not natural (FEVER) or not having the same challenges as document-level classiﬁcation.3.2 Interpretation for Evidence Extraction Given any interpretation method as well as our model ^yt=f(D;t), we compute attribution scores with respect to the predicted class ytfor each token in the RoBERTa input representation. We then average over the absolute value of attribution score for each token in that sentence to give sentencelevel scoresfs1;:::;s ng. These give us a ranking of the sentences. Given a ﬁxed number of evidence sentenceskto extract, we can extract the top k sentences by these scores. We experiment with the following four widely used interpretation techniques in the present work. LIME (Ribeiro et al., 2016) offers explanations of an input by approximating the model’s predictions locally with an interpretable model. Input Gradient (Hechtlinger, 2016) and Integrated Gradients (Sundararajan et al., 2017) use gradients of the label with respect to the input to assess input importance; Integrated Gradients approximates the integral of this gradient with respect to the input along a straight path from a reference baseline.3 DeepLIFT (Shrikumar et al., 2017) attributes the change in the output from a reference output in terms of the difference in input from the reference input. Unless stated otherwise, we use DeepLIFT as our interpretation method, since it achieves the best results (comparable to Input Gradient) among the four interpretation options. Full comparison of interpretation methods is in Appendix B. To verify the extracted evidence (Jacovi and Goldberg, 2020), our main technique (SUFFICIENT ) feeds the model', metadata={'source': '/tmp/tmp7z053w48', 'page': 3}),\n",
       " Document(page_content='the extracted evidence (Jacovi and Goldberg, 2020), our main technique (SUFFICIENT ) feeds the model increasingly larger subsets of the document ranked by attribution scores (e.g., ﬁrst fsmaxg, thenfsmax;s2nd-maxg, etc.) until it (a) makes the same prediction as when taking the whole document as input and (b) assigns that prediction at least \\x15times the probability4when the whole document is taken as input. We consider this attribution faithful: it is a subset of the input supporting the model’s decision judged as important by the attribution method. 3.3 Improving Evidence Extraction While many document-level extraction settings do not have sentence-level attributions labeled for ev3We use the most typical baseline that consists of replacing the inputs in Dwith[MASK] tokens from RoBERTa. 4The value of\\x15is a tolerance hyper-parameter for selecting sentences and it set to 0:8throughout the experiments. Our method is robust to the choice of \\x15in a reasonable range, as shown in Appendix B.', metadata={'source': '/tmp/tmp7z053w48', 'page': 3}),\n",
       " Document(page_content='ery decision, one can in practice annotate a small fraction of a dataset with such ground-truth rationales. This is indeed the case for our brain MRI case study. Past work has shown signiﬁcant beneﬁts from integrating this supervision into learning (Strout et al., 2019; Dua et al., 2020; Pruthi et al., 2021). Assume that a subset of our labeled data consists of(D;t;y t;Et)tuples with ground truth evidence sentence indices Et=fi1;:::;i mg. We consider two modiﬁcations to our model training, namely attention regularization (Pruthi et al., 2021), entropy maximization (Feng et al., 2018), and their combination. An illustration of both methods is shown in Figure 2. Attention regularization Attention regularization encourages our model f(D;t)to leverage more information from Et. Speciﬁcally, let A=f\\x0b1;:::;\\x0b ngbe the attention vector from the[CLS] token in the ﬁnal layer to all tokens inD. During learning, we add the following loss to the training objective: `attn=\\x00logP i2Et\\x0bi, encouraging the model to attend to any token iin the labeled sentence-level evidence set. Entropy maximization When there is no sufﬁcient information contained in the text to infer any predictions, entropy maximization encourages a model to be uncertain, represented by a uniform probability distribution across all classes (DeYoung et al., 2020; Feng et al., 2019). Doing so should encourage the model to notmake predictions based on irrelevant sentences. We can achieve this by taking a reduced document D0=DnEtas input by removing evidence Etfrom original documentD. We treat (D0;t)pairs as extra training examples where we aim to maximize the entropy \\x00P yP(yjD0) logP(yjD0)over all possible y.5 4 Experiments 4.1 Datasets and Evaluation Metrics We investigate our methods on (a) a small collection of brain MRI reports from radiologists’ observations; and (b) a modiﬁed version of the DocRED datatset. The statistics for both datatsets are included in Appendix D. For both datasets, we evaluate on task', metadata={'source': '/tmp/tmp7z053w48', 'page': 4}),\n",
       " Document(page_content='The statistics for both datatsets are included in Appendix D. For both datasets, we evaluate on task accuracy (captured by either accuracy or prediction macro-F1) as well as evidence selection 5We found this to work better than enforcing a uniform distribution over attention, which is much harder for the model to achieve. Transformer[CLS] [0] Severe encephalomalacia … [1] Moderate enlargement … [2] No abnormal … [3] Near complete …Final attention layer Label Distribution P(y|D) Entropy maximization: (ENTROPY)\\u2028Attention regularization  (ATTN) Transformer deletes relevant sentence; maximizes prediction entropy Transformerencourages attentions on supporting evidence [CLS] [0] Severe… [1] Moderate … [2] No … [3] Near… [CLS] [0] Severe… [1] Moderate … [2] No … [3] Near… Standard Supervised  Training:  max log P(y|D)Figure 2: An illustration of attention regularization and entropy maximization using the example in Table 1. The model is predicting the label for key feature t2. accuracy (macro-F1) or precision, measuring how well the model’s evidence selection aligns with human annotations. We will use the SUFFICIENT method deﬁned in Section 3.2 to select evidence sentences which guarantee that our predictions on the given evidence subsets will match the model’s predictions on the full document. For the brain MRI report dataset, we evaluate evidence extraction by precision since human annotators typically only need to refer to one sentence to reach the conclusion but our model and baselines may extract more than one sentence. 4.1.1 Brain MRI Reports We present a new dataset of de-identiﬁed radiology reports from brain MRIs. It consists of the “ﬁndings” sections of reports, which present observations about the image, with labels for preselected key features by attending physicians and fellows. Crucially, these features are labeled based on the original radiology image , not the report. The document-level labels are therefore noisy because the radiologists’ labels may', metadata={'source': '/tmp/tmp7z053w48', 'page': 4}),\n",
       " Document(page_content=', not the report. The document-level labels are therefore noisy because the radiologists’ labels may disagree with the ﬁndings written in the report. A key feature is an observable variable t, which can take on dtpossible values. We focus on the evaluation of two key features, namely contrast enhancement andmass effect , since they appear in most of manually annotated reports. For our RoBERTa classiﬁcation model, we only feed the document and train separate classiﬁers for each key feature, with no shared parameters between these. Annotation We have a moderate number (327) of reports that have noisy labels from the process', metadata={'source': '/tmp/tmp7z053w48', 'page': 4}),\n",
       " Document(page_content='above. We treat these as our training set. However, all of these labels are document-level. To evaluate models’ performance on more ﬁnegrained evidence labels, we randomly select 86 unlabeled reports (not overlapping with the 327 for training) and asked four radiology residents to (1) assign key feature labels and reach consensus, while (2) highlighting sentences that support their decision making. We use Prodigy6as our annotation interface. See Appendix E for more details about our annotation instructions. Pseudo sentence-level supervision Since we have limited number of annotated reports for evaluation, we need a way to prepare weak sentencelevel supervision ( Et) while training. To achieve this, we use sentences selected by our rule-based system as pseudo evidence to supervise models’ behavior. We use 10% of this as supervision while training for consistency with the DocRED setting. Rule-based system Our rule-based system uses keyword matching to identify instances of mass effect and contrast enhancement in the reports, and negspaCy to detect negations of these key features. Data split For the results in Section 5, we evaluate on reports that contain ground truth ﬁne-grained annotations for either contrast enhancement or mass effect, respectively. There are 64 and 68 out of 86 documents total in each of these categories. We call this the BRAIN MRI set. When we restrict to this set for evaluation, all of the documents we study where the annotators labeled something related to contrast enhancement end up having an explicit mention of it. However, for mass effect , this is not always the case; Table 9 in Appendix shows an example where mass effect is discussed implicitly in the ﬁrst sentence. 4.1.2 Adapted DocRED DocRED (Yao et al., 2019) is a document-level relation extraction (RE) dataset with large scale human annotation of relevant evidence sentences. Unlike sentence-level RE tasks (Qin et al., 2018; Alt et al., 2020), it requires reading multiple sentences and', metadata={'source': '/tmp/tmp7z053w48', 'page': 5}),\n",
       " Document(page_content='RE tasks (Qin et al., 2018; Alt et al., 2020), it requires reading multiple sentences and reasoning about complex interactions between entities. We adapt this to a document-level relation classiﬁcation task: a document Dand two entity mentionse1;e2within the document are provided and the task is to predict the relation rbetweene1 ande2. We synthesize these examples from the 6https://prodi.gyModel Names Input Text DIRECT None FULLDOC Full document ENT Sentences containing at least one of the two query entities FIRST 2 First two sentences from a doc. FIRST 3 First three sentences from a doc. BESTPAIR Two sentences yielding highest prediction prob. (incl. variants using regularization) SUFFICIENT Sufﬁcient sentences selected by interpretation methods (incl. variants using regularization) Table 2: Model names used in the experiments and their associated evidence given as inputs. original dataset and sample random entity pairs from documents to which we assign an NAclass to construct negative pairs exhibiting no relation. The model input is represented as: [CLS]<ent-1>[SEP]<ent-2>[SEP]<doc>[SEP] . We use the encoding of [CLS] in the last layer to make predictions. To make the setting more realistic, we do not use the large-scale evidence annotation and assume there is limited sentence-level supervision available. To be speciﬁc, we include 10% ﬁne-grained annotations in our adapted DocRED dataset. 4.2 Models Due to richer and higher-quality supervisions in the DocRED setting, we conduct a larger set of ablations and comparisons there. We compare against a subset of these models in the radiology setting. Baselines We consider a number of baselines for adapted DocRED which return both predicted labels and evidence. (1) DIRECT predicts the relation directly from the entity pairs without any sentences as input, using a model trained with just these inputs. (2) FULLDOCtakes the full document as selected evidence and uses the base RoBERTa model (3)ENTtakes all sentences with', metadata={'source': '/tmp/tmp7z053w48', 'page': 5}),\n",
       " Document(page_content='full document as selected evidence and uses the base RoBERTa model (3)ENTtakes all sentences with entity mentions e1 ande2as input; (4) FIRST 2, F IRST 3retrieve the ﬁrst2and3sentences from a document, respectively; and (5) BESTPAIRchooses the best sentence pair by ﬁrst taking each individual sentence as input to the model and then picking top two sentences having highest probabilities on their predictions. SUFFICIENT is our main method for both datasets, which we then augment with additional supervision as described in Section 3.3. We use subscripts attn, entropy, both andnone to represent attention regularization, entropy max-', metadata={'source': '/tmp/tmp7z053w48', 'page': 5}),\n",
       " Document(page_content='imization, the combination of two, and neither. Both BESTPAIRandSUFFICIENT methods leverage backbone RoBERTa models trained with loss functions mentioned above, differing only in their evidence selection. Table 2 summarizes the abbreviated names of models and their inputs. Training details are described in Appendix A. Metrics We report both the accuracy and F 1for the model ( Full Doc ) as well as evaluation of Evidence selection compared to human judgments, either precision or F 1. We also report results in theReduced Doc setting, where only the selected evidence sentences are fed to the RoBERTa model (trained over whole documents) as input. For our SUFFICIENT method, this accuracy is the same as the full method by construction, but note that it can differ for other methods. This reduced setting serves as a sanity check for the faithfulness of our explanation techniques. Note once again that accuracy in the Full Doc case can differ for our methods that are trained with different regularization schemes, as these yield different models that return different predicted labels in addition to different evidence. 5 Results 5.1 Results on Brain MRI Table 3 shows the performance of our models and baselines in terms of label prediction and evidence extraction. For each result, we perform a paired bootstrap test comparing to SUFFICIENT none. We underline results that are better at a signiﬁcance level ofp= 0:05on the corresponding metrics. In themass effect setting, our SUFFICIENT bothmodel achieves the highest evidence extraction precision of the learning-based models, exceeds FULLDOC, FIRST 2/3, and BESTPAIRon the metric by a large margin, and nearly matches that of the rule-based system. It is difﬁcult to be more reliable than a rule-based system, which will nearly always make correctly-sourced predictions. But this model is able to combine that reliability with the higher F1of a learned model . Note that due to the high base rates of certain ﬁndings, we focus on F 1instead', metadata={'source': '/tmp/tmp7z053w48', 'page': 6}),\n",
       " Document(page_content='a learned model . Note that due to the high base rates of certain ﬁndings, we focus on F 1instead of accuracy. We see a similar pattern on contrast enhancement as well, although the evidence precision is lower in that case. These results show that learning-based systems make accurate predictions in this domain, and thatModelLabel Evidence Full Doc Reduced Doc Acc F1 Acc F1 Pre Len Mass Effect FULLDOC 66.6 42.1 66.6 42.1 16.5 10.1 FIRST 2 \\x00 \\x00 82.4 45.2 21.3 2.00 FIRST 3 \\x00 \\x00 82.4 45.2 24.0 3.00 RULE 77.9 11.8 77.9 11.8 84.8 1.46 BESTPAIR none 66.6 42.1 82.4 52.2 24.3 2.00 BESTPAIR both 76.7 60.0 79.4 44.3 50.7 2.00 SUFFICIENT none 66.6 42.1 Identical to Full Doc16.5 2.84 SUFFICIENT attn 69.2 47.6 65.6 2.31 SUFFICIENT entropy 45.3 0.0 15.8 2.50 SUFFICIENT both 76.7 60.0 77.8 1.51 Contrast Enhancement FULLDOC 69.5 60.9 69.5 60.9 13.5 10.1 FIRST 2 \\x00 \\x00 67.2 55.3 14.1 2.00 FIRST 3 \\x00 \\x00 70.3 62.4 14.6 3.00 RULE 68.8 56.5 68.8 56.5 87.1 1.67 BESTPAIR none 69.5 60.9 73.4 67.7 10.9 2.00 BESTPAIR both 90.8 87.2 89.1 88.4 54.7 2.00 SUFFICIENT none 69.5 60.9 Identical to Full Doc33.5 2.84 SUFFICIENT attn 85.8 81.0 60.7 2.48 SUFFICIENT entropy 71.5 59.5 25.2 2.55 SUFFICIENT both 90.8 87.2 71.7 1.50 Table 3: Model performance on B RAIN MRI. Models are evaluated under two settings by taking (a) full document (Full Doc); (b) selected evidence (Reduced Doc) as inputs. R ULE is the baseline mentioned in Section 4.1.1. Prestands for the precision of evidence selection, andLenis the average number of sentences extracted. Underlined results are better than S UFFICIENT noneon the corresponding metric according to a paired bootstrap test with p= 0:05. ModelLabel Evidence Full Doc Reduced Doc Acc F1 Acc F1 F1 Len DIRECT \\x00 \\x00 66.4 45.3 \\x00 \\x00 FULLDOC 83.0 66.0 83.0 66.0 34.9 8.03 FIRST 2 \\x00 \\x00 75.3 58.1 47.9 2.00 FIRST 3 \\x00 \\x00 77.5 60.7 44.6 3.00 ENT \\x00 \\x00 82.4 65.4 61.5 3.93 BESTPAIR none 83.0 66.0 73.9 55.3 39.2 2.00 BESTPAIR attn 83.2 65.0 73.4 53.5 43.9 2.00 BESTPAIR entropy 81.8 64.2 78.5 58.2 52.3', metadata={'source': '/tmp/tmp7z053w48', 'page': 6}),\n",
       " Document(page_content='55.3 39.2 2.00 BESTPAIR attn 83.2 65.0 73.4 53.5 43.9 2.00 BESTPAIR entropy 81.8 64.2 78.5 58.2 52.3 2.00 BESTPAIR both 82.7 66.5 81.6 65.3 66.2 2.00 SUFFICIENT none 83.0 66.0 Identical to Full Doc67.2 1.42 SUFFICIENT attn 83.2 65.0 70.3 1.45 SUFFICIENT entropy 81.8 64.2 69.9 1.65 SUFFICIENT both 82.7 66.5 73.1 1.65 human \\x00 \\x00 \\x00 \\x00 \\x00 1.59 Table 4: Model performance on adapted DocRED. Models are evaluated under two settings as in BRAIN MRI. Underlined results are better than S UFFICIENT noneon the corresponding metric according to a paired bootstrap test with p= 0:05.', metadata={'source': '/tmp/tmp7z053w48', 'page': 6}),\n",
       " Document(page_content='ModelMass Effect Ctr. Enhance. Mean Max Mean Max SUFFICIENT none 7.3 7.4 28.6 29.8 SUFFICIENT both 18.9 19.2 37.9 42.0 Table 5: Distributions of attribution mass over explicit cues (“enhancement” for contrast enhancement and “effect” for mass effect ) for our best model and the baseline. Mean/Max is the mean of instance-wise average/maximum of the normalized attribution mass falling on the given token. their evidence extraction can be improved with better training, even in spite of the small size of the training set. In section 5.2, we focus on the adapted DocRED setting, which allows us to examine our model’s performance in a higher-data regime. Attribution scores are more peaked at the occurrence of key terms. We conduct analysis on how the attribution scores from SUFFICIENT both are peaked around the correct evidence compare to that from SUFFICIENT noneusing our manually annotated set BRAIN MRI . We compute the mean of the instance-wise average and maximum of the normalized attribution mass falling into a few explicit tokens: enhancement for contrast enhancement and effect for mass effect, which are common explicit indicators in the context of speciﬁed key features. The results in Table 5 show attribution scores being peaked around the correct terms, highlighting that these models can be guided to not only make correct predictions but attend to the right information. Table 9 in the Appendix shows visualizations of attribution scores for an example in BRAIN MRI using DeepLIFT. Even though baseline models make correct predictions, their attribution mass is diffused over the document. With the help of regularization, our model is capable of capturing implicit cues such as downward displacement of the brain stem, although it is trained on an extremely small training set with only explicit cues like mass effect in a weak sentence-level supervision framework. 5.2 Results on Adapted DocRED Comparison to baselines Table 4 shows that the ENTbaseline is quite strong at', metadata={'source': '/tmp/tmp7z053w48', 'page': 7}),\n",
       " Document(page_content='on Adapted DocRED Comparison to baselines Table 4 shows that the ENTbaseline is quite strong at DocRED evidence extraction. However, our best method still exceeds this method on both label accuracy as well as evidence extraction while extracting more succinct explanations. We see that the ability to extract a variable-length explanation is key, with FIRST 2, FIRST 3andBESTPAIR performing poorly. Notably, these methods exhibit a drop in accuracy inthe reduced doc setting for each method compared to the full doc setting, showing that the explanations extracted are not faithful. Learning-based models with appropriate regularization perform relatively better in this larger-data setting From Table 3 and Table 4, we can observe that various regularization techniques applied to SUFFICIENT models maintain or improve overall model performance on both key feature and relation classiﬁcation. We see that our SUFFICIENT methods do not compromise on accuracy but make predictions based on plausible evidence sets, which is more evident when we have richer training data. We perform further error analysis in Appendix F. Faithfulness of techniques One may be concerned that, like attention values (Jain and Wallace, 2019), our feature attribution methods may not faithfully reﬂect the computation of the model. We emphasize again that the SUFFICIENT paradigm on top of the DeepLIFT method isfaithful by our deﬁnition. For a model f, we measure the faithfulness by checking the agreement between ^y=f(D;t) andy0=f(^Et;t), where ^Etis the extracted evidence we feed into the same model under the reduced document setting. This is shown for all methods in the “Reduced doc” columns in Tables 3 and 4. We see a drop in performance from techniques such as BESTPAIR: the full model does not make the same judgment on these evidence subsets, but by deﬁnition it does in the S UFFICIENT setting. As further evidence of faithfulness, we note that only a relatively small number of evidence sentences, in line', metadata={'source': '/tmp/tmp7z053w48', 'page': 7}),\n",
       " Document(page_content='evidence of faithfulness, we note that only a relatively small number of evidence sentences, in line with human annotations, are extracted in the SUFFICIENT method. These small subsets are indicated by feature attribution methodsandsufﬁcient to reproduce the original model predictions with high conﬁdence, suggesting that these explanations are faithful. 6 Conclusion In this work, we develop techniques to employ small amount of data to improve reliability of document-level IE systems in two domains. We systematically evaluate our model from perspectives of faithfulness and plausibility and show that we can substantially improve models’ capability in focusing on supporting evidence while maintaining their predictive performance, leading to models that are “right for the right reasons.”', metadata={'source': '/tmp/tmp7z053w48', 'page': 7}),\n",
       " Document(page_content='Acknowledgments This work was partially supported by NSF Grant IIS-1814522 and a Texas Health Catalyst grant. Thanks to Scott Rudkin, Gregory Mittl, Raghav Mattay, and Chuan Liang for assistance with the annotation. References Christoph Alt, Aleksandra Gabryszak, and Leonhard Hennig. 2020. Probing linguistic features of sentence-level representations in neural relation extraction. In Proceedings of ACL . Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In Proceedings of the International Conference on Learning Representations (ICLR) . Hyejin Cho, Wonjun Choi, and Hyunju Lee. 2017. A method for named entity normalization in biomedical articles: application to diseases and plants. BMC Bioinformatics , 18(1). Fenia Christopoulou, Makoto Miwa, and Sophia Ananiadou. 2019. Connecting the dots: Document-level neural relation extraction with edge-oriented graphs. InProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) . Association for Computational Linguistics. Tessa Cook, James C. Gee, R. Nick Bryan, Jeffrey T. Duda, Po-Hao Chen, Emmanuel Botzolakis, Suyash Mohan, Andreas Rauschecker, Jeffrey Rudie, and Ilya Nasrallah. 2018. Bayesian network interface for assisting radiology interpretation and education. InMedical Imaging 2018: Imaging Informatics for Healthcare, Research, and Applications . SPIE. Dina Demner-Fushman, Wendy W. Chapman, and Clement J. McDonald. 2009. What can natural language processing do for clinical decision support? Journal of Biomedical Informatics , 42(5):760–772. Biomedical Natural Language Processing. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for', metadata={'source': '/tmp/tmp7z053w48', 'page': 8}),\n",
       " Document(page_content='In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics. Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and Byron C. Wallace. 2020. ERASER: A benchmark to evaluate rationalized NLP models. In Proceedings of the 58th Annual Meeting of the Associationfor Computational Linguistics . Association for Computational Linguistics. Finale Doshi-Velez and Been Kim. 2017. Towards a rigorous science of interpretable machine learning. arXiv: Machine Learning . Dheeru Dua, Sameer Singh, and Matt Gardner. 2020. Beneﬁts of intermediate annotations in reading comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 5627–5634, Online. Association for Computational Linguistics. M.T. Duong, J.D. Rudie, J. Wang, L. Xie, S. Mohan, J.C. Gee, and A.M. Rauschecker. 2019. Convolutional neural network for automated FLAIR lesion segmentation on clinical brain MR imaging. American Journal of Neuroradiology , 40(8):1282–1290. Shi Feng, Eric Wallace, and Jordan Boyd-Graber. 2019. Misleading failures of partial-input baselines. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics. Shi Feng, Eric Wallace, Alvin Grissom II, Mohit Iyyer, Pedro Rodriguez, and Jordan Boyd-Graber. 2018. Pathologies of neural models make interpretations difﬁcult. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics. Nan Guoshun, Guo Zhijiang, Sekuli ´c Ivan, and Lu Wei. 2020. Reasoning with latent structure reﬁnement for document-level relation extraction. In Proceedings of ACL . Suchin Gururangan, Ana Marasovi ´c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy,', metadata={'source': '/tmp/tmp7z053w48', 'page': 8}),\n",
       " Document(page_content='In Proceedings of ACL . Suchin Gururangan, Ana Marasovi ´c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Don’t stop pretraining: Adapt language models to domains and tasks. In Proceedings of ACL . Xu Han, Tianyu Gao, Yankai Lin, Hao Peng, Yaoliang Yang, Chaojun Xiao, Zhiyuan Liu, Peng Li, Jie Zhou, and Maosong Sun. 2020. More data, more relations, more context and more openness: A review and outlook for relation extraction. In Proceedings of the 1st Conference of the Asia-Paciﬁc Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing , pages 745–758, Suzhou, China. Association for Computational Linguistics. Yotam Hechtlinger. 2016. Interpretation of prediction models using the input gradient. In Proceedings of the NeurIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems . Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, Jayne Seekins, David Mong, Safwan Halabi, Jesse Sandberg, Ricky Jones, David Larson, Curtis', metadata={'source': '/tmp/tmp7z053w48', 'page': 8}),\n",
       " Document(page_content='Langlotz, Bhavik Patel, Matthew Lungren, and Andrew Ng. 2019. Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison. Proceedings of the AAAI Conference on Artiﬁcial Intelligence , 33:590–597. Alon Jacovi and Yoav Goldberg. 2020. Towards faithfully interpretable NLP systems: How should we deﬁne and evaluate faithfulness? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics. Sarthak Jain and Byron C. Wallace. 2019. Attention is not Explanation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 3543–3556, Minneapolis, Minnesota. Association for Computational Linguistics. Sarthak Jain, Sarah Wiegreffe, Yuval Pinter, and Byron C. Wallace. 2020. Learning to faithfully rationalize by construction. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics. John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning , ICML ’01, page 282–289, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc. Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2016. Rationalizing neural predictions. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics. Jiao Li, Yueping Sun, Robin J. Johnson, Daniela Sciaky, Chih-Hsuan Wei, Robert Leaman, Allan Peter Davis, Carolyn J. Mattingly, Thomas C. Wiegers, and Zhiyong Lu. 2016a. BioCreative v CDR task corpus: a resource for chemical disease relation extraction. Database , 2016:baw068. Jiwei Li, Will Monroe, and Dan Jurafsky. 2016b. Understanding neural networks through', metadata={'source': '/tmp/tmp7z053w48', 'page': 9}),\n",
       " Document(page_content=', 2016:baw068. Jiwei Li, Will Monroe, and Dan Jurafsky. 2016b. Understanding neural networks through representation erasure. ArXiv , abs/1612.08220. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. In arXiv . Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In International Conference on Learning Representations . Matthew B. A. McDermott, Tzu-Ming Harry Hsu, Wei-Hung Weng, Marzyeh Ghassemi, and Peter Szolovits. 2020. Chexpert++: Approximating thechexpert labeler for speed, differentiability, and probabilistic output. CoRR , abs/2006.15229. Tim Miller. 2019. Explanation in artiﬁcial intelligence: Insights from the social sciences. Artiﬁcial Intelligence , 267:1–38. Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP , pages 1003–1011, Suntec, Singapore. Association for Computational Linguistics. Bhargavi Paranjape, Mandar Joshi, John Thickstun, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2020. An information bottleneck approach for controlling conciseness in rationale extraction. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Association for Computational Linguistics. Dhruvesh Patel, Sandeep Konam, and Sai P. Selvaraj. 2020. Weakly supervised medication regimen extraction from medical conversations. In ClinicalNLP@EMNLP . Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina Toutanova, and Wen tau Yih. 2017. Cross-sentence n-ary relation extraction with graph LSTMs. Transactions of the Association for Computational Linguistics , 5:101–115. Ewoud Pons, Loes M. M. Braun, M. G. Myriam Hunink, and Jan A. Kors.', metadata={'source': '/tmp/tmp7z053w48', 'page': 9}),\n",
       " Document(page_content='Linguistics , 5:101–115. Ewoud Pons, Loes M. M. Braun, M. G. Myriam Hunink, and Jan A. Kors. 2016. Natural language processing in radiology: A systematic review. Radiology , 279(2):329–343. PMID: 27089187. Danish Pruthi, Bhuwan Dhingra, Graham Neubig, and Zachary C. Lipton. 2020. Weakly- and semisupervised evidence extraction. In Findings of the Association for Computational Linguistics: EMNLP 2020 . Association for Computational Linguistics. Danish Pruthi, Bhuwan Dhingra, Livio Baldini Soares, M. Collins, Zachary C. Lipton, Graham Neubig, and William W. Cohen. 2021. Evaluating explanations: How much do explanations from the teacher aid students? Transactions of the Association for Computational Linguistics . Pengda Qin, Weiran Xu, and William Yang Wang. 2018. Robust distant supervision relation extraction via deep reinforcement learning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics. Andreas M. Rauschecker, Jeffrey D. Rudie, Long Xie, Jiancong Wang, Michael Tran Duong, Emmanuel J. Botzolakis, Asha M. Kovalovich, John Egan, Tessa C. Cook, R. Nick Bryan, Ilya M. Nasrallah, Suyash Mohan, and James C. Gee. 2020. Artiﬁcial intelligence system approaching', metadata={'source': '/tmp/tmp7z053w48', 'page': 9}),\n",
       " Document(page_content='neuroradiologist-level differential diagnosis accuracy at brain MRI. Radiology , 295(3):626–637. Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \"why should i trust you?\": Explaining the predictions of any classiﬁer. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD ’16, page 1135–1144, New York, NY , USA. Association for Computing Machinery. Jeffrey Rudie, Long Xie, Jiancong Wang, Jeffrey Duda, Joshua Choi, Raghav Mattay, Po-Hao Chen, R Nick Bryan, Emmanuel Botzolakis, Ilya Nasrallah, Tessa Cook, Suyash Mohan, James Gee, and Andreas Rauschecker. 2019. Artiﬁcial Intelligence System for Automated Brain MR Diagnosis Performs at Level of Academic Neuroradiologists and Augments Resident Performance. In Proceedings of the Society for Imaging Informatics in Medicine (SIIM) . Jeffrey D. Rudie, Jeffrey Duda, Michael Tran Duong, Po-Hao Chen, Long Xie, Robert Kurtz, Jeffrey B. Ware, Joshua Choi, Raghav R. Mattay, Emmanuel J. Botzolakis, James C. Gee, R. Nick Bryan, Tessa S. Cook, Suyash Mohan, Ilya M. Nasrallah, and Andreas M. Rauschecker. 2021. Brain MRI deep learning and bayesian inference system augments radiology resident performance. Journal of Digital Imaging, 34(4):1049–1058. Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. 2017. Grad-CAM: Visual explanations from deep networks via gradient-based localization. In 2017 IEEE International Conference on Computer Vision (ICCV) . IEEE. Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. 2017. Learning important features through propagating activation differences. In Proceedings of the 34th International Conference on Machine Learning - Volume 70 , ICML’17, page 3145–3153. JMLR.org. Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2014. Deep inside convolutional networks: Visualising image classiﬁcation models and saliency maps. In Workshop at International Conference on Learning', metadata={'source': '/tmp/tmp7z053w48', 'page': 10}),\n",
       " Document(page_content='image classiﬁcation models and saliency maps. In Workshop at International Conference on Learning Representations . Akshay Smit, Saahil Jain, Pranav Rajpurkar, Anuj Pareek, Andrew Ng, and Matthew Lungren. 2020. Combining automatic labelers and expert annotations for accurate radiology report labeling using BERT. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 1500–1519, Online. Association for Computational Linguistics. Julia Strout, Ye Zhang, and Raymond Mooney. 2019. Do human rationales improve machine explanations? In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , pages 56–62, Florence, Italy. Association for Computational Linguistics.Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution for deep networks. In Proceedings of the 34th International Conference on Machine Learning , volume 70 of Proceedings of Machine Learning Research , pages 3319–3328. PMLR. Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and Christopher D. Manning. 2012. Multi-instance multi-label learning for relation extraction. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning , pages 455– 465, Jeju Island, Korea. Association for Computational Linguistics. Xiaozhi Wang, Xu Han, Yankai Lin, Zhiyuan Liu, and Maosong Sun. 2018. Adversarial multi-lingual neural relation extraction. In Proceedings of the 27th International Conference on Computational Linguistics, pages 1156–1166, Santa Fe, New Mexico, USA. Association for Computational Linguistics. Chaojun Xiao, Yuan Yao, Ruobing Xie, Xu Han, Zhiyuan Liu, Maosong Sun, Fen Lin, and Leyu Lin. 2020. Denoising relation extraction from documentlevel distant supervision. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Association for Computational Linguistics. Yuan Yao, Deming', metadata={'source': '/tmp/tmp7z053w48', 'page': 10}),\n",
       " Document(page_content='in Natural Language Processing (EMNLP) . Association for Computational Linguistics. Yuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin, Zhenghao Liu, Zhiyuan Liu, Lixin Huang, Jie Zhou, and Maosong Sun. 2019. DocRED: A large-scale document-level relation extraction dataset. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics. Ye Zhang, Iain Marshall, and Byron C. Wallace. 2016. Rationale-augmented convolutional neural networks for text classiﬁcation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics. Wenxuan Zhou, Kevin Huang, Tengyu Ma, and Jing Huang. 2021. Document-level relation extraction with adaptive thresholding and localized context pooling. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence .', metadata={'source': '/tmp/tmp7z053w48', 'page': 10}),\n",
       " Document(page_content='A Implementation Details We train all RoBERTa models for 15 epochs with early stopping using 1TITAN-Xp GPU. We use AdamW (Loshchilov and Hutter, 2019) as our optimizer and initialize the model with robertabase for DocRED and biomed-robertabase (Gururangan et al., 2020) for brain MRI data, both with 125M parameters. The batch size is set to 16 for RoBERTa models trained with both attention regularization and entropy maximization and 8 for models with other loss functions, and the learning rate is 1e-5 with linear schedule warmup. The maximum number of tokens in each document is capped at 296 for modiﬁed DocRED and 360 for radiology reports. These numbers are chosen such that the number of tokens for around 95% of the documents is within these limits. Remaining tokens are clipped from the input. The hidden state of the [CLS] token from the ﬁnal layer is fed as input to a linear projection head to make predictions. The average training time for each model is around 4 GPU hours. B Interpretation Methods Comparison We evaluate four interpretation methods on SUFFICIENT none and SUFFICIENT bothusing adapted DocRED. These methods are widely used in the literature, namely Integrated Gradients, LIME, DeepLIFT, and Input Gradient, as discussed in Section 3.2. We compare their evidence extraction capabilities by selecting a wide range of \\x15, which controls the number of sentences to be selected. Results are shown in Figure 3. The four techniques generally perform similarly, with DeepLIFT and Input Gradient performing slightly better. For each interpretation method, the result of SUFFICIENT bothis signiﬁcantly better than that of SUFFICIENT none. Similar values of \\x15between 0.8 and 0.9 (preferring to select more sentences) work well across all methods. Table 6 shows the comparison over the threshold ( \\x15= 0:8) we choose for our experiments in Section 5. In general, our method is robust to model interpretation techniques and evidence selection threshold \\x15. Sentence ranking step', metadata={'source': '/tmp/tmp7z053w48', 'page': 11}),\n",
       " Document(page_content='robust to model interpretation techniques and evidence selection threshold \\x15. Sentence ranking step mentioned in Section 3.2 requires 0.3 GPU hour for Input Gradient and DeepLIFT, 2.5 GPU hours for Integrated Gradients, and 14 GPU hours for LIME. We choose 30 steps to approximate the integral for Integrated Gradients and 100 samples for each input to train the surrogate interpretable model (a linear model in Figure 3: Evidence F1 on adapted DocRED under four model interpretation methods for S UFFICIENT bothover a wide range of \\x15. our case) for LIME. SUFFICIENT none SUFFICIENT both LIME 54.0 70.8 Integrated Gradients 60.6 70.3 DeepLIFT 67.3 73.1 Input Gradient 68.3 73.5 Table 6: Evidence F1 on adapted DocRED under four model interpretation methods for S UFFICIENT noneand SUFFICIENT bothwhen\\x15= 0:8. C Limitations and Risks There are a few limitations of our work. First, we currently test our methods on document-level classiﬁcation and slot-ﬁlling tasks, but there are other task formats like span extraction that we do not investigate here. Second, we focus on off-theshelf pre-trained models (i.e. RoBERTa) in this paper, though we believe our methods could also be applied and adopted to other models. Finally, and most critically, the interpretation techniques we use are all fundamentally approximate; while visualizing model rationales can be useful in the context of clinical decision support systems, our evidence sets are not proof positive that a model’s predictions are reliable. Such systems need to be carefully deployed to avoid misleading practitioners into trusting them too readily. We view this as the principal risk of our work. D Dataset statistics We provide the statistics for both adapted DocRED and brain MRI reports dataset in Table 7. Both datasets are in English and the DocRED dataset is publicly available at https://github.com/ thunlp/DocRED .', metadata={'source': '/tmp/tmp7z053w48', 'page': 11}),\n",
       " Document(page_content='E Annotation Instructions We recruited four radiology residents to make annotations. They did not receive compensation for this project speciﬁcally. The annotation instructions for the BrainMRI dataset are provided in Figure 4. These were developed jointly with the annotators. In particular, decisions to exclude normal brain activity and confounders such as SVID were made to increase interannotator agreement after an initial round of annotation, making it easier for the labeling to focus on a single core disease or diagnosis per report. F Error Analysis The ﬁrst example in Table 8 shows a representative case where our model predicts the correct relation and extracts reasonable supporting evidence. Unsurprisingly, this happens most often in simple cases when reasoning over the interaction of sentences is not required. We observe a few common types of errors. First, we see potential alternatives for relations or evidence extraction . From around 60% of our randomly selected error cases, our model either predicts debatably correct relations or picks sentences that are related but not perfectly aligned with human annotations. The second row in Table 8 illustrates an example where the two entities exhibit multiple relationships; the model’s prediction is correct ( Vienna is place where Martinelli was both born and died), but differs from the annotated ground truth and supporting evidence. Such relations are relatively frequent in this dataset; a more complex multi-label prediction format is necessary to fully support these. Another type of error is complex logical reasoning. Even if our model can extract right evidence, it still fails in around 10% of random error cases requiring sophisticated reasoning. For example, to correctly predict the relation between Theobald Tiger and21 December 1935 in the third example in Table 8, a model needs to recognize that Theobald Tiger andKurt Tucholsky are in fact the same entity by referring to pseudonym , which is a challenging', metadata={'source': '/tmp/tmp7z053w48', 'page': 12}),\n",
       " Document(page_content='andKurt Tucholsky are in fact the same entity by referring to pseudonym , which is a challenging relation to recognize. Finally, the model sometimes selects more sentences than we truly need . Interestingly, this is an error in terms of evidence plausibility but not in terms of prediction . The number of extracted sentences is very high in around 25% of the random error cases. The last row from Table 8 is one ofrepresentative examples with this kind of error. Although our model possibly has already successfully extracted right evidence in the ﬁrst two steps, it continues selecting unnecessary sentences because the prediction conﬁdence is not high enough, a drawback in our way of selecting evidence mentioned in Section 4.2. Moreover, our model extracts one more sentence on average when predicting incorrect relations, suggesting that in these cases it does not cleanly focus on the correct information.', metadata={'source': '/tmp/tmp7z053w48', 'page': 12}),\n",
       " Document(page_content='Dataset Setting # doc. # inst. # word/inst. # sent./inst. # relation # NA% Adapted DocREDtrain 3053 38180 203 8.1 96+1 33 val 1000 12323 203 8.1 96+1 33 Brain MRItrain 327 327 177 11.6 \\x00 \\x00 val 86 86 132 10.1 \\x00 \\x00 Table 7: Statistics of the two document-level IE datasets. Each document may have multiple entity pairs of interest, giving rise to multiple instances in the adapted DocRED setting. For adapted DocRED, we have 96 relations from the data plus an NArelation that we introduce for 1/3 of the data. Figure 4: Annotation instructions.', metadata={'source': '/tmp/tmp7z053w48', 'page': 13}),\n",
       " Document(page_content='Type Example Predicts correctly and extracts right evidence[0]Delphine “Delphi” Greenlaw is a ﬁctional character on the New Zealand soap opera Shortland Street , who was portrayed by Anna Hutchison between 2002 and 2004. ... Predicted relation : country of origin Relation : country of origin Extracted Evidence :[0] Annotated Evidence :[0] Predicts debatably correct answer, extracts reasonable evidence[0]Anton Erhard Martinelli (1684 – September 15 , 1747) was an Austrian architect and master - builder of Italian descent. [1]Martinelli was born in Vienna . ... [3]Anton Erhard Martinelli supervised the construction of several important buildings in Vienna , such as ... [4]Hedesigned ... [6]Hedied in Vienna in 1747. Predicted relation : place of birth Relation : place of death Extracted Evidence :[1] Annotated Evidence :[0, 6] Predict incorrect example on examples requiring high amount of reasoning[0]Kurt Tucholsky (9 January 1890 – 21 December 1935 ) was a German - Jewish journalist, satirist, and writer. [1]He also wrote under the pseudonyms Kaspar Hauser (after the historical ﬁgure), Peter Panter, Theobald Tiger and Ignaz Wrobel. ... Predicted relation :NA Relation : date of death Extracted Evidence :[0] Annotated Evidence :[0] Selecting more sentences than are needed[0]Henri de Boulainvilliers ... was a French nobleman, writer and historian. ... [2]Primarily remembered as an early modern historian of the French State ,Boulainvilliers also published an early French translation of Spinoza’s Ethics and ... [3]TheComte de Boulainvilliers traced his lineage to ...[5]Much of Boulainvilliers ’ historical work ... Predicted relation : country of citizenship Relation : country of citizenship Extracted Evidence :[2, 0, 1, 5, 4, 3] Annotated Evidence :[0, 2] Table 8: Four types of representative examples that show models’ behavior. In our adapted DocRED task, models are asked to predict relations among heads andtails. Here we use model S UFFICIENT bothfor illustrations, which', metadata={'source': '/tmp/tmp7z053w48', 'page': 14}),\n",
       " Document(page_content='predict relations among heads andtails. Here we use model S UFFICIENT bothfor illustrations, which has the best evidence extraction performance. Sentences in extracted evidence are ranked by DL. Model An Example of mass effect , label: positive , evidence: 0 or 6 SUFFICIENT none [0] These images show evidence of downward displacement of the brain stem with collapse of the interpeduncular cistern and caudal displacement of the mammary bodies typical for intracran ial hypertension .[1] There is diffuse pachymeningeal enhancement evident .[2] B ilateral extra axial collections are evident the do not conform to the imaging characteristics ofCSF are seen over lying the hemispheres. [3] These likely reﬂect blood t inged hygromas and there does appear to beablood products inthedeep tendon portion of the right sided collection onthe patient ’sleft seeimage 14series 2. [4]There does appear to be a discrete linear subd ural he matoma along the right tentorial leaf. [5] Subdural collection is noted on both sides of the falx as well .[6]There is mass effect at the level of the tentorial incisure due to transtentorial her niation with deformity of the mid brain .[7] There is no evidence an acute infarct.[8] No parenchymal hemorrh age is evident .[9]Apart from the meningeal enhancement there is no abnormal enhancement noted. SUFFICIENT both [0] These images show evidence of downward displacement of thebrain stem with collapse of the interpeduncular cistern and caudal displacement of the mammary bodies typical for intrac ranial hypertension . [1] There is diffuse pachymeningeal enhancement evident. [2] Bilateral extra axial collections are evident the do not conform to the imaging characteristics of CSF are seen overlying the hemispheres. [3] These likely reﬂect blood tinged hyg romas and there does appear to be ablood products in the deep tendon portion of the right sided collection on the patient’s left seeimage 14 series 2. [4] There does appear to be a discrete linear subdural', metadata={'source': '/tmp/tmp7z053w48', 'page': 14}),\n",
       " Document(page_content='on the patient’s left seeimage 14 series 2. [4] There does appear to be a discrete linear subdural hematoma along the right tentorial leaf. [5] Subdural collection is noted on both sides of the falx as well. [6] There is mass effect at the level of the tentorial inc isure due to transtentorial her niation with deformity of the midbrain .[7] There is no evidence an acute infarct. [8] No parenchymal hemorrhage is evident. [9] Apart from the meningeal enhancement there is no abnormal enhancement noted. Table 9: An illustration of models’ attribution scores over a report from B RAIN MRI using DeepLift with and w/o regularization techniques. S UFFICIENT bothappears to leverage more information from right sentences.', metadata={'source': '/tmp/tmp7z053w48', 'page': 14})]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 2000,\n",
    "    chunk_overlap = 100,\n",
    ")\n",
    "# text_splitter = CharacterTextSplitter(        \n",
    "#     separator = '\\n\\n',\n",
    "#     chunk_size = 15,  # minimum?\n",
    "#     chunk_overlap = 10,\n",
    "#     length_function = len,\n",
    "# )\n",
    "split_text = text_splitter.split_documents(documents=pages)\n",
    "print(len(split_text))\n",
    "split_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making Document-Level Information Extraction\n",
      "Right for the Right Reasons\n",
      "Liyan Tang1, Dhruv Rajan1, Suyash Mohan2, Abhijeet Pradhan3, R. Nick Bryan1, Greg Durrett1\n",
      "1The University of Texas at Austin\n",
      "2University of Pennsylvania\n",
      "3Galileo CDS Inc.\n",
      "lytang@utexas.edu, dhruv.rajan@utexas.edu\n",
      "Suyash.Mohan@pennmedicine.upenn.edu, ap@galileocds.com\n",
      "nick.bryan@austin.utexas.edu, gdurrett@cs.utexas.edu\n",
      "Abstract\n",
      "Document-level models for information ex-\n",
      "traction tasks like slot-ﬁlling are ﬂexible: they\n",
      "can be applied to settings where information\n",
      "is not necessarily localized in a single sen-\n",
      "tence. For example, key features of a diag-\n",
      "nosis in a radiology report may not be ex-\n",
      "plicitly stated in one place, but nevertheless\n",
      "can be inferred from parts of the report’s text.\n",
      "However, these models can easily learn spuri-\n",
      "ous correlations between labels and irrelevant\n",
      "information. This work studies how to en-\n",
      "sure that these models make correct inferences\n",
      "from complex text and make those inferences\n",
      "in an auditable way: beyond just being right,\n",
      "are these models “right for the right reasons?”\n",
      "We experiment with post-hoc evidence extrac-\n",
      "tion in a predict-select-verify framework using\n",
      "feature attribution techniques. We show that\n",
      "regularization with small amounts of evidence\n",
      "supervision during training can substantially\n",
      "improve the quality of extracted evidence. We\n",
      "evaluate on two domains: a small-scale la-\n",
      "beled dataset of brain MRI reports and a large-\n",
      "scale modiﬁed version of DocRED (Yao et al.,\n",
      "2019) and show that models’ plausibility can\n",
      "be improved with no loss in accuracy.1\n",
      "1 Introduction\n",
      "Document-level information extraction (Yao et al.,\n",
      "2019; Christopoulou et al., 2019; Xiao et al., 2020;\n",
      "Guoshun et al., 2020) has seen great strides due to\n",
      "the rise of pre-trained models (Devlin et al., 2019).\n",
      "But in high-stakes domains like medical informa-\n",
      "tion extraction (Irvin et al., 2019; McDermott et al.,\n",
      "2020; Smit et al., 2020), machine learning models\n",
      "are still too error-prone to use broadly. Since they\n",
      "are not perfect, they typically play the role of as-\n",
      "sisting users in tasks like building cohorts (Pons\n",
      "et al., 2016) or in providing clinical decision sup-\n",
      "port (Demner-Fushman et al., 2009).\n",
      "1Code available at https://github.com/\n",
      "Liyan06/DocumentIE .\n",
      "Transformer\n",
      "[0]Severe encephalomalacia in the temporal lobes a nd\n",
      "frontal lobes bilaterally with reactive gliosis in the left\n",
      "frontal lobe. [1] Moderate enlargement of the ventricular\n",
      "system. [2] No abnormal enhancement. [3] Near completeopaciﬁcation of the left maxillary sinus. …evidence\tsents: \t0,\t1\n",
      "Interpretlabel\t(mass\teﬀect):\t\n",
      "negative\n",
      "Accurate?Plausible? (model\n",
      "evidence agrees w/human-\n",
      "labeled evidence)Faithful?  (model predicti on\n",
      "on evidence agrees w/full doc)Figure 1: Our basic model setup. A Transformer-based\n",
      "model makes document-level predictions on an exam-\n",
      "ple of our brain MRI reports. An interpretation method\n",
      "extracts the evidence sentences used by the model. Our\n",
      "system is evaluated according to the criteria of accu-\n",
      "racy, faithfulness, and plausibility.\n",
      "To be most usable in conjunction with users,\n",
      "these systems should not just produce a decision,\n",
      "but a justiﬁcation for their answer. The ideal system\n",
      "therefore obtains high predictive accuracy, but also\n",
      "returns a rationale that allows a human to verify the\n",
      "predicted label (Rudie et al., 2019).\n",
      "Our goal is to study document-level informa-\n",
      "tion extraction systems that are both accurate and\n",
      "which make predictions based on the correct infor-\n",
      "mation (Doshi-Velez and Kim, 2017). This process\n",
      "involves identifying what evidence the model actu-\n",
      "ally used, verifying the model’s prediction based on\n",
      "that evidence, and checking whether that evidence\n",
      "aligns with what humans would use, which would\n",
      "allow a user to quickly see if the system is correct.\n",
      "For example, in Figure 1, localizing the prediction\n",
      "ofmass effect (a feature expressing whether there\n",
      "is evidence of brain displacement by a mass like aarXiv:2110.07686v2  [cs.CL]  18 May 2022\n"
     ]
    }
   ],
   "source": [
    "print(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This paper examines how to ensure document-level models for information extraction tasks make correct inferences from complex text. It evaluates the performance of the model on two datasets and discusses various interpretation methods, attention regularization, entropy maximization, human rationales, axiomatic attribution, multi-instance multi-label learning, and adversarial multi-lingual neural relation extraction. Results show that DeepLIFT and Input Gradient perform slightly better than the other two methods.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import AnalyzeDocumentChain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "model = OpenAI(temperature=0, max_tokens=1000, openai_api_key=os.environ['OPENAI_TOKEN'])\n",
    "summary_chain = load_summarize_chain(llm=model, chain_type='map_reduce')\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    results = summary_chain.run(input_documents=split_text)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokens Used: 24471\n",
       "\tPrompt Tokens: 20411\n",
       "\tCompletion Tokens: 4060\n",
       "Successful Requests: 4\n",
       "Total Cost (USD): $0.48941999999999997"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This paper examines how to ensure document-level models for information extraction tasks make correct inferences from complex text. It evaluates the performance of the model on two datasets and discusses various interpretation methods, attention regularization, entropy maximization, human rationales, axiomatic attribution, multi-instance multi-label learning, and adversarial multi-lingual neural relation extraction. Results show that DeepLIFT and Input Gradient perform slightly better than the other two methods.\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
